% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
  ignorenonframetext,
  aspectratio=169,
]{beamer}
\newif\ifbibliography
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% remove section numbering
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\usetheme[]{Boadilla}
\usefonttheme[]{professionalfonts}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{biblatex}
\addbibresource{bibliography.bib}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{accents}
\usepackage[T2A,T1]{fontenc}


% bold letters (vectors)
\newcommand{\bnull}{\bm{0}}
\newcommand{\ba}{\bm{a}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bc}{\bm{c}}
\newcommand{\bd}{\bm{d}}
\newcommand{\be}{\bm{e}}
% \newcommand{\bf}{\bm{f}}
\newcommand{\bg}{\bm{g}}
\newcommand{\bh}{\bm{h}}
\newcommand{\bi}{\bm{i}}
\newcommand{\bj}{\bm{j}}
\newcommand{\bk}{\bm{k}}
\newcommand{\bl}{\bm{l}}
% \newcommand{\bm}{\bm{m}}
\newcommand{\bn}{\bm{n}}
\newcommand{\bo}{\bm{o}}
\newcommand{\bp}{\bm{p}}
\newcommand{\bq}{\bm{q}}
\newcommand{\br}{\bm{r}}
\newcommand{\bs}{\bm{s}}
\newcommand{\bt}{\bm{t}}
\newcommand{\bu}{\bm{u}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}

% bold letters (matrices)
\newcommand{\bA}{\bm{A}}
\newcommand{\bB}{\bm{B}}
\newcommand{\bC}{\bm{C}}
\newcommand{\bD}{\bm{D}}
\newcommand{\bE}{\bm{E}}
% \newcommand{\bf}{\bm{f}}
\newcommand{\bG}{\bm{G}}
\newcommand{\bH}{\bm{H}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bJ}{\bm{J}}
\newcommand{\bK}{\bm{K}}
\newcommand{\bL}{\bm{L}}
\newcommand{\bM}{\bm{M}}
\newcommand{\bN}{\bm{N}}
\newcommand{\bO}{\bm{O}}
\newcommand{\bP}{\bm{P}}
\newcommand{\bQ}{\bm{Q}}
\newcommand{\bR}{\bm{R}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bT}{\bm{T}}
\newcommand{\bU}{\bm{U}}
\newcommand{\bV}{\bm{V}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\hbY}{\hat{\bm{Y}}}
\newcommand{\bZ}{\bm{Z}}


% calligraphic letters
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}

% greek letters
\newcommand{\eps}{\varepsilon}
\newcommand{\sd}{\sigma}
\newcommand{\ssd}{\sigma^2}
\newcommand{\Sd}{\Sigma}
\newcommand{\Sdi}{\Sigma^{-1}}

\newcommand{\gb}[1]{\beta_#1}
\newcommand{\hbe}[1]{\hat{\beta}_#1}
\newcommand{\beps}{\bm{\varepsilon}}
\newcommand{\hbeps}{\hat{\bm{\varepsilon}}}

\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\hbbeta}{\hat{\bm{\beta}}}
\newcommand{\hssd}{\hat{\sigma^2}}
\newcommand{\bchi}{\bm{\chi}}
\newcommand{\bdelta}{\bm{\delta}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\betah}{\bm{\etah}}
\newcommand{\bpi}{\bm{\pi}}


% prior and posterior parameters
\DeclareSymbolFont{cyrhelper}{T2A}{\familydefault}{m}{n}
\DeclareMathAccent{\post}{\mathord}{cyrhelper}{18}


\newcommand{\btheta}{\bm{\theta}}
\newcommand{\hbtheta}{\hat{\bm{\theta}}}

\newcommand{\thetapri}{\breve{\bm{\theta}}}
\newcommand{\thetapo}{\post{\bm{\theta}}}
\newcommand{\mupri}{\breve{\bm{\mu}}}
\newcommand{\mupo}{\post{\bm{\mu}}}
\newcommand{\Sdpri}{\breve{\Sigma}}
\newcommand{\Sdpo}{\post{\Sigma}}
\newcommand{\Sdipri}{\breve{\Sigma}^{-1}}
\newcommand{\Sdipo}{\post{\Sigma}^{-1}}

\newcommand{\apri}{\breve{a}}
\newcommand{\apo}{\post{a}}
\newcommand{\bpri}{\breve{b}}
\newcommand{\bpo}{\post{b}}

\newcommand{\btaus}{\bm{\tau}^2}
\newcommand{\taus}{\tau^2}


% other
\newcommand{\ty}{\tilde{\bm{y}}}
\newcommand{\tX}{\tilde{\bm{X}}}


\renewcommand{\bar}{\overline}


% statistics
\providecommand{\Pr}{}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\var}{{\mathds{V}\mathrm{ar}}}
\newcommand{\cov}{{\mathds{C}\mathrm{ov}}}
\newcommand{\corr}{{\mathrm{Corr}}}
\newcommand{\ov}{\overline}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\IG}{\text{IG}}

\newcommand{\sumin}{\sum_{i = 1}^n}
\newcommand{\sumjn}{\sum_{j = 1}^n}
\definecolor{lmugreen}{RGB}{0,136,58}
\setbeamercolor{structure}{fg=lmugreen}
\usecolortheme[named=lmugreen]{structure}
\beamertemplatenavigationsymbolsempty
\usefonttheme{professionalfonts}
\usepackage{listings}
\lstset{
  language=R,
  basicstyle=\scriptsize\ttfamily,
  commentstyle=\ttfamily\color{gray},
  backgroundcolor=\color{white},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b,
  breaklines=false,
  breakatwhitespace=false,
  title=\lstname,
  escapeinside={},
  keywordstyle={},
  morekeywords={},
  belowskip=-1.2\baselineskip
}
\usepackage{caption}
\DeclareCaptionFont{tiny}{\tiny}
\captionsetup{font=scriptsize,labelfont=scriptsize,justification=centering}
\usepackage{textpos}
\addtobeamertemplate{frametitle}{}{%
  \begin{textblock*}{100mm}(0.88\textwidth,-0.5cm)
    \includegraphics[height=1cm,width=2cm]{lmu_logo}
  \end{textblock*}}
\AtBeginSection[]{%
  \begin{frame}[noframenumbering,plain]%
    \frametitle{Outline}%
    \setcounter{tocdepth}{1}%
    \tableofcontents[currentsection]%
  \end{frame}}
\AtBeginEnvironment{thebibliography}{\scriptsize}
\usepackage{booktabs}

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Bayesianische Regression},
  pdfauthor={Lona Koers},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Bayesianische Regression}
\subtitle{Lineare und logistische Modelle}
\author{Lona Koers}
\date{25. Juli 2025}
\institute{LMU}

\begin{document}
\frame{\titlepage}

\begin{frame}{Motivation für bayesianische Regression}
\protect\phantomsection\label{motivation-fuxfcr-bayesianische-regression}
Regressionsmodelle sind in der Wissenschaft und bei Anwedern sehr
beliebt, aber \ldots{}

\vspace{0.5cm}

\(\to\) \ldots{} wie können wir Vorwissen (korrekt) modellieren?

\(\to\) \ldots{} wie können wir Unsicherheit im Modell und in der
Vorhersage darstellen?
\end{frame}

\section{\texorpdfstring{Bayesianisches \textbf{lineares} Modell
(LM)}{Bayesianisches lineares Modell (LM)}}\label{bayesianisches-lineares-modell-lm}

\begin{frame}{Frequentistisches \(\to\) bayesianisches lineares Modell}
\protect\phantomsection\label{frequentistisches-to-bayesianisches-lineares-modell}
Annahmen:

\begin{enumerate}
\tightlist
\item
  i.i.d. Daten \(\bD = (\by, \bX)\)
\item
  Kondition auf \(\bX\) (implizit)
\end{enumerate}

\textbf{Frequentistisches} lineares Modell:
\(\by \sim \Ncal(\bX \btheta, \ssd \bI)\)

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  Gewichtsparameter \(\btheta\) als Zufallsvariable interpretieren
\end{enumerate}

\vspace{0.5cm}

\begin{block}{\textbf{Bayesianisches} lineares Modell:}
\protect\phantomsection\label{bayesianisches-lineares-modell}
\[\by \mid \btheta, \ssd \sim \Ncal(\bX \btheta, \ssd \bI)\]
\end{block}
\end{frame}

\begin{frame}{Priori-Verteilungen für das bayesianische LM}
\protect\phantomsection\label{priori-verteilungen-fuxfcr-das-bayesianische-lm}
\textbf{A-Priori}-Annahme für \(\btheta\) (und evtl. \(\ssd\)) notwendig
\(\to\) sehr vielseitige Modell-Anpassung möglich

\begin{block}{Normal-Invers-Gamma Priori:}
\protect\phantomsection\label{normal-invers-gamma-priori}
\[
\begin{aligned}
  \btheta \mid \ssd &\sim  \Ncal(\mupri, \ssd \Sdpri) \\
  \ssd &\sim \IG(\apri, \bpri) \\
  \btheta, \ssd &\sim \text{NIG}(\mupri, \ssd \Sdpri, \apri, \bpri)
\end{aligned}
\]

mit Priori Parametern: \(\mupri, \Sdpri, \apri\) und \(\bpri\)
\end{block}

\textbf{Vorteil}: NIG-Priori ist mit Normalverteilungs-Likelihood
konjugiert \(\to\) exakte Inferenz möglich (mehr dazu später)
\end{frame}

\begin{frame}{Uninformative Priori als Spezialfall der NIG-Priori}
\protect\phantomsection\label{uninformative-priori-als-spezialfall-der-nig-priori}
\begin{block}{Uninformative Priori}
\protect\phantomsection\label{uninformative-priori}
z.B. mit NIG-Priori mit Priori Parametern

\[
\mupri = \bnull, \quad \Sdipri = \bnull \; \text{i.e.}\; \Sdpri \to \infty, \quad \apri = - \frac{p}{2}, \quad \bpri = 0
\]

\(\implies\) flache (und damit uninformative) Priori und maximaler
Einfluss der Daten auf die Posteriori:

\[
\btheta \mid \ssd \overset{a}{\sim}  \Ncal(\mupri, \ssd \infty) \; \implies \; p(\btheta\mid \ssd) \propto 1
\]
\end{block}
\end{frame}

\begin{frame}{Regularisierung: frequentistisch \(\to\) bayesianisch}
\protect\phantomsection\label{regularisierung-frequentistisch-to-bayesianisch}
\textbf{Erinnerung}: Regularisierung im \emph{frequentistischen} LM
durch Minimierung von
\[\text{PLS}(\btheta) = (\by - \bX \btheta)^\top (\by - \bX \btheta) + \lambda \ \text{pen}(\btheta)\]
mit Regularisierungs-Parameter \(\lambda > 0\). \vspace{0.5cm}

\textbf{Bayesianische Regularisierung} durch Wahl der Priori-Verteilung
für \(\btheta\)
\end{frame}

\begin{frame}{Regularisierung durch Wahl der Priori-Verteilung}
\protect\phantomsection\label{regularisierung-durch-wahl-der-priori-verteilung}
\begin{columns}[T]
\begin{column}{0.55\linewidth}
\begin{block}{Ridge Regularisierung}
\protect\phantomsection\label{ridge-regularisierung}
\begin{itemize}
\item
  Frequentistisch \autocite{hoerl_ridge_1970,hoerl_ridge_1970-1}:
  \(\text{pen}(\btheta) = \|\btheta\|_2^2\)
\item
  Bayesianisch \autocite{mackay_bayesian_1992}:
  \(\btheta \sim \Ncal(\bnull, \taus \bI) \; \text{mit} \; \taus \propto \frac{1}{\lambda}\)
\end{itemize}
\end{block}

\begin{block}{Lasso Regularisierung}
\protect\phantomsection\label{lasso-regularisierung}
\begin{itemize}
\item
  Frequentistisch \autocite{tibshirani_regression_1996}:
  \(\text{pen}(\btheta) = \|\btheta\|_1\)
\item
  Bayesianisch \autocite{park_bayesian_2008}: \[
  \begin{aligned}
  \btheta \mid \btaus &\sim \Ncal(\bnull, \btaus \bI) \\
  \taus_j &\overset{\text{i.i.d.}}{\sim} \text{Exp}(0.5 \lambda^2), \quad j = 1, \dots, p
  \end{aligned}
  \]
\end{itemize}
\end{block}
\end{column}

\begin{column}{0.45\linewidth}
\includegraphics[width=\linewidth,height=0.75\textheight,keepaspectratio]{../figures/plot_priors.png}
\end{column}
\end{columns}

\textbf{Problem}: keine Variablenselektion (im Gegensatz zu
frequentistischem Lasso)

\(\to\) Alternative Priori für Variablenselektion: Spike and Slab
\autocite{mitchell_bayesian_1988}, Horseshoe
\autocite{carvalho_horseshoe_2010}, u.v.m.
\end{frame}

\section{\texorpdfstring{Bayesianisches \textbf{generalisiertes}
lineares Modell
(GLM)}{Bayesianisches generalisiertes lineares Modell (GLM)}}\label{bayesianisches-generalisiertes-lineares-modell-glm}

\begin{frame}{Bayesianisches LM \(\to\) \textbf{GLM}}
\protect\phantomsection\label{bayesianisches-lm-to-glm}
\[
\text{LM:} \; \by \mid \btheta, \ssd \sim \Ncal(\bX \btheta, \ssd \bI) \quad
\to \quad \text{GLM:} \; \by \mid \btheta \sim F(g^{-1}(\bX \btheta))
\]

\begin{itemize}
\tightlist
\item
  Verteilungsannahme von \(\by\) wird (äquivalent zum frequentistischen
  GLM) auf alle Verteilungen \(F\) der Exponentatialfamilie ausgeweitet
\item
  Skala des linearen Prädiktors \(\bX \btheta\) wird mit der
  Link-Funktion \(g^{-1}\) angepasst
\end{itemize}
\end{frame}

\begin{frame}{Bayesianisches GLM \(\to\) \textbf{logistisches} Modell}
\protect\phantomsection\label{bayesianisches-glm-to-logistisches-modell}
\begin{block}{\(\to\) Bayesianisches logistisches Modell}
\protect\phantomsection\label{to-bayesianisches-logistisches-modell}
\[
\begin{aligned}
  \by_i \mid \btheta &\sim \text{Bin}(1, g^{-1}(\bx_i \btheta)), \quad i = 1, \dots, n \\
  g^{-1}(\bx_i \btheta) &= \sigma(\bx_i \btheta)
\end{aligned}
\]

Für Beobachtungen \(\bx_i = (1, x_{i1}, \dots, x_{ip})^\top\) und
Sigmoid-Link \(\sigma(y) = \frac{\exp(y)}{1 + \exp(y)}\)
\end{block}

\textbf{Priori Wahl}

\begin{itemize}
\tightlist
\item
  I. Allg. äquivalent zum LM möglich, z.B. Normalverteilungs-Priori
\item
  Für Regularisierung können dieselben Priori wie im LM verwendet werden
  \autocite{ohara_review_2009,fahrmeir_bayesian_2010,van_erp_shrinkage_2019}
\item
  Verteilungen mit schweren Rändern (z.B. t-Verteilung, Cauchy
  Verteilung) addressieren Separation und fördern Shrinkage
  \autocite{gelman_weakly_2008,ghosh_use_2017}
\end{itemize}
\end{frame}

\section{Inferenz im bayesianischen
(G)LM}\label{inferenz-im-bayesianischen-glm}

\begin{frame}{Inferenz der Parameter Posteriori}
\protect\phantomsection\label{inferenz-der-parameter-posteriori}
\textbf{Erinnerung}

\begin{itemize}
\tightlist
\item
  Inferenz im frequentistischen LM: z.B. \emph{kleinste Quadrate
  Schätzung} mit
\end{itemize}

\[
\hbtheta_{KQ} = (\bX^\top \bX)^{-1} \bX^\top \by \quad \text{mit} \quad \hbtheta_{KQ} \overset{a}{\sim} \Ncal(\btheta, \ssd (\bX^\top \bX)^{-1})
\]

\begin{itemize}
\tightlist
\item
  Inferenz im bayesianischen LM: \emph{Bayes-Regel} zur Ermittlung der
  Parameter Posteriori
\end{itemize}

\[
p(\btheta \mid \by) = \frac{p(\by \mid \btheta) \; p(\btheta)}{\int p(\by \mid \btheta) \; p(\btheta) d \btheta}
= \frac{\Lcal(\btheta) \; p(\btheta)}{\int \Lcal(\btheta) \; p(\btheta) d \btheta}
\]
\end{frame}

\begin{frame}{exakte Inferenz mit konjugierten Priori}
\protect\phantomsection\label{exakte-inferenz-mit-konjugierten-priori}
\begin{block}{Inferenz mit konjugierten Priori}
\protect\phantomsection\label{inferenz-mit-konjugierten-priori}
Bayesianisches LM: aus der Konjugiertheit von
\(\by \mid \btheta, \ssd \sim \Ncal\) und
\(\btheta, \ssd \sim \text{NIG}\) ergibt sich

\[
\btheta, \ssd \mid \by \sim \text{NIG}(\mupo, \Sdpo, \apo, \bpo)
\] mit \[
\mupo = \Sdpo (\Sdipri \mupri + \bX^\top \by), \quad \Sdpo = (\bX^\top \bX + \Sdipri)^{-1}
\]
\end{block}

\begin{itemize}
\tightlist
\item
  \textbf{uninformative NIG-Priori}: a-Posteriori Mean und KQ-Schätzer
  sind äquivalent: \[
  \mupri = \bnull, \quad \Sdipri = \bnull \; \implies \; \mupo = (\bX^\top \bX)^{-1} \bX^\top \by,\quad \Sdpo = (\bX^\top \bX)^{-1}
  \]
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{Ridge}: Spezialfall der NIG-Priori \(\to\) konjugierte
  Berechnung der Parameter Posteriori
\item
  \textbf{Lasso}: Posteriori hat keine geschlossene Form, man kann aber
  mit z.B. Gibbs-Sampling daraus simulieren
  \autocite{park_bayesian_2008}
\end{itemize}
\end{frame}

\begin{frame}{Posteriori Update für Priori-Verteilungen}
\protect\phantomsection\label{posteriori-update-fuxfcr-priori-verteilungen}
\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{../figures/plot_update.png}
\end{frame}

\begin{frame}{Approximative Inferenz}
\protect\phantomsection\label{approximative-inferenz}
\textbf{Problem}: Inferenz mit konjugierten Priori ist nur sehr selten
möglich \autocite{polson_bayesian_2013}

\(\to\) \textbf{Approximative bayesianische Inferenz}, z.B. mit

\begin{itemize}
\tightlist
\item
  Sampling Methoden (Markov chain Monte Carlo Methoden)

  \begin{itemize}
  \tightlist
  \item
    \textbf{Metropolis-Hastings Algorithmus}
    \autocite{hastings_monte_1970}
  \item
    Gibbs Sampling (bei bedingte Konjugiertheit)
    \autocite{dellaportas_bayesian_1993}
  \item
    Hamiltonian Monte Carlo (v.a. hochdimensionale Posteriori)
    \autocite{neal_probabilistic_1993}
  \end{itemize}
\item
  Deterministischer Approximation, z.B. \textbf{Laplace Approximation}
  \autocite{tierney_accurate_1986}
\item
  U.v.m.
\end{itemize}
\end{frame}

\begin{frame}{Metropolis Hastings Algorithmus
\autocite{hastings_monte_1970}}
\protect\phantomsection\label{metropolis-hastings-algorithmus-hastings_monte_1970}
\begin{itemize}
\item
  \textbf{Idee}: Aus der Posteriori \(p(\btheta\mid \by)\) ziehen, ohne
  Annahmen über ihre exakte Form machen zu müssen
\item
  \textbf{Problem}: Ergebnisse sind am Besten, wenn die Posteriori bis
  auf eine Konstante (meist die Normalisierungskonstante
  \(\int p(\by \mid \btheta) \; p(\btheta) d \btheta\)) bekannt sind
\item
  \textbf{Inputs}:

  \begin{itemize}
  \tightlist
  \item
    Anzahl der Ziehungen \(K \to\) frei wählbar
  \item
    Likelihood \(p(\btheta \mid \by)\) und Priori \(p(\btheta) \to\)
    bekannt
  \item
    Proposal Verteilung \(q \to\) muss sorgfältig gewählt werden
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Metropolis Hastings: Wahl der Proposal Verteilung \(q\)}
\protect\phantomsection\label{metropolis-hastings-wahl-der-proposal-verteilung-q}
Effizienz des Algorithmus ist stark abhängig von der Proposal
Verteilung.

\(\to\) \textbf{Optionen}:

\begin{itemize}
\item
  \textbf{Normalverteilung} \[
  q(\btheta^{(*)} \mid \btheta^{(k)}) \sim \Ncal(\btheta^{(k)} \mid -H^{-1}(\btheta^{(k)}))
  \] mit Mittelwert beim letzten Sample. Die Hesse-Matrix \(H\) von
  \(p(\btheta \mid \by)\) wird meist mit IWLS geschätzt
  \autocite{gamerman_markov_1998,lenk_bayesian_2000,scott_data_2011}
\item
  \textcite{scott_data_2011} schlägt \textbf{Verteilungen mit schweren
  Rändern} vor \(\to\) mehr Mixing, kürzerer Burn-in und schnellere
  Konvergenz
\end{itemize}
\end{frame}

\begin{frame}{Laplace Approximation (LA)
\autocite{tierney_accurate_1986}}
\protect\phantomsection\label{laplace-approximation-la-tierney_accurate_1986}
\begin{itemize}
\tightlist
\item
  \textbf{Idee}: Approximation der Posteriori mit einer Normalverteilung
  \[
  p(\btheta \mid \by) \approx \Ncal(\hbtheta_{MAP}, H^{-1}(\hbtheta_{MAP}))
  \] mit Maximum-a-Posteriori Schätzer \(\hbtheta_{MAP}\) und
  Hesse-Matrix \(H\) von \(p(\btheta \mid \by)\)
\item
  Erweiterung für hierarchische Modelle: Integrated Nested Laplace
  Appriximation (INLA) \autocite{rue_approximate_2009}
\end{itemize}
\end{frame}

\begin{frame}{\emph{Beispiel:} Vergleich von approximativen Inferenz
Methoden}
\protect\phantomsection\label{beispiel-vergleich-von-approximativen-inferenz-methoden}
\textbf{Setup}: 1.000 synthetische Datensätze mit \(n=100\) und \[
\begin{aligned}
    &\bX \sim \Ncal(\bnull, \bI), \quad \btheta = (-0.5, 2, 1)\\
    &\text{linear: } \by \mid \btheta \sim \Ncal(\bX \btheta, \bI)\\
    &\text{logistic: } \by \mid \btheta \sim \text{Ber}(\sigma(\bX \btheta))
\end{aligned}
\]

\textbf{Experiment}: für jeden Datensatz wurde angepasst:

\begin{itemize}
\tightlist
\item
  Ein lineares und ein logsistisches Regressionsmodell mit
  \(\btheta \sim \Ncal(0, 10 \cdot \bI)\) und \(\ssd = 10\)
\item
  Mit Laplace Approximation und Metropolis-Hastings (\(K\) = 5.000,
  Burn-in = 500, Thinning Intervall = 10)
\end{itemize}
\end{frame}

\begin{frame}{\emph{Beispiel:} LA vs.~Metropolis-Hastings}
\protect\phantomsection\label{beispiel-la-vs.-metropolis-hastings}
\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{../figures/approx_plot_regr.png}
\includegraphics[width=0.97\linewidth,height=\textheight,keepaspectratio]{../figures/approx_plot_class.png}
\end{frame}

\begin{frame}{Vorhersagen mit bayesianischen Modellen}
\protect\phantomsection\label{vorhersagen-mit-bayesianischen-modellen}
Aus der Bayes Regel:
\[p(\by) = \int p(\by, \btheta) d\btheta = \int p(\by \mid \btheta) p(\btheta) d\btheta\]

\begin{block}{Posterior Predictive Distribution}
\protect\phantomsection\label{posterior-predictive-distribution}
\(\to\) Vorhersagen für neue Daten \(\tX\) im bayesianischen GLM:
\autocite{box_sampling_1980,barbieri_posterior_2015}

\[
p(\ty \mid \by) = \int p(\ty, \btheta \mid \by) d \btheta 
    = \int p(\ty \mid \btheta, \by) p(\btheta) d \btheta
    \overset{\ty \perp \by \mid \btheta}{=}  \int p(\ty \mid \btheta) p(\btheta) d \btheta
\]
\end{block}

Für das bayesianische \textbf{logistische Modell} berechnet man
\[p(\ty = 1 \mid \by)\] mit
\(y_i \in \{0 \text{ (negativ)}, 1 \text{ (positiv)}\}\)
\end{frame}

\begin{frame}{Berechnung der Posterior Predictive Distribution}
\protect\phantomsection\label{berechnung-der-posterior-predictive-distribution}
\textbf{Analytische Berechnung}: z.B. für die NIG-Priori möglich

Sonst: \textbf{Approximation}

\begin{itemize}
\tightlist
\item
  für Metropolis-Hastings:
  \(p(\ty = 1 \mid \by) \approx \frac{1}{K} \sum_{k=1}^{K} \sigma(\tX \btheta^{(k)})\)
\item
  für Laplace Approximation:

  \begin{itemize}
  \tightlist
  \item
    Samples
    \(\btheta^{(s)} \sim \Ncal(\hbtheta_{MAP}, H^{-1}(\hbtheta_{MAP}))\)
    mit \(s = 1, \dots, S\) aus der LA-approximierten Parameter
    Posteriori ziehen und wie bei Metropolis-Hastings vorgehen oder
  \item
    LA-approximierte PPD durch Integration analytisch berechnen
  \end{itemize}
\end{itemize}
\end{frame}

\section{Zusammenfassung}\label{zusammenfassung}

\begin{frame}
\textbf{Zusammenfassung anhand eines simplen Beispiels}: \[
\begin{aligned}
y_i = \theta_{true} x_i + \epsilon_i\quad &\text{mit} \quad\epsilon_i \sim \Ncal(0, 5) \quad \text{für} \quad i = 1, \dots, 20 \\
&\quad \theta_{true} = 2, \quad \theta \sim \Ncal(0, 5)
\end{aligned}
\]
\end{frame}

\begin{frame}
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{../figures/plot_example.png}
\end{frame}

\begin{frame}
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{../figures/plot_ppd.png}
\end{frame}

\begin{frame}[fragile]
\begin{block}{Fazit}
\protect\phantomsection\label{fazit}
\begin{itemize}
\tightlist
\item
  Bayesianische Regression ist nicht immer sinnvoller als
  frequentistische Regression
\item
  Anwendungsmöglichkeiten u.a.:

  \begin{itemize}
  \tightlist
  \item
    kleine Stichproben
  \item
    explizite Nutzung von Vorwissen
  \item
    einfache Regularisierung + Erhalt der Interpretierbarkeit
  \item
    Verteilungen (statt Konfidenzintervallen) zur Quantifizierung von
    Unsicherheit
  \end{itemize}
\item
  Inferenz: modernere Methoden notwendig
\end{itemize}
\end{block}

\begin{block}{Literatur Empfehlungen}
\protect\phantomsection\label{literatur-empfehlungen}
\begin{itemize}
\tightlist
\item
  Bayesianische Regression (v.a. für praktische Anwendung):
  \textcite{gelman_bayesian_2013}
\item
  Priori Verteilungen (v.a. Shrinkage):
  \textcite{van_erp_shrinkage_2019}
\item
  Software:

  \begin{itemize}
  \tightlist
  \item
    \texttt{brms} und \texttt{tidybayes} in \texttt{R}
  \item
    \texttt{PyMC} in \texttt{python}
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\section{Referenzen}\label{referenzen}

\section{Appendix}\label{appendix}

\begin{frame}{Praktische Anwendbarkeit von (bayesianischer)
Regularisierung}
\protect\phantomsection\label{praktische-anwendbarkeit-von-bayesianischer-regularisierung}
Vorteile von bayesianischer Regularisierung sind u.a.:

\begin{itemize}
\tightlist
\item
  Probabilistisches Modell trotz Regularisierung
\item
  Regularisierung-Parameter muss nicht als Hyperparameter optimiert
  werden (z.B. durch Priori auf \(\taus\))
\item
  Mehr Anpassungsmöglichkeiten durch Priori-Spezifikation
\end{itemize}
\end{frame}

\begin{frame}{Frequentistische vs.~bayesianische Lasso Regularisierung}
\protect\phantomsection\label{frequentistische-vs.-bayesianische-lasso-regularisierung}
\begin{figure}
\centering
\includegraphics[width=\linewidth,height=0.75\textheight,keepaspectratio]{../figures/van_erp_fig2_lasso.jpg}
\caption{Contour plot representing the sum of squared residuals,
classical lasso constraint region (left), bivariate lasso prior and
posterior distribution (right), and the classical and Bayesian penalized
point estimates. Aus \textcite{van_erp_shrinkage_2019}}
\end{figure}
\end{frame}

\begin{frame}{\emph{Beispiel:} Vergleich von Regularisierungs-Priori in
linearer und logistischer Regression}
\protect\phantomsection\label{beispiel-vergleich-von-regularisierungs-priori-in-linearer-und-logistischer-regression}
\textbf{Setup}: zwei synthetische Datensätze mit \(n=150\)

\begin{itemize}
\tightlist
\item
  \textbf{Szenario A}: Sparses Setting ohne Kollinearität oder
  Überparametrisierung

  \begin{itemize}
  \tightlist
  \item
    \(n = 150,\; n_{train} = 100,\;n_{test} = 50\)
  \item
    \(\btheta = (2, 1.5, 0, 0, 0)\)
  \item
    \(\bX \sim \Ncal(\bnull, \bI)\)
  \item
    linear: \(\by \mid \btheta \sim \Ncal(\bX \btheta,\bI)\),
    logistisch:
    \(\by \mid \btheta \sim \text{Ber}(\sigma(\bX \btheta))\)
  \end{itemize}
\item
  \textbf{Szenario B}: uninformatives Setting mit \(n \approx p\) und
  Kollinearität zwischen informativen und uniformativen Koeffizienten

  \begin{itemize}
  \tightlist
  \item
    \(n = 150,\; n_{train} = 30,\;n_{test} = 120\)
  \item
    \(\btheta = (2, 1.5, 0, \overset{26\; \text{times}}{\dots}, 0)\)
  \item
    \(\bX \sim \Ncal(\bnull, \Sd), \quad \Sd = \begin{pmatrix} 1 &        &         &        &        \\ & \!\!S_3\!\! &        & 0      &        \\ &        & I_{26} &        &        \\ & 0      &        &        &        \\ \end{pmatrix}, \qquad S_3 = \begin{pmatrix} 1   & 0.8 & 0.8\\ 0.8 & 1   & 0.8\\ 0.8 & 0.8 & 1 \end{pmatrix}\)
  \item
    linear: \(\by \mid \btheta \sim \Ncal(\bX \btheta, \bI)\),
    logistisch:
    \(\by \mid \btheta \sim \text{Ber}(\sigma(\bX \btheta))\)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\textbf{Experiment}: Für jeden Datensatz wurde angepasst:

\begin{itemize}
\tightlist
\item
  bayesianisches lineares und logistisches Modell (MCMC mit \(K=20000\),
  Burn-in = 1000, Thinning = 10) mit
\item
  uninformative, Ridge und Lasso Priori
\end{itemize}
\end{frame}

\begin{frame}{Ergebnis: Regularisierung in bayesianischen Modellen (I)}
\protect\phantomsection\label{ergebnis-regularisierung-in-bayesianischen-modellen-i}
\textbf{Evaluation}

\begin{itemize}
\tightlist
\item
  Anzahl korrekt als einflussreich identifizierter Kovariablen (Hits)
\item
  Anzahl fälschlich als einflussreich identifizierter Kovariablen (FP)
\item
  Mean log posterior predictive density (MLPPD)
  \autocite{gelman_understanding_2013}
\end{itemize}

\textbf{Ergebnis}

\begin{table}[ht]
  \small
  \centering
  \begin{tabular}{@{} ll  rrr   rrr @{}}
    \toprule
    Model & Prior 
      & \multicolumn{3}{c}{Scenario A} 
      & \multicolumn{3}{c}{Scenario B} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-8}
          &      
      & Hits (of 2) & FP (of 3)  & MLPPD      
      & Hits (of 2) & FP (of 28) & MLPPD     \\
    \midrule
    Linear & flat   & 2 &  0 & -1.425 & 2 &  1 &   -1.605     \\
    Linear & LASSO  & 2 &  0 & -1.424 & 2 &  0 &   -1.464     \\
    Linear & ridge  & 2 &  0 & -1.427 & 2 &  0 &   -1.575     \\
    \specialrule{1.5pt}{0pt}{0pt}
    Logit  & flat   & 2 &  1 & -0.390 & 0 & 21 &   $-\infty$  \\
    Logit  & LASSO  & 1 &  0 & -0.463 & 1 &  0 &   -0.485     \\
    Logit  & ridge  & 1 &  0 & -0.455 & 1 &  0 &   -0.493     \\
    \bottomrule
  \end{tabular}
\end{table}
\end{frame}

\begin{frame}{Ergebnis: Regularisierung in bayesianischen Modellen (II)}
\protect\phantomsection\label{ergebnis-regularisierung-in-bayesianischen-modellen-ii}
\includegraphics[width=\linewidth,height=0.8\textheight,keepaspectratio]{../figures/reg_all.png}
\end{frame}

\begin{frame}{Posterior Inference: Verteilung von \(\btheta\)}
\protect\phantomsection\label{posterior-inference-verteilung-von-btheta}
\begin{itemize}
\item
  Im frequentistischen Modell:
  \[\hbtheta_{KQ} \overset{a}{\sim} \Ncal(\btheta, \ssd (\bX^\top \bX)^{-1})\]
\item
  Im bayesianischen Modell (NIG-Prior): \[
  \btheta \sim \Tcal(2 \apo, \mupo, \bpo / \apo \Sdpo) \approx \Ncal(\mupo, \bpo / \apo \Sdpo)\]
  für großes \(p\)
\end{itemize}
\end{frame}

\begin{frame}{Posterior Inference: Der Metropolis (-Hastings)
Algorithmus}
\protect\phantomsection\label{posterior-inference-der-metropolis--hastings-algorithmus}
\begin{enumerate}
\tightlist
\item
  \(\btheta^{(1)}\) initialisieren
\item
  Für \(k = 1, \dots, K\)

  \begin{enumerate}
  \item
    \(\btheta^{(*)}\) aus der \textit{Proposal Verteilung}
    \(q(\btheta^{(*)} \mid \btheta^{(k)})\) ziehen
  \item
    \textit{Akzeptanzwahrscheinlichkeit} berechnen

    \[
         \alpha = \min \Bigl(
             1, \frac{
                 p(\btheta^{(*)} \mid \by)\; p(\btheta^{(*)}) \; q(\btheta^{(k)} \mid \btheta^{(*)})}{
                     p(\btheta^{(k)} \mid \by)\; p(\btheta^{(k)}) \; q(\btheta^{(*)} \mid \btheta^{(k)})
                 }
             \Bigr)
     \]
  \item
    Vorschlag \(\btheta^{(*)}\) akzeptieren oder verwerfen (für
    \(u \sim \text{Uni}[0, 1]\))

    \[
     \begin{cases}
             u \le \alpha & \btheta^{(k+1)} = \btheta^{(*)}\\
             u > \alpha & \btheta^{(k+1)} = \btheta^{(k)}\\
     \end{cases}
     \]
  \end{enumerate}
\end{enumerate}

Und z.B. \[
q(\btheta^{(*)} \mid \btheta^{(k)}) \sim \Ncal(\btheta^{(k)} \mid -H^{-1}(\btheta^{(k)}))
\] mit
\(H(\btheta) = \nabla_{\btheta}^2  \log \Bigl(p(\btheta^{(k)} \mid \by)\; p(\btheta^{(k)})\Bigr)\)
\end{frame}

\begin{frame}{Posterior Inference: Mittelwert und Varianz für Laplace
Approximation}
\protect\phantomsection\label{posterior-inference-mittelwert-und-varianz-fuxfcr-laplace-approximation}
Exemplarische Berechnung für ein bayesianisches logistisches Modell mit
der Prior \(\btheta \sim \Ncal(\bnull, \ssd \bI)\):

Es gilt dass mit Laplace Approximation
\(p(\btheta \mid \by) \approx \Ncal(\hbtheta_{MAP}, H^{-1}(\hbtheta_{MAP}))\)
mit

\[
\begin{aligned}
    \hbtheta_{MAP} &= \arg \max_{\btheta} p(\btheta \mid \by)
        \overset{\text{Bayes' rule}}{=} \arg \max_{\btheta} p(\by \mid \btheta) p(\btheta) \; d\btheta  \\
        &= \arg \max_{\btheta} \sumin \log \Bigl( \sigma(y_i \; \bx_i\btheta)\Bigr) - \frac{1}{2 \ssd} \btheta^\top \btheta \\
    H(\btheta) &= - \nabla^2_{\btheta}  \log p (\btheta \mid \by) = \frac{1}{\ssd} \bI + \sumin
    \sigma(y_i \; \bx_i\btheta) \Bigl(1 - \sigma(y_i \; \bx_i\btheta)\Bigr)
        \bx_i \bx_i^\top.
\end{aligned}
\]
\end{frame}

\begin{frame}[allowframebreaks]{}
  \bibliographytrue
  \printbibliography[heading=none]
\end{frame}

\end{document}
