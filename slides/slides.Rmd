---
title: "Bayesianische Regression"
subtitle: "lineare und logistische Modelle"
author: "Lona Koers"
institute: "LMU"
date: "25. Juli 2025"
bibliography: bibliography.bib
link-citations: true

header-includes:
  - |-
    \definecolor{lmugreen}{RGB}{0,136,58}
    \setbeamercolor{structure}{fg=lmugreen}
    \usecolortheme[named=lmugreen]{structure}
  - |-
    \beamertemplatenavigationsymbolsempty
    \usefonttheme{professionalfonts}
  - |-
    \usepackage{listings}
    \lstset{
      language=R,
      basicstyle=\scriptsize\ttfamily,
      commentstyle=\ttfamily\color{gray},
      backgroundcolor=\color{white},
      showspaces=false,
      showstringspaces=false,
      showtabs=false,
      tabsize=2,
      captionpos=b,
      breaklines=false,
      breakatwhitespace=false,
      title=\lstname,
      escapeinside={},
      keywordstyle={},
      morekeywords={},
      belowskip=-1.2\baselineskip
    }
  - |-
    \usepackage{caption}
    \DeclareCaptionFont{tiny}{\tiny}
    \captionsetup{font=scriptsize,labelfont=scriptsize,justification=centering}
  - |-
    \usepackage{textpos}
    \addtobeamertemplate{frametitle}{}{%
      \begin{textblock*}{100mm}(0.88\textwidth,-0.5cm)
        \includegraphics[height=1cm,width=2cm]{lmu_logo}
      \end{textblock*}}
  - |-
    \AtBeginSection[]{%
      \begin{frame}[noframenumbering,plain]%
        \frametitle{Outline}%
        \setcounter{tocdepth}{1}%
        \tableofcontents[currentsection]%
      \end{frame}}
  - |-
    \AtBeginEnvironment{thebibliography}{\scriptsize}

output:
  beamer_presentation:
    theme: "Boadilla"
    fonttheme: "professionalfonts"
    includes:
      in_header: ../paper/shortcuts.tex
    slide_level: 2
    citation_package: biblatex
    # citation_package: natbib
    keep_tex: true
    pandoc_args:
      # - "--natbib"
      # - "-V"
      # - "biblio-style=plain"
      - "-V"
      - "aspectratio=169"
    md_extensions: +fenced_divs+raw_tex

---

## Motivation und Intuition

TODO: gutes Beispiel

- Generalisierte Lineare Modelle (GLMs) 

- Punktvorhersage vs. Verteilung vorhersagen
- warum reicht uns ein CI / PI

# Bayesianische **lineare** Modelle

## Frequentistisches $\to$ bayesianisches lineares Modell

Annahmen:

1. i.i.d. Daten $\bD = (\by, \bX)$
2. Kondition auf $\bX$ (implizit)

**Frequentistisches** lineares Modell:  $\by \sim \Ncal(\bX \btheta, \ssd \bI)$

3. Gewichtsparameter $\btheta$ als Zufallsvariable interpretieren

::: {.block}
### **Bayesianisches** lineares Modell:
$$\by \mid \btheta, \ssd \sim \Ncal(\bX \btheta, \ssd \bI)$$
:::

## Modelldefinition (Prior-Verteilungen)

**Prior**-Annahme für $\btheta$ (und evtl. $\ssd$) notwendig $\to$ sehr vielseitige Modell-Anpassung möglich

:::{.block}
### 1. Normal-Invers-Gamma Prior:

$$
\begin{aligned}
  \btheta \mid \ssd &\sim  \Ncal(\mupri, \ssd \Sdpri) \\
  \ssd &\sim \IG(\apri, \bpri) \\
  \btheta, \ssd &\sim \text{NIG}(\mupri, \ssd \Sdpri, \apri, \bpri)
\end{aligned}
$$

mit Prior Parametern: $\mupri, \Sdpri, \apri$ und $\bpri$
:::

**Vorteil**: NIG-Prior ist mit Normalverteilungs-Likelihood konjugiert $\to$ exakte Inferenz möglich (mehr dazu später)

TODO: Bild

## Uninformative Prior als Spezialfall der NIG-Prior

:::{.block}
### 2. Uninformative Prior

z.B. mit NIG-Prior mit Prior Parametern

$$
\begin{aligned}
\mupri = \bnull&, \quad \Sdipri = \bnull \text{  i.e., } \Sdpri \to \infty \\
\apri = - \frac{p}{2}&, \quad \bpri = 0
\end{aligned}
$$

$\implies$ flache (und damit uninformative) Prior und maximaler Einfluss der Daten auf die Posterior:

$$
\btheta \mid \ssd \overset{a}{\sim}  \Ncal(\mupri, \ssd \infty) \; \implies \; p(\btheta\mid \ssd) \propto 1
$$
:::

TODO: Bild

## Regularisierung: frequentistisch vs. bayesianisch

**Erinnerung**: *frequentistische* Regularisierung durch Minimierung von 
$$\text{PLS}(\btheta) = (\by - \bX \btheta)^\top (\by - \bX \btheta) + \lambda \ \text{pen}(\btheta)$$
mit Regularisierungs-Parameter $\lambda > 0$.
<br/>
<br/>

<!-- Idee: immer wenn wir regularisieren, treffen wir in unserem Modell implizit die Annahme, dass wir es für unwahrscheinlich halten, dass viele Kovariablen einen Einfluss haben. Das macht beim frequentistischen Modell Chaor (das Modell ist nicht mehr so richtig probabilistisch) aber passt super in die Prior von bayesianischen Modellen -->


**Bayesianische Regularisierung** durch Wahl der Prior-Verteilung für $\btheta$

## Regularisierung durch Prior Wahl

:::{.block}
### 3. Ridge Regularisierung

Frequentistisch [@hoerl_ridge_1970;@hoerl_ridge_1970-1]: $\text{pen}(\btheta) = \|\btheta\|_2^2$

Bayesianisch [@mackay_bayesian_1992]: $\btheta \sim \Ncal(\bnull, \taus \bI)$ mit $\taus \propto \frac{1}{\lambda}$ 
:::

:::{.block}
### 4. Lasso Regularisierung 

Frequentistisch [@tibshirani_regression_1996]: $\text{pen}(\btheta) = \|\btheta\|_1$

Bayesianisch [@park_bayesian_2008]:
$$
\begin{aligned}
\btheta \mid \btaus &\sim \Ncal(\bnull, \btaus \bI) \\
\taus_j &\overset{\text{i.i.d.}}{\sim} \text{Exp}(0.5 \lambda^2), \quad j = 1, \dots, p
\end{aligned}
$$
:::

Problem: keine Variablenselektion (im Gegensatz zu frequentistischem Lasso)

$\to$ Alternative Priors für Variablenselektion: Spike and Slab [@mitchell_bayesian_1988], Horseshoe [@carvalho_horseshoe_2010], u.v.m.

## Regularisierung in Anwendung

Vorteile von bayesianischer Regularisierung sind u.a.:

- Probabilistisches Modell trotz Regularisierung
- Regularisierung-Parameter muss nicht als Hyperparameter optimiert werden (z.B. durch Prior auf $\taus$)
- Mehr Anpassungsmöglichkeiten durch Prior-Spezifikation


TODO: Bild regularization Priors + update


# Bayesianische **generalisierte** lineare Modelle (GLMs)


## Bayesianisches LM $\to$ **GLM**
<!-- (alt: Generalisierung des bayesianischen linearen Modells) -->

$$
\text{LM:} \; \by \mid \btheta, \ssd \sim \Ncal(\bX \btheta, \ssd \bI) \quad
\to \quad \text{GLM:} \; \by \mid \btheta \sim F(g^{-1}(\bX \btheta))
$$

- Verteilungsannahme von $\by$ wird (äquivalent zum frequentistischen GLM) auf alle Verteilungen $F$ der Exponentatialfamilie ausgeweitet
- Skala des linearen Prädiktors $\bX \btheta$ wird mit der Link-Funktion $g^{-1}$ angepasst


## GLM $\to$ **logistisches** Modell

:::{.block}
### Bayesianisches logistisches Modell

$$
\begin{aligned}
  \by_i \mid \btheta &\sim \text{Bin}(1, g^{-1}(\bx_i \btheta)), \quad i = 1, \dots, n \\
  g^{-1}(\bx_i \btheta) &= \sigma(\bx_i \btheta)
\end{aligned}
$$

Für Beobachtungen $\bx_i = (1, x_{i1}, \dots, x_{ip})^\top$ und Sigmoid-Link $\sigma(y) = \frac{\exp(y)}{1 + \exp(y)}$

:::

**Prior Wahl**

- Im Allgemeinen äquivalent zum LM möglich, z.B. Normalverteilung-Prior
- Verteilungen mit schweren Rändern (z.B. t-Verteilung, Cauchy Verteilung) verringern Separation und fördern Shrinkage [@gelman_weakly_2008;@ghosh_use_2017]
- Für Regularisierung können dieselben Priors verwendet werden [@ohara_review_2009;@fahrmeir_bayesian_2010;@van_erp_shrinkage_2019]


## Beispiel 1: Regularisierung in data-sparse Szenarien

# Posterior Inference

**Erinnerung**: Bayes-Regel zur Ermittlung der Parameter Posterior
$$
p(\btheta \mid \by) = \frac{p(\by \mid \btheta) \; p(\btheta)}{\int p(\by \mid \btheta) \; p(\btheta) d \btheta},
$$
wobei $p(\by \mid \btheta)$ die Modell-Likelihood ist.

## bayesianisches LM: exakte Inferenz mit konjugierten Prioris

Frequentisitsches LM: z.B. kleinste Quadrate Schätzung mit
$$
\hbtheta_{KQ} = (\bX^\top \bX)^{-1} \bX^\top \by \quad \text{mit} \quad \hbtheta_{KQ} \sim \Ncal(\btheta, \ssd (\bX^\top \bX)^{-1})
$$

:::{.block}
### Inferenz mit konjugierten Priors

Bayesianisches LM: Nutzung der Konjugiertheit von $\by \mid \btheta, \ssd \sim \Ncal$ und $\btheta, \ssd \sim \text{NIG}$

$$
\btheta, \ssd \mid \by \sim \text{NIG}(\mupo, \Sdpo, \apo, \bpo)
$$
mit
$$
\mupo = \Sdpo (\Sdipri \mupri + \bX^\top \by), \quad \Sdpo = (\bX^\top \bX + \Sdipri)^{-1}
$$
:::


$\to$ im Fall der uninformativen NIG-Prior sind der Posterior Mean und der KQ-Schätzer äquivalent:

$$
\begin{aligned}
\mupri = \bnull&, \Sdipri = \bnull \\
\implies \mupo = (\bX^\top \bX)^{-1} \bX^\top \by&, \Sdpo = (\bX^\top \bX)^{-1}
\begin{aligned}
$$

$\to$ Marginale Posterior Verteilungen von $\btheta$ und $\ssd$ durch Integration für z.B. Kredibilitätsintervalle

- Ridge: Spezialfall der NIG-Prior $\to$ konjugierte Berechnung der Parameter Posterior
- Lasso: Prior hat keine geschlossene Form, aber man die Posterior einfach mit Gibbs-Sampling simulieren [@park_bayesian_2008]

## bayesianisches GLM: approximative Inferenz

Inferenz mit konjugierten Priors ist nur sehr selten möglich [@polson_bayesian_2013] $\to$ Nutzung von Methoden der **approximative bayesianische Inferenz**, um aus der Posterior zu samplen:

- Markov chain Monte Carlo Methoden
  - **Metropolis-Hastings Algorithmus** [@hastings_monte_1970]
  - Gibbs Sampling (bedingte Konjugiertheit) [@dellaportas_bayesian_1993]
  - Hamiltonian Monte Carlo (v.a. hochdimensionale Posterior) [@neal_probabilistic_1993]
- Data Augmentation mit latenten Variablen, um künstlich Konjugiertheit zu induzieren [@albert_bayesian_1993;@holmes_efficient_nodate;@fruhwirth-schnatter_auxiliary_2007;@scott_data_2011]
- **Laplace Approximation** [@tierney_accurate_1986]

### Metropolis Hastings Algorithmus [@hastings_monte_1970]

- **Idee**: Aus der Posterior $p(\btheta\mid \by) ziehen, ohne Annahmen über ihre exakte Form machen zu müssen
- **Problem**: Ergebnisse sind am Besten, wenn die Posterior bis auf eine Konstante (meist die Normalisierungskonstante $\int p(\by \mid \btheta) \; p(\btheta) d \btheta$) bekannt sind

- **Inputs**:
  - Anzahl der Ziehungen $K \to$ frei wählbar
  - Likelihood $p(\btheta \mid \by)$ und Prior $p(\btheta) \to$ bekannt
  - Proposal Verteilung $q \to$ muss gut gewählt werden für 


### Metropolis Hastings: Wahl der Proposal Verteilung

Effizienz des Algorithmus ist stark abhängig von der Proposal Verteilung

**Optionen**:

- **Normalverteilung**
  $$
  q(\btheta^{(*)} \mid \btheta^{(k)}) \sim \Ncal(\btheta^{(k)} \mid -H^{-1}(\btheta^{(k)}))
  $$
    mit Mittelwert beim letzten Sample. Hessian $H$ wird meist mit IWLS geschätzt [@gamerman_markov_1998;@lenk_bayesian_2000;@scott_data_2011] 

- @scott_data_2011 schlägt **Verteilungen mit schweren Rändern** vor $\to$ mehr Mixing, kürzerer Burn-in und schnellere Konvergenz


### Laplace Approximation (LA) [@tierney_accurate_1986]

- **Idee**: Approximation der Posterior mit einer Normalverteilung
  $$
  p(\btheta \mid \by) \approx \Ncal(\hbtheta_{MAP}, H^{-1}(\hbtheta_{MAP})),
  $$
  wobei $\hbtheta_{MAP}$ das maximum posterior estimate und $H$ die Hesse-Matrix von $p(\btheta \mid \by)$ ist
- Modifikation für hierarchische Modelle: Integrated Nested Laplace Appriximation (INLA) [@rue_approximate_2009]


## *Beispiel 2:* Vergleich von Approximate Inference Methods

**Setup**: 1.000 synthetische Datensätze mit $n=100$ und 
$$
\begin{aligned}
    &\bX \sim \Ncal(\bnull, \bI), \quad \btheta = (-0.5, 2, 1)\\
    &\text{linear: } \by \mid \btheta \sim \Ncal(\bX \btheta, \bI)\\
    &\text{logistic: } \by \mid \btheta \sim \text{Ber}(\sigma(\bX \btheta))
\end{aligned}
$$

**Experiment**: für jeden Datensatz wurde

- ein lineares und ein logsistisches Regressionsmodell mit $\btheta \sim \Ncal(0, 10 \cdot \bI)$ und $\ssd = 10$
- mit Laplace Approximation und Metropolis-Hastings ($K$ = 5.000, Burn-in = 500, Thinning Intervall = 10) 

angepasst

## *Beispiel 2:* Vergleich von Approximate Inference Methods

\image{../figures/approx_all.png}


<!-- - LA war langsamer (kann aber auch an Implementierung liegen!) -->
<!-- - Lineare Regression: ganz gute Schätzungen, Varianz bei LA viel höher -->
<!-- - Logistische Regression: LA liefert bessere Schätzung und kleinere Varianz, dauert aber länger -->
<!-- - generell eher zur Veranschaulichung, ich würde empfehlen modernere Methoden zu benutzen, die sind eigentlich überall implementiert -->

## Vorhersagen mit bayesianischen Modellen

<!-- - In Machine Learning interessieren wir uns oft eher für Vorhersagen -->

Aus der Bayes Regel:
$$p(\by) = \int p(\by, \btheta) d\btheta = \int p(\by \mid \btheta) p(\btheta) d\btheta$$
:::{.block} 
### Posterior Predictive Distribution

$\to$ Vorhersagen für $\tX$ im Bayesianischen GLM [@box_sampling_1980;@barbieri_posterior_2015]

$$
p(\ty \mid \by) = \int p(\ty, \btheta \mid \by) d \btheta 
    = \int p(\ty \mid \btheta, \by) p(\btheta) d \btheta
    \overset{\ty \perp \by \mid \btheta}{=}  \int p(\ty \mid \btheta) p(\btheta) d \btheta
$$

<!-- -> = marginale Verteilung von neuen y bedingt auf die alten Daten (reminder: implizite Bedingung auf Daten X) -->
<!-- -> ist quasi der Mittelwert der bedingten Verteilung von neuen y (also der Likelihood) über die Parameter Posterior -->
:::

Für das bayesianische logistische Modell berechnet man $p(\ty = 1 \mid \btheta, \by)$ mit $y_i \in \{0 \text{ (negative)}, 1 \text{ (positive)}\}$

TODO: Bild


## Berechnung der Posterior Predictive Distribution

**Analytische Berechnung**: z.B. für die NIG-Prior ergibt sich
  $$\ty \mid \btheta, \ssd, \by \sim \Tcal(2 \apo, \tX \btheta, \frac{\bpo}{\apo} (\bI + \tX \Sdpo \tX^\top))$$

Alternativ (und viel wichtiger): **Approximation**

- für Metropolis-Hastings: p(\ty = 1 \mid \btheta, \by) \approx \frac{1}{K} \sum_{k=1}^{K} \sigma(\tX \btheta^{(k)})
- für Laplace Approximation:
    - LA-approximierte PPD analytisch berechnen: 
$$
p(\ty = 1 \mid \btheta, \by) = \int \sigma(\tX \btheta) \; \Ncal_{\btheta}(\hbtheta_{MAP}, H^{-1} (\hbtheta_{MAP})) \; d \btheta
$$
    - Samples $\btheta^{(s)} \sim \Ncal(\hbtheta_{MAP}, H^{-1}(\hbtheta_{MAP}))$ mit $s = 1, \dots, S$ aus der LA-approximierten Parameter Posterior ziehen und wie bei Metropolis-Hastings vorgehen

<!-- wir haben schon gesamplet und können die samples aus der Parameter Posterior jetzt einfach wiederverwenden, um die posterior predictive distribution zu approximieren -->


## Literatur Empfehlungen

- Bayesianische Regression (v.a. für praktische Anwendung): @gelman_bayesian_2013
- Prior Verteilungen (v.a. Shrinkage): @van_erp_shrinkage_2019 und @celeux_regularization_2012
- Software: z.B. `brms` in `R`, `PyMC` in `python`

# Referenzen

::: {#refs}
:::

# Anhang

## Herleitung der kojugierten Priori


## Ergebnisse des Regularisierungsexperiments


## Posterior Inference: exakter Metropolis (-Hastings) Algorithmus


1. Initialize $\btheta^{(1)}$
2. For $k = 1, \dots, K$
    1. Draw $\btheta^{(*)}$ from the \textit{proposal distribution} $q(\btheta^{(*)} \mid \btheta^{(k)})$
    2. calculate the \textit{acceptance probably} 
        $$
            \alpha = \min \Bigl(
                1, \frac{
                    p(\btheta^{(*)} \mid \by)\; p(\btheta^{(*)}) \; q(\btheta^{(k)} \mid \btheta^{(*)})}{
                        p(\btheta^{(k)} \mid \by)\; p(\btheta^{(k)}) \; q(\btheta^{(*)} \mid \btheta^{(k)})
                    }
                \Bigr)
        $$
    3. Accept or discard the proposal $\btheta^{(*)}$ (for $u \sim \text{Uni}[0, 1]$)
        $$
        \begin{cases}
                u \le \alpha & \btheta^{(k+1)} = \btheta^{(*)}\\
                u > \alpha & \btheta^{(k+1)} = \btheta^{(k)}\\
        \end{cases}
        $$

Und z.B. 
$$
q(\btheta^{(*)} \mid \btheta^{(k)}) \sim \Ncal(\btheta^{(k)} \mid -H^{-1}(\btheta^{(k)}))
$$
mit $H(\btheta) = \nabla_{\btheta}^2  \log \Bigl(p(\btheta^{(k)} \mid \by)\; p(\btheta^{(k)})\Bigr)$


## Posterior Inference: Mittelwert und Varianz für Laplace Approximation

Exemplarische Berechnung für ein bayesianisches logistisches Modell mit der Prior $\btheta \sim \Ncal(\bnull, \ssd \bI)$: 

Es gilt dass mit Laplace Approximation $p(\btheta \mid \by) \approx \Ncal(\hbtheta_{MAP}, H^{-1}(\hbtheta_{MAP}))$ mit

$$
\begin{aligned}
    \hbtheta_{MAP} &= \arg \max_{\btheta} p(\btheta \mid \by)
        \overset{\text{Bayes' rule}}{=} \arg \max_{\btheta} p(\by \mid \btheta) p(\btheta) \; d\btheta  \\
        &= \arg \max_{\btheta} \sumin \log \Bigl( \sigma(y_i \; \bx_i\btheta)\Bigr) - \frac{1}{2 \ssd} \btheta^\top \btheta \\
    H(\btheta) &= - \nabla^2_{\btheta}  \log p (\btheta \mid \by) = \frac{1}{\ssd} \bI + \sumin
    \sigma(y_i \; \bx_i\btheta) \Bigl(1 - \sigma(y_i \; \bx_i\btheta)\Bigr)
        \bx_i \bx_i^\top.
\end{aligned}
$$

## Experiment-Setup: Regularisierung


## Experiment-Setup: approximate Inference




