\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bishop_pattern_2019}
\citation{bishop_pattern_2019}
\citation{bishop_pattern_2019}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{fahrmeir_regression_2021}
\citation{gelman_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear Bayesian Model}{2}{section.2}\protected@file@percent }
\newlabel{sec:bayesian_lm}{{2}{2}{Linear Bayesian Model}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model definition}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:LM}{{1}{2}{Model definition}{equation.2.1}{}}
\newlabel{eq:BLM}{{2}{2}{Model definition}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Prior choice}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:NIGprior}{{3}{2}{Normal (Inverse Gamma) Prior}{equation.2.3}{}}
\citation{zellner_assessing_1986}
\citation{hoerl_ridge_1970,hoerl_ridge_1970-1}
\citation{hsiang_bayesian_1975,mackay_bayesian_1992}
\newlabel{eq:flat-prior}{{4}{3}{Uninformative Prior}{equation.2.4}{}}
\newlabel{sec:lm-regularization}{{2.2}{3}{Regularization Priors}{section*.4}{}}
\citation{tibshirani_regression_1996}
\citation{park_bayesian_2008}
\citation{mitchell_bayesian_1988}
\citation{carvalho_horseshoe_2010}
\citation{held_likelihood_2020}
\newlabel{eq:ridge}{{5}{4}{Regularization Priors}{equation.2.5}{}}
\newlabel{eq:lasso}{{6}{4}{Regularization Priors}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Bayesian inference with closed form priors}{4}{subsection.2.3}\protected@file@percent }
\newlabel{eq:LSE}{{7}{4}{Parameter posterior distribution}{equation.2.7}{}}
\citation{park_bayesian_2008}
\citation{box_sampling_1980,barbieri_posterior_2015}
\citation{murphy_conjugate_nodate}
\newlabel{eq:NIGpost}{{8}{5}{Parameter posterior distribution}{equation.2.8}{}}
\citation{nelder_generalized_1972,west_dynamic_1985}
\citation{west_dynamic_1985}
\citation{gelman_weakly_2008}
\citation{ghosh_use_2017}
\citation{van_erp_shrinkage_2019,fahrmeir_bayesian_2010,ohara_review_2009}
\@writefile{toc}{\contentsline {section}{\numberline {3}Logistic Bayesian Model}{6}{section.3}\protected@file@percent }
\newlabel{sec:bayesian_logit}{{3}{6}{Logistic Bayesian Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Generalized Linear Regression Model}{6}{subsection.3.1}\protected@file@percent }
\newlabel{sec:logit-glm}{{3.1}{6}{Bayesian Generalized Linear Regression Model}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Bayesian Logistic Regression Model}{6}{subsection.3.2}\protected@file@percent }
\newlabel{sec:logit-logit}{{3.2}{6}{Bayesian Logistic Regression Model}{subsection.3.2}{}}
\newlabel{eq:logit}{{9}{6}{Model definition}{equation.3.9}{}}
\citation{hastings_monte_1970}
\citation{gamerman_markov_1998,lenk_bayesian_2000,scott_data_2011}
\citation{scott_data_2011}
\citation{dellaportas_bayesian_1993}
\citation{neal_probabilistic_1993}
\citation{albert_bayesian_1993}
\citation{holmes_efficient_nodate,fruhwirth-schnatter_auxiliary_2007,scott_data_2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Approximate Bayesian inference}{7}{subsection.3.3}\protected@file@percent }
\newlabel{sec:logit-inf}{{3.3}{7}{Approximate Bayesian inference}{subsection.3.3}{}}
\citation{tierney_accurate_1986}
\citation{rue_approximate_2009}
\newlabel{eq:ppd-sample}{{10}{8}{Posterior predictive distribution}{equation.3.10}{}}
\citation{van_erp_shrinkage_2019}
\citation{gelman_understanding_2013}
\citation{van_erp_shrinkage_2019}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulation Study}{9}{section.4}\protected@file@percent }
\newlabel{sec:simulation}{{4}{9}{Simulation Study}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Regularization and variable selection}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Evaluation under scenario A and B. Linear Regression regardless of prior identified influential coefficients more accurately (Hits) and misclassified less coefficients as influential (FP). We see the effect of regularization more strongly in Logistic regression, where regardless of the scenario, less coefficients were declared influential. Regularization priors performed better in the non-informative scenario B. Except for the flat prior, Bayesian logistic regression made more accurate predictions than linear regression, but the MLPPD is not very different between regularization and no regularization.\relax }}{10}{table.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:reg_AB}{{1}{10}{Evaluation under scenario A and B. Linear Regression regardless of prior identified influential coefficients more accurately (Hits) and misclassified less coefficients as influential (FP). We see the effect of regularization more strongly in Logistic regression, where regardless of the scenario, less coefficients were declared influential. Regularization priors performed better in the non-informative scenario B. Except for the flat prior, Bayesian logistic regression made more accurate predictions than linear regression, but the MLPPD is not very different between regularization and no regularization.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Performance of approximate inference algorithms in Bayesian regression}{10}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Estimated model parameters with $95\%$ credibility interval (CI). The true parameters are notes as black crosses. Linear models produced more accurate estimates and smaller parameter CIs than logistic models. The necessity of regularization becomes apparent in scenario B, where the parameter estimates of the unregularized logistic model are very uncertain.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:reg-params}{{1}{11}{Estimated model parameters with $95\%$ credibility interval (CI). The true parameters are notes as black crosses. Linear models produced more accurate estimates and smaller parameter CIs than logistic models. The necessity of regularization becomes apparent in scenario B, where the parameter estimates of the unregularized logistic model are very uncertain.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results of approximate Bayesian inference with a linear model.\relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig:approx-regr}{{2}{11}{Results of approximate Bayesian inference with a linear model.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results of approximate Bayesian inference with a logistic model.\relax }}{11}{figure.caption.15}\protected@file@percent }
\newlabel{fig:approx-class}{{3}{11}{Results of approximate Bayesian inference with a logistic model.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Conclusion}{section.5}{}}
\citation{fahrmeir_regression_2021}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{V}{appendix.A}\protected@file@percent }
\newlabel{sec:app}{{A}{V}{Appendix}{appendix.A}{}}
\citation{murphy_conjugate_nodate}
\citation{gelman_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {B}Electronic appendix}{VII}{appendix.B}\protected@file@percent }
\newlabel{sec:el_app}{{B}{VII}{Electronic appendix}{appendix.B}{}}
\bibstyle{dcu}
\bibdata{bibliography}
\bibcite{albert_bayesian_1993}{{1}{1993}{{Albert and\ Chib}}{{}}}
\bibcite{barbieri_posterior_2015}{{2}{2015}{{Barbieri}}{{}}}
\bibcite{bishop_pattern_2019}{{3}{2019}{{Bishop}}{{}}}
\bibcite{box_sampling_1980}{{4}{1980}{{Box}}{{}}}
\bibcite{carvalho_horseshoe_2010}{{5}{2010}{{Carvalho et~al.}}{{Carvalho, Polson and\ Scott}}}
\bibcite{dellaportas_bayesian_1993}{{6}{1993}{{Dellaportas and\ Smith}}{{}}}
\bibcite{fahrmeir_bayesian_2010}{{7}{2010}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib and\ Konrath}}}
\bibcite{fahrmeir_regression_2021}{{8}{2021}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib, Lang and\ Marx}}}
\bibcite{fruhwirth-schnatter_auxiliary_2007}{{9}{2007}{{Frühwirth-Schnatter and\ Frühwirth}}{{}}}
\bibcite{gamerman_markov_1998}{{10}{1998}{{Gamerman}}{{}}}
\bibcite{gelman_bayesian_2013}{{11}{2013}{{Gelman, Carlin, Stern, Dunson, Vehtari and\ Rubin}}{{}}}
\bibcite{gelman_understanding_2013}{{12}{2013}{{Gelman, Hwang and\ Vehtari}}{{}}}
\bibcite{gelman_weakly_2008}{{13}{2008}{{Gelman et~al.}}{{Gelman, Jakulin, Pittau and\ Su}}}
\bibcite{ghosh_use_2017}{{14}{2017}{{Ghosh et~al.}}{{Ghosh, Li and\ Mitra}}}
\bibcite{hastings_monte_1970}{{15}{1970}{{Hastings}}{{}}}
\bibcite{held_likelihood_2020}{{16}{2020}{{Held and\ Sabanés~Bové}}{{}}}
\bibcite{hoerl_ridge_1970}{{17}{1970a}{{Hoerl and\ Kennard}}{{}}}
\bibcite{hoerl_ridge_1970-1}{{18}{1970b}{{Hoerl and\ Kennard}}{{}}}
\bibcite{holmes_efficient_nodate}{{19}{n.d.}{{Holmes}}{{}}}
\bibcite{hsiang_bayesian_1975}{{20}{1975}{{Hsiang}}{{}}}
\bibcite{lenk_bayesian_2000}{{21}{2000}{{Lenk and\ DeSarbo}}{{}}}
\bibcite{mackay_bayesian_1992}{{22}{1992}{{MacKay}}{{}}}
\bibcite{mitchell_bayesian_1988}{{23}{1988}{{Mitchell and\ Beauchamp}}{{}}}
\bibcite{murphy_conjugate_nodate}{{24}{n.d.}{{Murphy}}{{}}}
\bibcite{neal_probabilistic_1993}{{25}{1993}{{Neal}}{{}}}
\bibcite{nelder_generalized_1972}{{26}{1972}{{Nelder and\ Wedderburn}}{{}}}
\bibcite{ohara_review_2009}{{27}{2009}{{O'Hara and\ Sillanpää}}{{}}}
\bibcite{park_bayesian_2008}{{28}{2008}{{Park and\ Casella}}{{}}}
\bibcite{rue_approximate_2009}{{29}{2009}{{Rue et~al.}}{{Rue, Martino and\ Chopin}}}
\bibcite{scott_data_2011}{{30}{2011}{{Scott}}{{}}}
\bibcite{tibshirani_regression_1996}{{31}{1996}{{Tibshirani}}{{}}}
\bibcite{tierney_accurate_1986}{{32}{1986}{{Tierney et~al.}}{{Tierney, and\ Kadane}}}
\bibcite{van_erp_shrinkage_2019}{{33}{2019}{{van Erp et~al.}}{{van Erp, Oberski and\ Mulder}}}
\bibcite{west_dynamic_1985}{{34}{1985}{{West et~al.}}{{West, ~, and\ Migon}}}
\bibcite{zellner_assessing_1986}{{35}{1986}{{Zellner}}{{}}}
\gdef \@abspage@last{22}
