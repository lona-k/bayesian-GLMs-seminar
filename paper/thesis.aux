\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nelder_generalized_1972}
\citation{gelman_weakly_2008}
\citation{tyralis_review_2024}
\citation{sondhi_bayesian_2021}
\citation{van_erp_shrinkage_2019,celeux_regularization_2012}
\citation{chien_informative_2023}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{fahrmeir_regression_2021}
\citation{gelman_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian Linear Model}{2}{section.2}\protected@file@percent }
\newlabel{sec:bayesian_lm}{{2}{2}{Bayesian Linear Model}{section.2}{}}
\newlabel{sec:bayesian_lm@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model Definition}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:LM}{{1}{2}{Model Definition}{equation.2.1}{}}
\newlabel{eq:LM@cref}{{[equation][1][]1}{[1][2][]2}}
\newlabel{eq:BLM}{{2}{2}{Model Definition}{equation.2.2}{}}
\newlabel{eq:BLM@cref}{{[equation][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Prior choice}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:NIGprior}{{3}{2}{Normal (Inverse Gamma) Prior}{equation.2.3}{}}
\newlabel{eq:NIGprior@cref}{{[equation][3][]3}{[1][2][]2}}
\citation{fahrmeir_regression_2021}
\citation{zellner_assessing_1986}
\citation{hoerl_ridge_1970,hoerl_ridge_1970-1}
\citation{hsiang_bayesian_1975,mackay_bayesian_1992}
\newlabel{eq:flat-prior}{{4}{3}{Uninformative Prior}{equation.2.4}{}}
\newlabel{eq:flat-prior@cref}{{[equation][4][]4}{[1][3][]3}}
\newlabel{sec:lm-regularization}{{2.2}{3}{Regularization Priors}{section*.4}{}}
\newlabel{sec:lm-regularization@cref}{{[subsection][2][2]2.2}{[1][3][]3}}
\citation{tibshirani_regression_1996}
\citation{park_bayesian_2008}
\citation{mitchell_bayesian_1988}
\citation{carvalho_horseshoe_2010}
\newlabel{eq:ridge}{{5}{4}{Regularization Priors}{equation.2.5}{}}
\newlabel{eq:ridge@cref}{{[equation][5][]5}{[1][3][]4}}
\newlabel{eq:lasso}{{6}{4}{Regularization Priors}{equation.2.6}{}}
\newlabel{eq:lasso@cref}{{[equation][6][]6}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Bayesian Inference with Closed Form Priors}{4}{subsection.2.3}\protected@file@percent }
\newlabel{eq:LSE}{{7}{4}{Parameter Posterior Distribution}{equation.2.7}{}}
\newlabel{eq:LSE@cref}{{[equation][7][]7}{[1][4][]4}}
\citation{held_likelihood_2020}
\citation{park_bayesian_2008}
\citation{box_sampling_1980,barbieri_posterior_2015}
\citation{murphy_conjugate_nodate}
\newlabel{eq:NIGpost}{{8}{5}{Parameter Posterior Distribution}{equation.2.8}{}}
\newlabel{eq:NIGpost@cref}{{[equation][8][]8}{[1][4][]5}}
\citation{nelder_generalized_1972,west_dynamic_1985}
\citation{west_dynamic_1985,hosack_prior_2017}
\citation{dayanik_constructing_2006,sondhi_bayesian_2021}
\citation{albert_bayesian_1993}
\citation{gelman_weakly_2008}
\citation{ghosh_use_2017}
\citation{van_erp_shrinkage_2019,fahrmeir_bayesian_2010,ohara_review_2009}
\citation{polson_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {3}Bayesian Logistic Model}{6}{section.3}\protected@file@percent }
\newlabel{sec:bayesian_logit}{{3}{6}{Bayesian Logistic Model}{section.3}{}}
\newlabel{sec:bayesian_logit@cref}{{[section][3][]3}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Generalized Linear Model}{6}{subsection.3.1}\protected@file@percent }
\newlabel{sec:logit-glm}{{3.1}{6}{Bayesian Generalized Linear Model}{subsection.3.1}{}}
\newlabel{sec:logit-glm@cref}{{[subsection][1][3]3.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Bayesian Logistic Model}{6}{subsection.3.2}\protected@file@percent }
\newlabel{sec:logit-logit}{{3.2}{6}{Bayesian Logistic Model}{subsection.3.2}{}}
\newlabel{sec:logit-logit@cref}{{[subsection][2][3]3.2}{[1][6][]6}}
\newlabel{eq:logit}{{9}{6}{Model Definition}{equation.3.9}{}}
\newlabel{eq:logit@cref}{{[equation][9][]9}{[1][6][]6}}
\citation{hastings_monte_1970}
\citation{gamerman_markov_1998,lenk_bayesian_2000,scott_data_2011}
\citation{scott_data_2011}
\citation{dellaportas_bayesian_1993}
\citation{neal_probabilistic_1993}
\citation{albert_bayesian_1993}
\citation{holmes_efficient_nodate,fruhwirth-schnatter_auxiliary_2007,scott_data_2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Approximate Bayesian Inference}{7}{subsection.3.3}\protected@file@percent }
\newlabel{sec:logit-inf}{{3.3}{7}{Approximate Bayesian Inference}{subsection.3.3}{}}
\newlabel{sec:logit-inf@cref}{{[subsection][3][3]3.3}{[1][6][]7}}
\citation{tierney_accurate_1986}
\citation{rue_approximate_2009}
\newlabel{eq:ppd-sample}{{10}{8}{Posterior Predictive Distribution}{equation.3.10}{}}
\newlabel{eq:ppd-sample@cref}{{[equation][10][]10}{[1][8][]8}}
\citation{brms_2017}
\citation{makalic_bayesreg_2016}
\citation{van_erp_shrinkage_2019}
\citation{gelman_understanding_2013}
\@writefile{toc}{\contentsline {section}{\numberline {4}Illustrative Examples}{9}{section.4}\protected@file@percent }
\newlabel{sec:simulation}{{4}{9}{Illustrative Examples}{section.4}{}}
\newlabel{sec:simulation@cref}{{[section][4][]4}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Regularization and Variable Selection}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Evaluation metrics of Bayesian linear and logistic regression, each with a flat, ridge, and LASSO prior, under Scenarios A and B. \relax }}{10}{table.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:reg_AB}{{1}{10}{Evaluation metrics of Bayesian linear and logistic regression, each with a flat, ridge, and LASSO prior, under Scenarios A and B. \relax }{table.caption.12}{}}
\newlabel{tab:reg_AB@cref}{{[table][1][]1}{[1][9][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Estimated model parameters with 95\% credibility intervals (CI). True parameter values are black crosses. \relax }}{10}{figure.caption.13}\protected@file@percent }
\newlabel{fig:reg-params}{{1}{10}{Estimated model parameters with 95\% credibility intervals (CI). True parameter values are black crosses. \relax }{figure.caption.13}{}}
\newlabel{fig:reg-params@cref}{{[figure][1][]1}{[1][10][]10}}
\citation{rue_approximate_2009}
\citation{MCMCglmm_2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Performance of Approximate Inference Algorithms}{11}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Bayesian \textbf  {linear} regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. MCMC is faster in this case. Both methods estimate the parameters accurately, though LA yields higher posterior uncertainty. \relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig:approx-regr}{{2}{11}{Bayesian \textbf {linear} regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. MCMC is faster in this case. Both methods estimate the parameters accurately, though LA yields higher posterior uncertainty. \relax }{figure.caption.14}{}}
\newlabel{fig:approx-regr@cref}{{[figure][2][]2}{[1][11][]11}}
\citation{gelman_weakly_2008}
\citation{fahrmeir_bayesian_2010}
\citation{piironen_comparison_2017}
\citation{angelopoulos_gentle_2022}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Bayesian \textbf  {logistic} regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. LA outperforms MCMC in both accuracy and precision of parameter estimates. While LA is slower to compute, it provides more stable estimates. \relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig:approx-class}{{3}{12}{Bayesian \textbf {logistic} regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. LA outperforms MCMC in both accuracy and precision of parameter estimates. While LA is slower to compute, it provides more stable estimates. \relax }{figure.caption.15}{}}
\newlabel{fig:approx-class@cref}{{[figure][3][]3}{[1][11][]12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Outlook}{12}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Conclusion and Outlook}{section.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]5}{[1][12][]12}}
\citation{fahrmeir_regression_2021}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{V}{appendix.A}\protected@file@percent }
\newlabel{sec:app}{{A}{V}{Appendix}{appendix.A}{}}
\newlabel{sec:app@cref}{{[appendix][1][2147483647]A}{[I][5][]V}}
\citation{murphy_conjugate_nodate}
\citation{gelman_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {B}Electronic Appendix}{VII}{appendix.B}\protected@file@percent }
\newlabel{sec:el_app}{{B}{VII}{Electronic Appendix}{appendix.B}{}}
\newlabel{sec:el_app@cref}{{[appendix][2][2147483647]B}{[I][7][]VII}}
\bibstyle{dcu}
\bibdata{bibliography}
\bibcite{albert_bayesian_1993}{{1}{1993}{{Albert and\ Chib}}{{}}}
\bibcite{angelopoulos_gentle_2022}{{2}{2022}{{Angelopoulos and\ Bates}}{{}}}
\bibcite{barbieri_posterior_2015}{{3}{2015}{{Barbieri}}{{}}}
\bibcite{box_sampling_1980}{{4}{1980}{{Box}}{{}}}
\bibcite{brms_2017}{{5}{2017}{{Bürkner}}{{}}}
\bibcite{carvalho_horseshoe_2010}{{6}{2010}{{Carvalho et~al.}}{{Carvalho, Polson and\ Scott}}}
\bibcite{celeux_regularization_2012}{{7}{2012}{{Celeux et~al.}}{{Celeux, Anbari, Marin and\ Robert}}}
\bibcite{chien_informative_2023}{{8}{2023}{{Chien et~al.}}{{Chien, Zhou, Hanson and\ Lystig}}}
\bibcite{dayanik_constructing_2006}{{9}{2006}{{Dayanik et~al.}}{{Dayanik, Lewis, Madigan, Menkov and\ Genkin}}}
\bibcite{dellaportas_bayesian_1993}{{10}{1993}{{Dellaportas and\ Smith}}{{}}}
\bibcite{fahrmeir_bayesian_2010}{{11}{2010}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib and\ Konrath}}}
\bibcite{fahrmeir_regression_2021}{{12}{2021}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib, Lang and\ Marx}}}
\bibcite{fruhwirth-schnatter_auxiliary_2007}{{13}{2007}{{Frühwirth-Schnatter and\ Frühwirth}}{{}}}
\bibcite{gamerman_markov_1998}{{14}{1998}{{Gamerman}}{{}}}
\bibcite{gelman_bayesian_2013}{{15}{2013}{{Gelman, Carlin, Stern, Dunson, Vehtari and\ Rubin}}{{}}}
\bibcite{gelman_understanding_2013}{{16}{2013}{{Gelman, Hwang and\ Vehtari}}{{}}}
\bibcite{gelman_weakly_2008}{{17}{2008}{{Gelman et~al.}}{{Gelman, Jakulin, Pittau and\ Su}}}
\bibcite{ghosh_use_2017}{{18}{2017}{{Ghosh et~al.}}{{Ghosh, Li and\ Mitra}}}
\bibcite{grammarly}{{19}{2025}{{Grammarly Inc.}}{{}}}
\bibcite{MCMCglmm_2010}{{20}{2010}{{Hadfield}}{{}}}
\bibcite{hastings_monte_1970}{{21}{1970}{{Hastings}}{{}}}
\bibcite{held_likelihood_2020}{{22}{2020}{{Held and\ Sabanés~Bové}}{{}}}
\bibcite{hoerl_ridge_1970}{{23}{1970a}{{Hoerl and\ Kennard}}{{}}}
\bibcite{hoerl_ridge_1970-1}{{24}{1970b}{{Hoerl and\ Kennard}}{{}}}
\bibcite{holmes_efficient_nodate}{{25}{2003}{{Holmes and\ Knorr-Held}}{{}}}
\bibcite{hosack_prior_2017}{{26}{2017}{{Hosack et~al.}}{{Hosack, Hayes and\ Barry}}}
\bibcite{hsiang_bayesian_1975}{{27}{1975}{{Hsiang}}{{}}}
\bibcite{lenk_bayesian_2000}{{28}{2000}{{Lenk and\ DeSarbo}}{{}}}
\bibcite{mackay_bayesian_1992}{{29}{1992}{{MacKay}}{{}}}
\bibcite{makalic_bayesreg_2016}{{30}{2016}{{Makalic and\ Schmidt}}{{}}}
\bibcite{mitchell_bayesian_1988}{{31}{1988}{{Mitchell and\ Beauchamp}}{{}}}
\bibcite{murphy_conjugate_nodate}{{32}{2007}{{Murphy}}{{}}}
\bibcite{neal_probabilistic_1993}{{33}{1993}{{Neal}}{{}}}
\bibcite{nelder_generalized_1972}{{34}{1972}{{Nelder and\ Wedderburn}}{{}}}
\bibcite{ohara_review_2009}{{35}{2009}{{O'Hara and\ Sillanpää}}{{}}}
\bibcite{chatgpt}{{36}{2025}{{OpenAI}}{{}}}
\bibcite{park_bayesian_2008}{{37}{2008}{{Park and\ Casella}}{{}}}
\bibcite{piironen_comparison_2017}{{38}{2017}{{Piironen and\ Vehtari}}{{}}}
\bibcite{polson_bayesian_2013}{{39}{2013}{{Polson et~al.}}{{Polson, ~, and\ Windle}}}
\bibcite{rue_approximate_2009}{{40}{2009}{{Rue et~al.}}{{Rue, Martino and\ Chopin}}}
\bibcite{scott_data_2011}{{41}{2011}{{Scott}}{{}}}
\bibcite{sondhi_bayesian_2021}{{42}{2021}{{Sondhi et~al.}}{{Sondhi, Segal, Snider, Humblet and\ McCusker}}}
\bibcite{tibshirani_regression_1996}{{43}{1996}{{Tibshirani}}{{}}}
\bibcite{tierney_accurate_1986}{{44}{1986}{{Tierney et~al.}}{{Tierney, and\ Kadane}}}
\bibcite{tyralis_review_2024}{{45}{2024}{{Tyralis and\ Papacharalampous}}{{}}}
\bibcite{van_erp_shrinkage_2019}{{46}{2019}{{van Erp et~al.}}{{van Erp, Oberski and\ Mulder}}}
\bibcite{west_dynamic_1985}{{47}{1985}{{West et~al.}}{{West, ~, and\ Migon}}}
\bibcite{zellner_assessing_1986}{{48}{1986}{{Zellner}}{{}}}
\citation{grammarly}
\citation{chatgpt}
\citation{tibshirani_regression_1996}
\gdef \@abspage@last{23}
