\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sondhi_bayesian_2021}
\citation{van_erp_shrinkage_2019,celeux_regularization_2012}
\citation{chien_informative_2023}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{fahrmeir_regression_2021}
\citation{gelman_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian Linear Model}{2}{section.2}\protected@file@percent }
\newlabel{sec:bayesian_lm}{{2}{2}{Bayesian Linear Model}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model definition}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:LM}{{1}{2}{Model definition}{equation.2.1}{}}
\newlabel{eq:BLM}{{2}{2}{Model definition}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Prior choice}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:NIGprior}{{3}{2}{Normal (Inverse Gamma) Prior}{equation.2.3}{}}
\citation{zellner_assessing_1986}
\citation{hoerl_ridge_1970,hoerl_ridge_1970-1}
\citation{hsiang_bayesian_1975,mackay_bayesian_1992}
\newlabel{eq:flat-prior}{{4}{3}{Uninformative Prior}{equation.2.4}{}}
\newlabel{sec:lm-regularization}{{2.2}{3}{Regularization Priors}{section*.4}{}}
\citation{tibshirani_regression_1996}
\citation{park_bayesian_2008}
\citation{mitchell_bayesian_1988}
\citation{carvalho_horseshoe_2010}
\citation{held_likelihood_2020}
\newlabel{eq:ridge}{{5}{4}{Regularization Priors}{equation.2.5}{}}
\newlabel{eq:lasso}{{6}{4}{Regularization Priors}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Bayesian inference with closed form priors}{4}{subsection.2.3}\protected@file@percent }
\newlabel{eq:LSE}{{7}{4}{Parameter posterior distribution}{equation.2.7}{}}
\citation{park_bayesian_2008}
\citation{box_sampling_1980,barbieri_posterior_2015}
\citation{murphy_conjugate_nodate}
\newlabel{eq:NIGpost}{{8}{5}{Parameter posterior distribution}{equation.2.8}{}}
\citation{nelder_generalized_1972,west_dynamic_1985}
\citation{west_dynamic_1985}
\citation{gelman_weakly_2008}
\citation{ghosh_use_2017}
\citation{van_erp_shrinkage_2019,fahrmeir_bayesian_2010,ohara_review_2009}
\@writefile{toc}{\contentsline {section}{\numberline {3}Bayesian Logistic Model}{6}{section.3}\protected@file@percent }
\newlabel{sec:bayesian_logit}{{3}{6}{Bayesian Logistic Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Generalized Linear Regression Model}{6}{subsection.3.1}\protected@file@percent }
\newlabel{sec:logit-glm}{{3.1}{6}{Bayesian Generalized Linear Regression Model}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Bayesian Logistic Regression Model}{6}{subsection.3.2}\protected@file@percent }
\newlabel{sec:logit-logit}{{3.2}{6}{Bayesian Logistic Regression Model}{subsection.3.2}{}}
\newlabel{eq:logit}{{9}{6}{Model definition}{equation.3.9}{}}
\citation{hastings_monte_1970}
\citation{gamerman_markov_1998,lenk_bayesian_2000,scott_data_2011}
\citation{scott_data_2011}
\citation{dellaportas_bayesian_1993}
\citation{neal_probabilistic_1993}
\citation{albert_bayesian_1993}
\citation{holmes_efficient_nodate,fruhwirth-schnatter_auxiliary_2007,scott_data_2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Approximate Bayesian inference}{7}{subsection.3.3}\protected@file@percent }
\newlabel{sec:logit-inf}{{3.3}{7}{Approximate Bayesian inference}{subsection.3.3}{}}
\citation{tierney_accurate_1986}
\citation{rue_approximate_2009}
\newlabel{eq:ppd-sample}{{10}{8}{Posterior predictive distribution}{equation.3.10}{}}
\citation{van_erp_shrinkage_2019}
\citation{gelman_understanding_2013}
\@writefile{toc}{\contentsline {section}{\numberline {4}Illustrative Examples}{9}{section.4}\protected@file@percent }
\newlabel{sec:simulation}{{4}{9}{Illustrative Examples}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Regularization and variable selection}{9}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Evaluation metrics under scenarios A and B. For linear regression, all priors correctly identified the influential variables and produced few or no false positives. The effect of regularization is more pronounced in logistic regression: in both scenarios, regularized models (lasso and ridge) declared fewer coefficients as influential and reduced false positives. In the low-information Scenario B, regularization priors improved both variable selection and predictive accuracy (MLPPD). Except under the flat prior, Bayesian logistic regression achieved slightly better predictive performance than linear regression, though differences in MLPPD between priors were small. \relax }}{10}{table.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:reg_AB}{{1}{10}{Evaluation metrics under scenarios A and B. For linear regression, all priors correctly identified the influential variables and produced few or no false positives. The effect of regularization is more pronounced in logistic regression: in both scenarios, regularized models (lasso and ridge) declared fewer coefficients as influential and reduced false positives. In the low-information Scenario B, regularization priors improved both variable selection and predictive accuracy (MLPPD). Except under the flat prior, Bayesian logistic regression achieved slightly better predictive performance than linear regression, though differences in MLPPD between priors were small. \relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Performance of approximate inference algorithms in Bayesian regression}{10}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Estimated model parameters with 95\% credibility intervals (CI). True parameter values are shown as black crosses. Linear models yielded more accurate estimates with narrower CIs than logistic models. In Scenario B, the uncertainty of the unregularized logistic model becomes especially pronounced, illustrating the need for regularization in high-dimensional, low-information settings. \relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:reg-params}{{1}{11}{Estimated model parameters with 95\% credibility intervals (CI). True parameter values are shown as black crosses. Linear models yielded more accurate estimates with narrower CIs than logistic models. In Scenario B, the uncertainty of the unregularized logistic model becomes especially pronounced, illustrating the need for regularization in high-dimensional, low-information settings. \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Bayesian linear regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. MCMC is faster in this setting. Both methods estimate parameters accurately, though LA yields higher posterior uncertainty. \relax }}{11}{figure.caption.14}\protected@file@percent }
\newlabel{fig:approx-regr}{{2}{11}{Bayesian linear regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. MCMC is faster in this setting. Both methods estimate parameters accurately, though LA yields higher posterior uncertainty. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Bayesian logistic regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. LA outperforms MCMC in both accuracy and precision of parameter estimates. While LA is slower to compute, it provides more stable estimates. \relax }}{11}{figure.caption.15}\protected@file@percent }
\newlabel{fig:approx-class}{{3}{11}{Bayesian logistic regression: Comparison of LA (red) and MCMC (blue) across 1,000 simulations. LA outperforms MCMC in both accuracy and precision of parameter estimates. While LA is slower to compute, it provides more stable estimates. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Outlook}{12}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Conclusion and Outlook}{section.5}{}}
\citation{fahrmeir_regression_2021}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{V}{appendix.A}\protected@file@percent }
\newlabel{sec:app}{{A}{V}{Appendix}{appendix.A}{}}
\citation{murphy_conjugate_nodate}
\citation{gelman_bayesian_2013}
\@writefile{toc}{\contentsline {section}{\numberline {B}Electronic appendix}{VII}{appendix.B}\protected@file@percent }
\newlabel{sec:el_app}{{B}{VII}{Electronic appendix}{appendix.B}{}}
\bibstyle{dcu}
\bibdata{bibliography}
\bibcite{albert_bayesian_1993}{{1}{1993}{{Albert and\ Chib}}{{}}}
\bibcite{barbieri_posterior_2015}{{2}{2015}{{Barbieri}}{{}}}
\bibcite{box_sampling_1980}{{3}{1980}{{Box}}{{}}}
\bibcite{carvalho_horseshoe_2010}{{4}{2010}{{Carvalho et~al.}}{{Carvalho, Polson and\ Scott}}}
\bibcite{celeux_regularization_2012}{{5}{2012}{{Celeux et~al.}}{{Celeux, Anbari, Marin and\ Robert}}}
\bibcite{chien_informative_2023}{{6}{2023}{{Chien et~al.}}{{Chien, Zhou, Hanson and\ Lystig}}}
\bibcite{dellaportas_bayesian_1993}{{7}{1993}{{Dellaportas and\ Smith}}{{}}}
\bibcite{fahrmeir_bayesian_2010}{{8}{2010}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib and\ Konrath}}}
\bibcite{fahrmeir_regression_2021}{{9}{2021}{{Fahrmeir et~al.}}{{Fahrmeir, Kneib, Lang and\ Marx}}}
\bibcite{fruhwirth-schnatter_auxiliary_2007}{{10}{2007}{{Frühwirth-Schnatter and\ Frühwirth}}{{}}}
\bibcite{gamerman_markov_1998}{{11}{1998}{{Gamerman}}{{}}}
\bibcite{gelman_bayesian_2013}{{12}{2013}{{Gelman, Carlin, Stern, Dunson, Vehtari and\ Rubin}}{{}}}
\bibcite{gelman_understanding_2013}{{13}{2013}{{Gelman, Hwang and\ Vehtari}}{{}}}
\bibcite{gelman_weakly_2008}{{14}{2008}{{Gelman et~al.}}{{Gelman, Jakulin, Pittau and\ Su}}}
\bibcite{ghosh_use_2017}{{15}{2017}{{Ghosh et~al.}}{{Ghosh, Li and\ Mitra}}}
\bibcite{hastings_monte_1970}{{16}{1970}{{Hastings}}{{}}}
\bibcite{held_likelihood_2020}{{17}{2020}{{Held and\ Sabanés~Bové}}{{}}}
\bibcite{hoerl_ridge_1970}{{18}{1970a}{{Hoerl and\ Kennard}}{{}}}
\bibcite{hoerl_ridge_1970-1}{{19}{1970b}{{Hoerl and\ Kennard}}{{}}}
\bibcite{holmes_efficient_nodate}{{20}{n.d.}{{Holmes}}{{}}}
\bibcite{hsiang_bayesian_1975}{{21}{1975}{{Hsiang}}{{}}}
\bibcite{lenk_bayesian_2000}{{22}{2000}{{Lenk and\ DeSarbo}}{{}}}
\bibcite{mackay_bayesian_1992}{{23}{1992}{{MacKay}}{{}}}
\bibcite{mitchell_bayesian_1988}{{24}{1988}{{Mitchell and\ Beauchamp}}{{}}}
\bibcite{murphy_conjugate_nodate}{{25}{n.d.}{{Murphy}}{{}}}
\bibcite{neal_probabilistic_1993}{{26}{1993}{{Neal}}{{}}}
\bibcite{nelder_generalized_1972}{{27}{1972}{{Nelder and\ Wedderburn}}{{}}}
\bibcite{ohara_review_2009}{{28}{2009}{{O'Hara and\ Sillanpää}}{{}}}
\bibcite{park_bayesian_2008}{{29}{2008}{{Park and\ Casella}}{{}}}
\bibcite{rue_approximate_2009}{{30}{2009}{{Rue et~al.}}{{Rue, Martino and\ Chopin}}}
\bibcite{scott_data_2011}{{31}{2011}{{Scott}}{{}}}
\bibcite{sondhi_bayesian_2021}{{32}{2021}{{Sondhi et~al.}}{{Sondhi, Segal, Snider, Humblet and\ McCusker}}}
\bibcite{tibshirani_regression_1996}{{33}{1996}{{Tibshirani}}{{}}}
\bibcite{tierney_accurate_1986}{{34}{1986}{{Tierney et~al.}}{{Tierney, and\ Kadane}}}
\bibcite{van_erp_shrinkage_2019}{{35}{2019}{{van Erp et~al.}}{{van Erp, Oberski and\ Mulder}}}
\bibcite{west_dynamic_1985}{{36}{1985}{{West et~al.}}{{West, ~, and\ Migon}}}
\bibcite{zellner_assessing_1986}{{37}{1986}{{Zellner}}{{}}}
\gdef \@abspage@last{22}
