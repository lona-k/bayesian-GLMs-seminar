In this paper, we reviewed the model specification, prior choice and approximate inference methods for Bayesian generalized linear models, focusing on regularization and approximation methods.
We detailed how the prior choice can stabilize estimation in difficult settings and how we can implement familiar regularization schemes  in a Bayesian setting.
On the example of logistic regression, we demonstrated the necessity of numerical methods for inference. We explained the two main algorithms, LA and MCMC, which form the basis of modern approximate inference methods for Bayesian GLMs.

In an applied example, we examined regularized Bayesian models under challenging data conditions.
In our synthetic data setting, regularization improved predictive performance and reduced the number of covariates falsely as informative declared, particularly in logistic regression.
In our comparison of MCMC and LA, we saw that LA often provides more accurate and precise estimates in logistic regression despite higher computational cost and lower speed. However, we have to admit that this could also be mitigated by the specific implementation used in the experiment.
Since these examples were meant to be illustrative, the results should not be over-interpreted.
In many cases, more efficient methods for regularization and approximate inference can be used and are readily available.\\

Some limitations of the described methods have been addressed above, such as the need for more complex prior distributions for real variable selection in \autoref{sec:lm-regularization}.
Additionally, we have only examined how Bayesian GLMs work compared to frequentist GLMs for linear relationships in the linear predictor.
For more complex scenarios, the Bayesian framework can be extended to hierarchical GLMs to model random effects or to Generalized Additive Models by including splines.
Although this is mathematically and computationally complex, the concept of regarding an already familiar frequentist concept from a Bayesian perspective is much the same as for the relatively simple case of GLMs.
