The (frequentist) linear regression model is probably the most widely used model in statistics and machine learning.
The frequentist and the Bayesian linear models are described in many introductory texts on statistical modeling, such as \citet{fahrmeir_regression_2021} or \citet{gelman_bayesian_2013}.


\subsection{Model Definition}

We observe an independent and identically distributed (i.i.d.) sample $\bD = (\by, \bX)$ with $n$ observations and $p$ covariates and assume a linear relationship between $\bX$ and $\by$.\footnote{For a detailed explanation of the notation see \Cref{sec:app}.}
The frequentist linear regression model then assumes
\begin{equation} \label{eq:LM}
    \by \sim \Ncal(\bX \btheta, \ssd \bI),
\end{equation}

where the weight parameter $\btheta$ and the variance $\ssd$ are estimated to obtain the fitted model.
A condition on $\bX$ is always implicit.\\

To view linear regression from a Bayesian perspective, we simply reinterpret the parameters as random variables.
Conditioning on $\btheta$ and $\ssd$, the likelihood takes the same form as in \autoref{eq:LM}:
\begin{equation} \label{eq:BLM}
    \by \mid \btheta, \ssd \sim \Ncal(\bX \btheta, \ssd \bI), 
\end{equation}

Note that to predict multiple outputs, an extension to multivariate linear regression is possible.

\subsection{Prior choice}
\subsubsection*{Normal (Inverse Gamma) Prior}

To complete the Bayesian linear model specification, we place conjugate priors on both $\btheta$ and $\ssd$.
\begin{equation} \label{eq:NIGprior}
    \begin{aligned}
        \btheta \mid &\ssd \sim  \Ncal(\mupri, \ssd \Sdpri) \\
        \ssd &\sim \IG(\apri, \bpri),
    \end{aligned}
\end{equation}

where $\mupri, \Sdpri, \apri$ and $\bpri$ are the prior parameters.
We choose a Gaussian prior on $\btheta$ because it is conjugate to the Gaussian likelihood of $\by$.
Since the inverse gamma (IG) distribution of $\ssd$ is conjugate to the Gaussian conditional distribution of $\btheta$, the joint prior of $\btheta$ and $\ssd$
\begin{equation*}
    p(\btheta, \ssd) \overset{\text{Bayes' rule}}{=} p(\btheta \mid \ssd) p(\ssd)
\end{equation*}

follows a normal-inverse-gamma (NIG) distribution.
We can then use Bayes' rule once again to derive the unconditional prior distribution of $\btheta$ as a multivariate Student t-distribution.
\begin{equation*}
    \btheta \sim \Tcal(2 \apri, \mupri, \frac{\apri}{\bpri} \Sdpri)
\end{equation*}

\subsubsection*{Uninformative Prior}
The idea of an uninformative (or flat) prior is to maximize the influence of the data on the posterior in the absence of prior knowledge.
Especially when little to no prior information is available, we can flatten the NIG prior by setting
\begin{equation*}
    \mupri = \bnull, \quad \Sdipri = \bnull \text{  i.e., } \Sdpri \to \infty 
\end{equation*}
and choosing $\apri = - \frac{p}{2}$ and $\bpri = 0$, where $p$ is the number of features in the model.

We can easily see that with this assumption, the prior for $\btheta$ becomes very flat while still retaining the useful qualities from the setup described in \autoref{eq:NIGprior}.

The prior distributional assumptions would then be:
\begin{equation} \label{eq:flat-prior}
    \begin{aligned}
        \btheta \mid \ssd &\overset{a}{\sim}  \Ncal(\mupri, \ssd \infty)\footnotemark, \quad &p(\btheta\mid \ssd) \propto 1\\
        \ssd &\sim \IG(-\frac{p}{2},  0), \quad &p(\ssd) \propto \frac{1}{\ssd}
    \end{aligned}
\end{equation}

\footnotetext{Informally stated for demonstration purposes.}

Note that we generally have to be careful with completely flat priors because they can result in improper posteriors. It is generally necessary to check if the resulting posterior is proper, which is the case here \citep[][]{fahrmeir_regression_2021}.
Another good solution for use cases with little prior knowledge that still require a proper posterior is Zellner's g-prior \citep{zellner_assessing_1986}.

\subsubsection*{Regularization Priors} \label{sec:lm-regularization}

% Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered.
Regularization (or penalization) regulates the trade-off between model complexity and out-of-sample performance, or equivalently bias vs. variance. 
In frequentist statistics, we minimize the penalized least squares criterion (PLS) 
\begin{equation*}
    \text{PLS}(\btheta) = (\by - \bX \btheta)^\top (\by - \bX \btheta) + \lambda \ \text{pen}(\btheta).
\end{equation*}

where $\lambda > 0$ controls the balance of the tradeoff and therefore the strength of regularization.\\

In the Bayesian view, we introduce a regularization prior on $\btheta$.
Concretely:
\begin{equation*}
    \begin{aligned}
        \by \mid \btheta, \ssd &\sim \Ncal(\bX \btheta, \ssd \bI),\footnotemark \\
        \btheta &\sim \text{regularization prior}\\
        \ssd &\sim \IG(\apri, \bpri).
    \end{aligned}
\end{equation*}

\footnotetext{Usually, it does not make sense to regularize the intercept.
To be completely accurate, we would need to separate the intercept from $\btheta$, i.e., split $\btheta$ into $(\theta_0, \btheta'^\top)$ and consequently set $\bX'$ as the design matrix without a column for the intercept.
We would then specify the model as $\by \mid \btheta, \ssd \sim \Ncal(\theta_0 \bI + \bX' \btheta', \ssd \bI)$.
We chose to simplify this and stick to the previously established definitions because we aim for an understandable explanation of the basic concept of Bayesian regularization.}

Although there are many options for regularization priors, we are going to focus on regularization priors that align directly with familiar frequentist penalties.\\

\textbf{Ridge regularization} \citep{hoerl_ridge_1970,hoerl_ridge_1970-1} uses $\text{pen}(\btheta) = \|\btheta\|_2^2$ and the Bayesian analogue \Citep[e.g.][]{hsiang_bayesian_1975,mackay_bayesian_1992} specifies
\begin{equation}\label{eq:ridge}
    \btheta \sim \Ncal(\bnull, \taus \bI),
\end{equation}

with $\taus$ controlling the degree of regularization akin to the role of $\lambda$.
In contrast to $\lambda$, $\taus$ does not need to be set in advance or optimized as a hyperparameter.
We can simply embed it in a hierarchical model by specifying a prior for $\taus$, e.g.\@ $\taus \sim \IG(\apri_\tau, \bpri_\tau)$, and estimate it alongside $\btheta$ and $\ssd$.\\

\textbf{LASSO} (least absolute shrinkage and selection operator) \textbf{regularization} \citep{tibshirani_regression_1996} uses $\text{pen}(\btheta) = \|\btheta\|_1$ to perform variable selection by setting elements $\theta_j$ of $\btheta$ to $0$ during estimation.
This means that LASSO regularization promotes a \textit{sparse} solution.
The Bayesian LASSO specifies a Laplace prior on $\btheta$ via the scale mixture representation \citep{park_bayesian_2008}
\begin{equation}\label{eq:lasso}
    \begin{aligned}
        \btheta \mid \btaus &\sim \Ncal(\bnull, \btaus \bI) \\
        \taus_j &\overset{\text{i.i.d.}}{\sim} \text{Exp}(0.5 \lambda^2), \quad j = 1, \dots, p,
    \end{aligned}
\end{equation}

where the regularization parameter $\lambda^2$ is often given a (hyper-) prior, e.g.\@ $\lambda^2 \sim \text{G}(\apri_\lambda, \bpri_\lambda)$.\\

Because Bayesian LASSO does not promote a sparse solution, discrete-mixture Spike-and-Slab priors \citep{mitchell_bayesian_1988} (which are necessary for categorical covariates) or the heavy-tailed horseshoe prior \citep{carvalho_horseshoe_2010} are preferred for variable selection.

\subsection{Bayesian Inference with Closed Form Priors}

\subsubsection*{Parameter Posterior Distribution}

In a frequentist linear model, we use least-squares (LS) estimation to obtain the estimate
\begin{equation}\label{eq:LSE}
    \hbtheta_{LS} = (\bX^\top \bX)^{-1} \bX^\top \by
\end{equation}

for $\btheta$.
Under Gaussian errors, this satisfies
\begin{equation*}
    \hbtheta_{LS} \sim \Ncal(\btheta, \ssd (\bX^\top \bX)^{-1}).
\end{equation*}

To quantify the uncertainty in the estimation, we can compute confidence intervals for $\btheta$, but these reflect only the variability in the estimator, not uncertainty about the true parameter itself.\\

In contrast, the Bayesian approach yields a full posterior distribution on $\btheta$ by updating the prior distribution with observed data using Bayes' rule.
With the NIG prior introduced in \autoref{eq:NIGprior}, conjugacy implies for the joint posterior $p(\btheta, \ssd \mid \by)$ that
\begin{equation*}
    \btheta, \ssd \mid \by \sim \text{NIG}(\mupo, \Sdpo, \apo, \bpo)
\end{equation*}

with posterior mean and variance \footnotemark
\begin{equation} \label{eq:NIGpost}
        \mupo = \Sdpo (\Sdipri \mupri + \bX^\top \by), \quad \Sdpo = (\bX^\top \bX + \Sdipri)^{-1}.
\end{equation}

\footnotetext{For the full calculation see \Cref{sec:app}}

Integrating out $\ssd$ yields $\btheta \mid \by \sim \Tcal(2 \apo, \mupo, \bpo / \apo \Sdpo)$ and Bayesian credibility intervals can be derived directly from this distribution \citep[see e.g.][on how to do this]{held_likelihood_2020}.\\

Since we defined the non-information prior (\autoref{eq:flat-prior}) as a special case of the NIG-distributed prior, we can use \autoref{eq:NIGpost} to directly calculate the posterior mean and variance as
\begin{equation*}
        \mupo = (\bX^\top \bX)^{-1} \bX^\top \by, \quad
        \Sdpo = \bX^\top \bX.
\end{equation*}

The posterior mean $\mupo$ coincides with $\hbtheta_{LS}$ (\autoref{eq:LSE}), which means that a Bayesian linear model with a non-informative prior converges to the frequentist solution.
More generally, as the prior variance $\Sdpri$ grows, $\mupo$ approaches $\hbtheta_{LS}$, since the likelihood (and thus the data) dominates the posterior.\\

Because Bayesian ridge regression is simply the NIG case of \autoref{eq:NIGprior} with finite $\Sdpo$, inference results in the same posterior update as in \autoref{eq:NIGpost}.
By contrast, the Bayesian LASSO's Laplace prior has no closed form posterior, but we can easily sample from it using Gibbs sampling \citep{park_bayesian_2008}.
We will go more into depth on approximate inference for Bayesian regression models in \Cref{sec:logit-inf}.

\subsubsection*{Posterior Predictive Distribution}
In many applications, we care more about predictions $\ty$ for new, unseen inputs $\tX$ (or test data $(\ty, \tX)$), independent of the training data $\bD$, than about $\btheta$ itself.
The Bayesian answer to this is the \textit{posterior predictive distribution} (PPD) \Citep[see e.g][]{box_sampling_1980,barbieri_posterior_2015}
\begin{equation*}
    p(\ty \mid \by) = \int p(\ty, \btheta \mid \by) d \btheta 
    = \int p(\ty \mid \btheta, \by) p(\btheta) d \btheta
    \overset{\ty \perp \by \mid \btheta}{=}  \int p(\ty \mid \btheta) p(\btheta) d \btheta,
\end{equation*}

which is an average of conditional probabilities over the posterior distribution of $\btheta$.\footnotemark

\footnotetext{
    A note on intuition: In essence, the PPD is the marginal distribution of $\ty$, conditioned on the data $\by$.
    We recognize the marginal distribution of $\by$ from Bayes' rule as the normalization constant, i.e., $p(\by) = \int p(\by, \btheta) d\btheta = \int p(\by \mid \btheta) p(\btheta) d\btheta$.
}

For the NIG prior in \autoref{eq:NIGprior}, one can show\footnote{see \Cref{sec:app}} that
\begin{equation*}
    \ty \mid \btheta, \ssd, \by \sim \Tcal(2 \apo, \tX \btheta, \frac{\bpo}{\apo} (\bI + \tX \Sdpo \tX^\top)).
\end{equation*}

Interestingly, the posterior predictive mean $\mupo = \tX \btheta$ of the t-distribution coincides with the least squares prediction and its scale matrix reflects both observational noise and posterior uncertainty.
Bayesian inference with the Gaussian conjugate is more thoroughly described by \citet{murphy_conjugate_nodate}.\\

If no closed form exists, the PPD can also be simulated (see \Cref{sec:logit-inf}).
