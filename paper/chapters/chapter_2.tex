The (frequentist) Linear Regression Model is probably the most widely used model in statistics and machine learning. Both the frequentist and the Bayesian Linear Models are described in many introductory texts on statistical modelling, such as \citep{fahrmeir_regression_2021} or \citep{gelman_bayesian_2013}.


\subsection{Model definition}

We observe an i.i.d. sample $\bD = ((y_1, \bx_1), \dots, (y_n, \bx_n))= (\by, \bX)$ and assume a linear relationship between $\bX$ and $\by$.
The frequentist linear regression model then assumes

\begin{equation} \label{eq:LM}
    \by \sim \Ncal(\bX \btheta, \ssd \bI),
\end{equation}

where the weight parameter $\btheta$ and the variance $\ssd$ are estimated to obtain the fitted model.
A condition on $\bX$ is always implicit. \\

To view Linear Regression from a Bayesian perspective, we simply reinterpret the parameters as random variables.
Conditioning on $\btheta$ and $\ssd$, the likelihood takes the same form as in \eqref{eq:LM}:

\begin{equation} \label{eq:BLM}
    \by \mid \btheta, \ssd \sim \Ncal(\bX \btheta, \ssd \bI), 
\end{equation}

Note that to predict multiple outputs, an extension to Multivariate Linear Regression is possible.

\subsection{Prior choice}
\subsubsection*{Normal (Inverse Gamma) Prior}

To complete the Bayesian linear model specification, we place conjugate priors on both $\btheta$ and $\ssd$. 

\begin{equation} \label{eq:NIGprior}
    \begin{aligned}
        \btheta \mid &\ssd \sim  \Ncal(\mupri, \ssd \Sdpri) \\
        \ssd &\sim \IG(\apri, \bpri),
    \end{aligned}
\end{equation}

where $\mupri, \Sdpri, \apri$ and $\bpri$ are the prior parameters.
We choose a Gaussian prior on $\btheta$ because it is conjugate to the Gaussian likelihood of $\by$.
Since the Inverse-Gamma distribution of $\ssd$ is conjugate to the Gaussian conditional distribution of $\btheta$, the joint prior of $\btheta$ and $\ssd$

\begin{equation*}
    p(\btheta, \ssd) \overset{\text{Bayes' rule}}{=} p(\btheta \mid \ssd) p(\ssd)
\end{equation*}

follows a Normal Inverse Gamma (NIG).
We can then use Bayes' rule once again to derive the unconditional prior distribution of $\btheta$ as a multivariate Student t-distribution. 

\begin{equation*}
    \btheta \sim \Tcal(2 \apri, \mupri, \frac{\apri}{\bpri} \Sdpri)
\end{equation*}

\subsubsection*{Uninformative Prior}
The idea of an uninformative (or flat) prior is to maximize the influence of the data on the posterior in the absence of prior knowledge. Especially when little to no prior information is available, we can flatten the NIG prior by setting
\begin{equation*}
    \mupri = \bnull, \quad \Sdipri = \bnull \text{ i.e. } \Sdpri \to \infty 
\end{equation*}
and choosing $\apri = - \frac{p}{2}$ and $\bpri = 0$, where $p$ is the number of features in the model.

We can easily see that with this assumption, the prior for $\btheta$ becomes very flat while still retaining the useful qualities from the setup described in \eqref{eq:NIGprior}.

The prior distributional assumptions would then be:

\begin{equation} \label{eq:flat-prior}
    \begin{aligned}
        \btheta \mid \ssd &\overset{a}{\sim}  \Ncal(\mupri, \ssd \infty)\footnotemark, \quad p(\btheta\mid \ssd) &\propto 1\\
        \ssd &\sim \IG(-\frac{p}{2},  0), \quad p(\ssd) &\propto \frac{1}{\ssd}
    \end{aligned}
\end{equation}

\footnotetext{Informally stated for demonstational purposes.}

Note that we generally have to be careful with completely flat priors; it is necessary to check if the resulting posterior is proper (which is the case here). \\

Another good solution for use-cases with little prior knowledge that still require a proper posterior is Zellner's g-prior \citep{zellner_assessing_1986}.

\subsubsection*{Regularization Priors}

Regularization (or penalization) regulates the trade off between model fit and complexity, or equivalently bias vs. variance.
In frequentist statistics, we minimize the Penalized Least Squares criterion (PLS) 

\begin{equation*}
    \text{PLS}(\btheta) = (\by - \bX \btheta)^\top (\by - \bX \btheta) + \lambda \ \text{pen}(\btheta).
\end{equation*}

where $\lambda > 0$ controls the balance of the tradeoff and therefore the strength of regularization.\\

In the Bayesian view, we introduce a regularization prior on $\btheta$.
Concretely:

\begin{equation}
    \begin{aligned}
        \by \mid \btheta, \ssd &\sim \Ncal(\bX \btheta, \ssd \bI),\footnotemark \\
        \btheta &\sim \text{regularization prior}\\
        \ssd &\sim \IG(\apri, \bpri),
    \end{aligned}
\end{equation}

\footnotetext{Usually, it does not make sense to regularize the intercept. To be completely accurate, we would need to separate the intercept from $\btheta$, i.e. split $\btheta$ into $(\theta_0, \btheta'^\top)$ and consequently set $\bX'$ as the design matrix without a column for the intercept. We would then specify the model as $\by \mid \btheta, \ssd \sim \Ncal(\theta_0 \bI + \bX' \btheta', \ssd \bI)$. We chose to simplify this and stick to the previously established definitions because we aim for an understandable explanation of the basic concept of Bayesian regularization.}

although there are many options for regularization priors, we are going to focus on regularization priors that align directly with familiar frequentist penalties.\\

\textbf{Ridge regularization} uses $\text{pen}(\btheta) = \|\btheta\|_2^2$ and the Bayesian analogue specifies

\begin{equation*}
    \btheta \sim \Ncal(\bnull, \taus \bI),
\end{equation*}

with $\taus$ controlling the degree of regularization akin to the role of $\lambda$.
In constrast to $\lambda$, $\taus$ does not need to be set in advance or optimized as a hyperparameter. We can simply embed it in a hierarchical model by specifying a prior for $\taus$, e.g. $\taus \sim \IG(\apri_\tau, \bpri_\tau)$, and estimate it alongside $\btheta$ and $\ssd$.\\

\textbf{Lasso regularization} uses $\text{pen}(\btheta) = \|\btheta\|_1$ to perform variable selection by setting elements $\theta_j$ of $\btheta$ to $0$ during estimation. This means that Lasso regularization promotes a \textit{sparse} solution.
The Bayesian Lasso specifies a Laplace prior on $\btheta$ via the scale-mixture representation \citet{park_bayesian_2008}

\begin{equation}
    \begin{aligned}
        \btheta \mid \btaus &\sim \Ncal(\bnull, \btaus \bI) \\
        \taus &\overset{\text{i.i.d.}}{\sim} \text{Exp}(0.5 \lambda^2), \quad j = 1, \dots, p,
    \end{aligned}
\end{equation}

where the regularization parameter $\lambda^2$ is often given a (hyper-) prior, e.g. $\lambda^2 \sim \text{G}(\apri_\lambda, \bpri_\lambda)$.\\

Because Bayesian Lasso does not promote a sparse solution, discrete-mixture Spike-and-Slab priors \citep{mitchell_bayesian_1988} or the heavy-tailed horseshoe prior \citep{carvalho_horseshoe_2010} are preferred for variable selection.

\subsection{Bayesian inference with closed form priors}

\subsubsection*{Parameter posterior distribution}

In a frequentist linear model, we use least-squares (LS) estimation to obtain the estimate
\begin{equation*}
    \hbtheta_{LS} = (\bX^\top \bX)^{-1} \bX^\top \by
\end{equation*}

for $\btheta$. Under Gaussian errors, this satisfies
\begin{equation}\label{LSE}
    \hbtheta_{LS} \sim \Ncal(\btheta, \ssd (\bX^\top \bX)^{-1}).
\end{equation}

To quantify the uncertainty in the estimation, we can compute confidence intervals for $\btheta$, but these reflect only the variability in the estimator, not uncertainty about the true parameter itself.\\

In contrast, the Bayesian approach yields a full posterior distribution on $\btheta$ by updating the prior distribution with observed data using Bayes' rule. With the NIG prior introduced in \eqref{eq:NIGprior}, conjugacy implies for the joint posterior $p(\btheta, \ssd \mid \by)$ that

\begin{equation*}
    \btheta, \ssd \mid \by \sim \text{NIG}(\mupo, \Sdpo, \apo, \bpo)
\end{equation*}

with posterior mean and variance \footnotemark

\begin{equation} \label{eq:NIGpost}
        \mupo = \Sdpo (\Sdipri \mupri + \bX^\top \by), \quad \Sdpo = (\bX^\top \bX + \Sdipri)^{-1}.
\end{equation}

\footnotetext{For the full calculation see Appendix \ref{app}}

Integrating out $\ssd$ yields $\btheta \mid \by \sim \Tcal(2 \apo, \mupo, \bpo / \apo \Sdpo)$ and Bayesian credibility intervals can be derived directly from this distribution \citep{held_likelihood_2020}.\\

Since we defined the non-information prior \eqref{eq:flat-prior} as a special case of the NIG-distributed prior, we can use \eqref{eq:NIGpost} to directly calculate the posterior mean and variance as

\begin{equation*}
        \mupo = (\bX^\top \bX)^{-1} \bX^\top \by, \quad
        \Sdpo = \bX^\top \bX.
\end{equation*}

The posterior mean $\mupo$ coincides with $\hbbeta_{LS}$ (\eqref{eq:LSE}), so a Bayesian linear model with a non-informative prior converges to the frequentist solution.
More generally, as the prior variance $\Sdpri$ grows, $\mupo$ approaches $\hbbeta_{LS}$, since the likelihood (and thus the data) dominates the posterior.\\

Bayesian Ridge regression is simply the NIG case in \eqref{eq:NIGprior} with finite $\Sdpo$, resulting in the same posterior update in \eqref{eq:NIGpost}.
By contrast, the Bayesian Lasso's Laplace prior has no closed-form posterior, but we can easily sample from it using Gibbs sampling \citet{park_bayesian_2008}.
We will go more into depth on approximate inference for Bayesian regression models in Section \ref{bayesian_logit}.

\subsubsection*{Posterior predictive distribution}
In many applications we care more about predictions $\ty$ for new, unseen inputs $\tX$ (or test data $(\ty, \tX)$), independent of the training data $\bD$ than about $\btheta$ itself.
The Bayesian answer to this is the \textit{posterior predictive distribution}

\begin{equation*}
    p(\ty \mid \by) = \int p(\ty, \btheta \mid \by) d \btheta 
    = \int p(\ty \mid \btheta, \by) p(\btheta) d \btheta
    \overset{\ty \perp \by \mid \btheta}{=}  \int p(\ty \mid \btheta) p(\btheta) d \btheta,
\end{equation*}

which is an average of conditional probabilities over the posterior distribution of $\btheta$.\footnotemark

\footnotetext{
    A note on intuition: In essence, the posterior predictive distribution is the marginal distribution of $\ty$, only for new response values $\ty$, i.e. $p(\by) = \int p(\by, \btheta) d\btheta = \int p(\by \mid \btheta) p(\btheta) d\btheta$. We recognize it from Bayes' rule as the normalization constant.
}

For the NIG prior in \eqref{eq:NIGprior}, one can show\footnote{see Appendix \ref{app}} that

\begin{equation*}
    \ty \mid \btheta, \ssd, \by \sim \Tcal(2 \apo, \tX \btheta, \frac{\bpo}{\apo} (\bI + \tX \Sdpo \tX^\top)).
\end{equation*}

Interestingly, the posterior predictive mean $\mupo = \tX \btheta$ of the t-distribution coincides with the least squares prediction and its scale matrix reflects both observational noise and posterior uncertainty.\\

If no closed form exists, the posterior predictive distribution can also be simulated.
