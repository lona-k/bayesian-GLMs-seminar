Generalized linear models (GLMs) are a fundamental tool in statistics and machine learning and are widely applied across various domains.
Their appeal lies in their simplicity, interpretability, and extensibility \citep{nelder_generalized_1972}.
However, GLMs also come with limitations:
They rely on maximum likelihood estimation (MLE), offer no mechanism for incorporating prior information or stabilizing inference \citep{gelman_weakly_2008}, and most importantly, only produce point estimates and thus cannot capture the full range of uncertainty in predictions and parameter estimates \citep{tyralis_review_2024}.\\

Bayesian GLMs take on a different viewpoint to addresses these shortcomings.
They offer a natural way to quantify uncertainty with posterior distributions, which is especially useful in data-scarce scenarios.
For instance, \citet{sondhi_bayesian_2021} demonstrate this in precision oncology, where Bayesian inference compensates for small sample sizes and stabilizes confidence estimation in effect sizes.
Recent work has also shown that Bayesian regularization techniques can perform on par with or even outperform classic regularization, while also offering greater flexibility and interpretability \citep{van_erp_shrinkage_2019,celeux_regularization_2012}.
Additionally, a Bayesian framework allows for the incorporation of domain knowledge through informative priors.
For example, \citet{chien_informative_2023} outline a framework for constructing priors directly from expert knowledge or prior experiments.\\

This paper explores Bayesian GLMs as an alternative to classical frequentist approaches.
In \Cref{sec:bayesian_lm}, we introduce Bayesian linear regression as a familiar starting point within the Bayesian framework.
\Cref{sec:bayesian_logit} extends this foundation to generalized models, focusing on logistic regression as the most-used GLM.
\Cref{sec:simulation} illustrates the application of regularization and approximate inference methods in Bayesian GLMs using synthetic data experiments.\\



