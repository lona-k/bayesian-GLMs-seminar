
@book{murphy_probabilistic_2023,
	title = {Probabilistic {Machine} {Learning}: {Advanced} {Topics}},
	abstract = {An advanced counterpart to Probabilistic Machine Learning: An Introduction, this high-level textbook provides researchers and graduate students detailed cove...},
	language = {en-US},
	urldate = {2025-05-22},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2023},
	keywords = {bayes. GLMM, bayes. LM, bayes. Logit, read, textbook, useful},
	file = {PDF:/Users/lona/Zotero/storage/R84B5RIV/Probabilistic Machine Learning Advanced Topics.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/BCXL9XTT/probabilistic-machine-learning.html:text/html},
}

@misc{krause_probabilistic_2025,
	title = {Probabilistic {Artificial} {Intelligence}},
	doi = {10.48550/arXiv.2502.05244},
	abstract = {Artificial intelligence commonly refers to the science and engineering of artificial systems that can carry out tasks generally associated with requiring aspects of human intelligence, such as playing games, translating languages, and driving cars. In recent years, there have been exciting advances in learning-based, data-driven approaches towards AI, and machine learning and deep learning have enabled computer systems to perceive the world in unprecedented ways. Reinforcement learning has enabled breakthroughs in complex games such as Go and challenging robotics tasks such as quadrupedal locomotion. A key aspect of intelligence is to not only make predictions, but reason about the uncertainty in these predictions, and to consider this uncertainty when making decisions. This is what this manuscript on "Probabilistic Artificial Intelligence" is about. The first part covers probabilistic approaches to machine learning. We discuss the differentiation between "epistemic" uncertainty due to lack of data and "aleatoric" uncertainty, which is irreducible and stems, e.g., from noisy observations and outcomes. We discuss concrete approaches towards probabilistic inference and modern approaches to efficient approximate inference. The second part of the manuscript is about taking uncertainty into account in sequential decision tasks. We consider active learning and Bayesian optimization -- approaches that collect data by proposing experiments that are informative for reducing the epistemic uncertainty. We then consider reinforcement learning and modern deep RL approaches that use neural network function approximation. We close by discussing modern approaches in model-based RL, which harness epistemic and aleatoric uncertainty to guide exploration, while also reasoning about safety.},
	language = {en},
	urldate = {2025-05-22},
	publisher = {arXiv},
	author = {Krause, Andreas and Hübotter, Jonas},
	month = feb,
	year = {2025},
	note = {arXiv:2502.05244 [cs]},
	keywords = {bayes. LM, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, not super useful, plate notation, read},
	file = {PDF:/Users/lona/Zotero/storage/4CFFHWB5/Krause and Hübotter - 2025 - Probabilistic Artificial Intelligence.pdf:application/pdf},
}

@incollection{fahrmeir_extensions_2021,
	address = {Berlin, Heidelberg},
	title = {Extensions of the {Classical} {Linear} {Model}},
	isbn = {978-3-662-63882-8},
	abstract = {This chapter discusses several extensions of the classical linear model. We first describe the general linear model and its applications in Sect. 4.1.},
	language = {en},
	urldate = {2025-05-23},
	booktitle = {Regression: {Models}, {Methods} and {Applications}},
	publisher = {Springer},
	author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian D.},
	editor = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian D.},
	year = {2021},
	doi = {10.1007/978-3-662-63882-8_4},
	keywords = {textbook},
	pages = {191--282},
	file = {Full Text PDF:/Users/lona/Zotero/storage/KZAXIUB7/Fahrmeir et al. - 2021 - Extensions of the Classical Linear Model.pdf:application/pdf},
}

@book{gelman_bayesian_2013,
	address = {New York},
	edition = {3},
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-0-429-11307-9},
	abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian AnalysisNow in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	month = nov,
	year = {2013},
	doi = {10.1201/b16018},
	keywords = {textbook, bayes. LM, bayes. Logit, useful, todo, skimmed},
	file = {PDF:/Users/lona/Zotero/storage/UT85HZN9/Gelman et al. - 2013 - Bayesian Data Analysis.pdf:application/pdf},
}

@article{tyralis_review_2024,
	title = {A review of predictive uncertainty estimation with machine learning},
	volume = {57},
	issn = {1573-7462},
	doi = {10.1007/s10462-023-10698-8},
	abstract = {Predictions and forecasts of machine learning models should take the form of probability distributions, aiming to increase the quantity of information communicated to end users. Although applications of probabilistic prediction and forecasting with machine learning models in academia and industry are becoming more frequent, related concepts and methods have not been formalized and structured under a holistic view of the entire field. Here, we review the topic of predictive uncertainty estimation with machine learning algorithms, as well as the related metrics (consistent scoring functions and proper scoring rules) for assessing probabilistic predictions. The review covers a time period spanning from the introduction of early statistical (linear regression and time series models, based on Bayesian statistics or quantile regression) to recent machine learning algorithms (including generalized additive models for location, scale and shape, random forests, boosting and deep learning algorithms) that are more flexible by nature. The review of the progress in the field, expedites our understanding on how to develop new algorithms tailored to users’ needs, since the latest advancements are based on some fundamental concepts applied to more complex algorithms. We conclude by classifying the material and discussing challenges that are becoming a hot topic of research.},
	language = {en},
	number = {4},
	urldate = {2025-05-30},
	journal = {Artificial Intelligence Review},
	author = {Tyralis, Hristos and Papacharalampous, Georgia},
	month = mar,
	year = {2024},
	keywords = {Boosting, Deep learning, Distributional regression, Ensemble learning, inference, Learning algorithms, Machine learning, Machine Learning, Predictive markers, Predictive medicine, Probabilistic forecasting, Probability and Statistics in Computer Science, Quantile regression, Random forests, review, Statistical Learning, todo},
	pages = {94},
	file = {Full Text PDF:/Users/lona/Zotero/storage/YBWPVZAB/Tyralis and Papacharalampous - 2024 - A review of predictive uncertainty estimation with machine learning.pdf:application/pdf},
}

@incollection{barbieri_posterior_2015,
	title = {Posterior {Predictive} {Distribution}},
	copyright = {Copyright © 2015 John Wiley \& Sons, Ltd. All rights reserved.},
	isbn = {978-1-118-44511-2},
	abstract = {The posterior predictive distribution is the distribution of future observations, conditioned on the information available from existing observations. It is the main Bayesian tool for treating predictive problems in statistics. We define the posterior predictive distribution and illustrate its main features in Bayesian parametric inference. We also focus on predictive model checking and selection, which are procedures for checking model adequacy and for selecting a model, when the analysis is based on a posterior predictive approach.},
	language = {en},
	urldate = {2025-05-30},
	booktitle = {Wiley {StatsRef}: {Statistics} {Reference} {Online}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Barbieri, Maria Maddalena},
	year = {2015},
	doi = {10.1002/9781118445112.stat07839},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07839},
	keywords = {Bayesian inference, cross-validation, decision theory, predictive model checking, predictive model selection},
	pages = {1--6},
	file = {PDF:/Users/lona/Zotero/storage/SKNSS89Z/Barbieri - 2015 - Posterior Predictive Distribution.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/EFSYU779/9781118445112.html:text/html},
}

@article{fahrmeir_bayesian_2010,
	title = {Bayesian regularisation in structured additive regression: a unifying perspective on shrinkage, smoothing and predictor selection},
	volume = {20},
	issn = {1573-1375},
	shorttitle = {Bayesian regularisation in structured additive regression},
	doi = {10.1007/s11222-009-9158-3},
	abstract = {This paper surveys various shrinkage, smoothing and selection priors from a unifying perspective and shows how to combine them for Bayesian regularisation in the general class of structured additive regression models. As a common feature, all regularisation priors are conditionally Gaussian, given further parameters regularising model complexity. Hyperpriors for these parameters encourage shrinkage, smoothness or selection. It is shown that these regularisation (log-) priors can be interpreted as Bayesian analogues of several well-known frequentist penalty terms. Inference can be carried out with unified and computationally efficient MCMC schemes, estimating regularised regression coefficients and basis function coefficients simultaneously with complexity parameters and measuring uncertainty via corresponding marginal posteriors. For variable and function selection we discuss several variants of spike and slab priors which can also be cast into the framework of conditionally Gaussian priors. The performance of the Bayesian regularisation approaches is demonstrated in a hazard regression model and a high-dimensional geoadditive regression model.},
	language = {en},
	number = {2},
	urldate = {2025-05-30},
	journal = {Statistics and Computing},
	author = {Fahrmeir, Ludwig and Kneib, Thomas and Konrath, Susanne},
	month = apr,
	year = {2010},
	keywords = {bayes. LM, skimmed, Statistical Learning, review, Bayesian Inference, Conditionally Gaussian priors, lasso, Linear Models and Regression, MCMC, Non-parametric Inference, P-splines, Parametric Inference, Spike and slab prior, Stochastic Modelling, Structured additive regression},
	pages = {203--219},
	file = {Full Text PDF:/Users/lona/Zotero/storage/HMF7DI6F/Fahrmeir et al. - 2010 - Bayesian regularisation in structured additive regression a unifying perspective on shrinkage, smoo.pdf:application/pdf},
}

@article{agresti_bayesian_2005,
	title = {Bayesian inference for categorical data analysis},
	volume = {14},
	issn = {1613-981X},
	doi = {10.1007/s10260-005-0121-y},
	abstract = {This article surveys Bayesian methods for categorical data analysis, with primary emphasis on contingency table analysis. Early innovations were proposed by Good (1953, 1956, 1965) for smoothing proportions in contingency tables and by Lindley (1964) for inference about odds ratios. These approaches primarily used conjugate beta and Dirichlet priors. Altham (1969, 1971) presented Bayesian analogs of small-sample frequentist tests for 2 x 2 tables using such priors. An alternative approach using normal priors for logits received considerable attention in the 1970s by Leonard and others (e.g., Leonard 1972). Adopted usually in a hierarchical form, the logit-normal approach allows greater flexibility and scope for generalization. The 1970s also saw considerable interest in loglinear modeling. The advent of modern computational methods since the mid-1980s has led to a growing literature on fully Bayesian analyses with models for categorical data, with main emphasis on generalized linear models such as logistic regression for binary and multi-category response variables.},
	language = {en},
	number = {3},
	urldate = {2025-05-30},
	journal = {Statistical Methods and Applications},
	author = {Agresti, Alan and Hitchcock, David B.},
	month = dec,
	year = {2005},
	keywords = {bayes. Logit, review, Bayesian Inference, Non-parametric Inference, Parametric Inference, Applied Statistics, Beta distribution, Binomial distribution, Dirichlet distribution, Empirical Bayes, Graphical models, Hierarchical models, History of Statistics, Logistic regression, Loglinear models, Markov chain Monte Carlo, Matched pairs, Multinomial distribution, Odds ratio, Smoothing, Statistical Theory and Methods},
	pages = {297--330},
	file = {Full Text PDF:/Users/lona/Zotero/storage/LCTELGFX/Agresti and Hitchcock - 2005 - Bayesian inference for categorical data analysis.pdf:application/pdf},
}

@article{ntzoufras_bayesian_2003,
	series = {Special issue {I}: {Model} {Selection}, {Model} {Diagnostics}, {Empirical} {Ba} yes and {Hierarchical} {Bayes}},
	title = {Bayesian variable and link determination for generalised linear models},
	volume = {111},
	issn = {0378-3758},
	doi = {10.1016/S0378-3758(02)00298-7},
	abstract = {In this paper, we describe full Bayesian inference for generalised linear models where uncertainty exists about the structure of the linear predictor, the linear parameters and the link function. Choice of suitable prior distributions is discussed in detail and we propose an efficient reversible jump Markov chain Monte-Carlo algorithm for calculating posterior summaries. We illustrate our method with two data examples.},
	number = {1},
	urldate = {2025-05-30},
	journal = {Journal of Statistical Planning and Inference},
	author = {Ntzoufras, Ioannis and Dellaportas, Petros and Forster, Jonathan J},
	month = feb,
	year = {2003},
	keywords = {bayes. Logit, MCMC, Logistic regression, Markov chain Monte-Carlo, Reversible jump, bayes. GLM},
	pages = {165--180},
	file = {PDF:/Users/lona/Zotero/storage/ANSNT896/Ntzoufras et al. - 2003 - Bayesian variable and link determination for generalised linear models.pdf:application/pdf;ScienceDirect Snapshot:/Users/lona/Zotero/storage/EH5IB9B9/S0378375802002987.html:text/html},
}

@article{clyde_model_2004,
	title = {Model {Uncertainty}},
	volume = {19},
	issn = {0883-4237, 2168-8745},
	doi = {10.1214/088342304000000035},
	abstract = {The evolution of Bayesian approaches for model uncertainty over the past decade has been remarkable. Catalyzed by advances in methods and technology for posterior computation, the scope of these methods has widened substantially. Major thrusts of these developments have included new methods for semiautomatic prior specification and posterior exploration. To illustrate key aspects of this evolution, the highlights of some of these developments are described.},
	number = {1},
	urldate = {2025-05-30},
	journal = {Statistical Science},
	author = {Clyde, Merlise and George, Edward I.},
	month = feb,
	year = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayes factors, bayes. LM, classification and regression trees, linear and nonparametric regression, MCMC, model averaging, objective prior distributions, reversible jump Markov chain Monte Carlo, review, todo, Variable selection},
	pages = {81--94},
	file = {Full Text PDF:/Users/lona/Zotero/storage/E2MS9PZE/Clyde and George - 2004 - Model Uncertainty.pdf:application/pdf},
}

@misc{noauthor_latent_nodate,
	title = {Latent {Dirichlet} {Allocation} ({LDA})},
	urldate = {2025-06-06},
	keywords = {plate notation},
	file = {http\://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf:/Users/lona/Zotero/storage/MLVQR63J/blei03a.pdf:application/pdf},
}

@article{buntine_operations_1994,
	title = {Operations for {Learning} with {Graphical} {Models}},
	volume = {2},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	doi = {10.1613/jair.62},
	abstract = {This paper is a multidisciplinary review of empirical,  statistical learning from a graphical model perspective.  Well-known  examples of graphical models include Bayesian networks, directed  graphs representing a Markov chain, and undirected networks  representing a Markov field.  These graphical models are extended to  model data analysis and empirical learning using the notation of  plates.  Graphical operations for simplifying and manipulating a  problem are provided including decomposition, differentiation, andthe  manipulation of probability models from the exponential family.  Two  standard algorithm schemas for learning are reviewed in a graphical  framework: Gibbs sampling and the expectation maximizationalgorithm.  Using these operations and schemas, some popular algorithms can be  synthesized from their graphical specification.  This includes  versions of linear regression, techniques for feed-forward networks,  and learning Gaussian and discrete Bayesian networks from data.  The  paper concludes by sketching some implications for data analysis and  summarizing how some popular algorithms fall within the framework  presented.  The main original contributions here are the decompositiontechniques  and the demonstration that graphical models provide a framework for  understanding and developing complex learning algorithms.},
	language = {en},
	urldate = {2025-06-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Buntine, W. L.},
	month = dec,
	year = {1994},
	keywords = {plate notation},
	pages = {159--225},
	file = {PDF:/Users/lona/Zotero/storage/L8D2JLQV/Buntine - 1994 - Operations for Learning with Graphical Models.pdf:application/pdf},
}

@book{fahrmeir_regression_2021,
	address = {Berlin, Heidelberg},
	title = {Regression: {Models}, {Methods} and {Applications}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-662-63881-1 978-3-662-63882-8},
	shorttitle = {Regression},
	language = {en},
	urldate = {2025-06-06},
	publisher = {Springer Berlin Heidelberg},
	author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian D.},
	year = {2021},
	doi = {10.1007/978-3-662-63882-8},
	keywords = {bayes. LM, bayes. Logit},
	file = {PDF:/Users/lona/Zotero/storage/S8BN4QJQ/Fahrmeir et al. - 2021 - Regression Models, Methods and Applications.pdf:application/pdf},
}

@article{zellner_assessing_1986,
	title = {On assessing prior distributions and {Bayesian} regression analysis with g-prior distributions},
	urldate = {2025-06-06},
	journal = {Bayesian Inference and Decision techniques},
	author = {Zellner, A.},
	year = {1986},
	note = {Publisher: Elsevier Science},
	keywords = {bayes. LM, lasso},
	file = {On assessing prior distributions and Bayesian regression analysis with g-prior distributions Snapshot:/Users/lona/Zotero/storage/6S34BC22/1571980074739548288.html:text/html},
}

@article{barbieri_optimal_2004,
	title = {Optimal predictive model selection},
	volume = {32},
	issn = {0090-5364},
	doi = {10.1214/009053604000000238},
	abstract = {Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model.},
	number = {3},
	urldate = {2025-06-06},
	journal = {The Annals of Statistics},
	author = {Barbieri, Maria Maddalena and Berger, James O.},
	month = jun,
	year = {2004},
	note = {arXiv:math/0406464},
	keywords = {bayes. LM, inference, Mathematics - Statistics Theory, Statistics - Statistics Theory},
	file = {Preprint PDF:/Users/lona/Zotero/storage/YSGRPQ2J/Barbieri and Berger - 2004 - Optimal predictive model selection.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/99KQ3C67/0406464.html:text/html},
}

@book{held_likelihood_2020,
	address = {Berlin, Heidelberg},
	series = {Statistics for {Biology} and {Health}},
	title = {Likelihood and {Bayesian} {Inference}: {With} {Applications} in {Biology} and {Medicine}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-662-60791-6 978-3-662-60792-3},
	shorttitle = {Likelihood and {Bayesian} {Inference}},
	language = {en},
	urldate = {2025-06-06},
	publisher = {Springer},
	author = {Held, Leonhard and Sabanés Bové, Daniel},
	year = {2020},
	doi = {10.1007/978-3-662-60792-3},
	keywords = {bayes. LM, bayes. Logit, Bayesian inference, capture-recapture method, choice of the prior distribution, frequentist inference, Hardy–Weinberg equilibrium, inference, lasso, likelihood inference, Markov models, maximum likelihood estimate, model averaging, model choice, quantifying disease risk, ridge, time series analysis, Wald Statistic},
	file = {Full Text PDF:/Users/lona/Zotero/storage/BAJ9W48K/Held and Sabanés Bové - 2020 - Likelihood and Bayesian Inference With Applications in Biology and Medicine.pdf:application/pdf},
}

@article{george_calibration_2000,
	title = {Calibration and empirical {Bayes} variable selection},
	volume = {87},
	issn = {0006-3444},
	doi = {10.1093/biomet/87.4.731},
	abstract = {For the problem of variable selection for the normal linear model, selection criteria such as aic, Cp , bic and ric have fixed dimensionality penalties. Such criteria are shown to correspond to selection of maximum posterior models under implicit hyperparameter choices for a particular hierarchical Bayes formulation. Based on this calibration, we propose empirical Bayes selection criteria that use hyperparameter estimates instead of fixed choices. For obtaining these estimates, both marginal and conditional maximum likelihood methods are considered. As opposed to traditional fixed penalty criteria, these empirical Bayes criteria have dimensionality penalties that depend on the data. Their performance is seen to approximate adaptively the performance of the best fixed‐penalty criterion across a variety of orthogonal and nonorthogonal set‐ups, including wavelet regression. Empirical Bayes shrinkage estimators of the selected coefficients are also proposed.},
	number = {4},
	urldate = {2025-06-06},
	journal = {Biometrika},
	author = {George, EdwardI. and Foster, Dean P.},
	month = dec,
	year = {2000},
	keywords = {calibration, inference},
	pages = {731--747},
	file = {Snapshot:/Users/lona/Zotero/storage/IPKU4T4S/232682.html:text/html},
}

@article{george_approaches_1997,
	title = {Approaches for {Bayesian} variable selection},
	volume = {7},
	issn = {1017-0405},
	abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George and McCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperparameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for posterior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probability. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
	language = {English},
	number = {2},
	journal = {STATISTICA SINICA},
	author = {George, E. I. and McCulloch, R. E.},
	month = apr,
	year = {1997},
	keywords = {bayes. LM, conjugate prior, GIBBS SAMPLER, Gibbs sampling, Gray Code, hierarchical models, lasso, Markov chain Monte Carlo, Metropolis-Hastings algorithms, MONTE-CARLO METHODS, normal mixtures, normalization constant, regression, REGRESSION, ridge, simulation},
	pages = {339--373},
	file = {PDF:/Users/lona/Zotero/storage/BG2AHWYB/George and McCulloch - 1997 - Approaches for Bayesian variable selection.pdf:application/pdf},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	doi = {10.1198/016214508000000337},
	abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
	language = {English},
	number = {482},
	urldate = {2025-06-06},
	journal = {JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	keywords = {bayes. LM, empirical Bayes, Gibbs sampler, hierarchical model, inverse Gaussian, lasso, LEAST ANGLE REGRESSION, linear regression, MODELS, NORMAL-DISTRIBUTIONS, penalized regression, scale mixture of normals, SCALE MIXTURES, VARIABLE SELECTION},
	pages = {681--686},
	file = {PDF:/Users/lona/Zotero/storage/LES8EARX/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf},
}

@article{ohara_review_2009,
	title = {A review of {Bayesian} variable selection methods: what, how and which},
	volume = {4},
	issn = {1936-0975, 1931-6690},
	shorttitle = {A review of {Bayesian} variable selection methods},
	doi = {10.1214/09-BA403},
	abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo \& Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Jeffreys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their different properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the different methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
	number = {1},
	urldate = {2025-06-06},
	journal = {Bayesian Analysis},
	author = {O'Hara, R. B. and Sillanpää, M. J.},
	month = mar,
	year = {2009},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {bayes. LM, BUGS, lasso, MCMC, simulation, software, Variable selection},
	pages = {85--117},
	file = {Full Text PDF:/Users/lona/Zotero/storage/6LDN3VDR/O'Hara and Sillanpää - 2009 - A review of Bayesian variable selection methods what, how and which.pdf:application/pdf},
}

@book{matsuura_bayesian_2022,
	address = {Singapore},
	title = {Bayesian {Statistical} {Modeling} with {Stan}, {R}, and {Python}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-981-19-4754-4 978-981-19-4755-1},
	language = {en},
	urldate = {2025-06-06},
	publisher = {Springer Nature},
	author = {Matsuura, Kentaro},
	year = {2022},
	doi = {10.1007/978-981-19-4755-1},
	keywords = {Bayesian Modeling, Python, R, RStan, simulation, software, Stan, Statistical Modeling},
	file = {Full Text PDF:/Users/lona/Zotero/storage/9TNWEPGN/Matsuura - 2022 - Bayesian Statistical Modeling with Stan, R, and Python.pdf:application/pdf},
}

@article{vehtari_practical_2017,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	issn = {1573-1375},
	doi = {10.1007/s11222-016-9696-4},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
	language = {en},
	number = {5},
	urldate = {2025-06-06},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	month = sep,
	year = {2017},
	keywords = {Applied Probability, bayes. LM, Bayesian computation, Bayesian Inference, Bayesian Network, inference, K-fold cross-validation, Leave-one-out cross-validation (LOO), Pareto smoothed importance sampling (PSIS), Probability and Statistics in Computer Science, Stan, Statistical Learning, Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences, Widely applicable information criterion (WAIC)},
	pages = {1413--1432},
	file = {Full Text PDF:/Users/lona/Zotero/storage/NP8P6AWR/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC.pdf:application/pdf},
}

@article{piironen_comparison_2017,
	title = {Comparison of {Bayesian} predictive methods for model selection},
	volume = {27},
	issn = {1573-1375},
	doi = {10.1007/s11222-016-9649-y},
	abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
	language = {en},
	number = {3},
	urldate = {2025-06-06},
	journal = {Statistics and Computing},
	author = {Piironen, Juho and Vehtari, Aki},
	month = may,
	year = {2017},
	keywords = {Bayesian Inference, Bayesian model selection, Bayesian Network, Cross-validation, inference, Machine Learning, Model Theory, Predictive medicine, Projection, Reference model, Selection bias, Statistical Learning},
	pages = {711--735},
	file = {Full Text PDF:/Users/lona/Zotero/storage/LK7846KI/Piironen and Vehtari - 2017 - Comparison of Bayesian predictive methods for model selection.pdf:application/pdf},
}

@article{van_erp_shrinkage_2019,
	title = {Shrinkage priors for {Bayesian} penalized regression},
	volume = {89},
	issn = {0022-2496},
	doi = {10.1016/j.jmp.2018.12.004},
	abstract = {In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.},
	urldate = {2025-06-06},
	journal = {Journal of Mathematical Psychology},
	author = {van Erp, Sara and Oberski, Daniel L. and Mulder, Joris},
	month = apr,
	year = {2019},
	keywords = {bayes. LM, Bayesian, Empirical Bayes, lasso, Penalization, Regression, Shrinkage priors},
	pages = {31--50},
	file = {Full Text:/Users/lona/Zotero/storage/BBZ7W4TY/van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:application/pdf;ScienceDirect Snapshot:/Users/lona/Zotero/storage/HNKIK23G/S0022249618300567.html:text/html},
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {0006-3444},
	doi = {10.1093/biomet/asq017},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	language = {English},
	number = {2},
	urldate = {2025-06-07},
	journal = {BIOMETRIKA},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	month = jun,
	year = {2010},
	keywords = {bayes. LM, BAYESIAN MODEL, GAUSSIAN GRAPHICAL MODELS, lasso, LASSO, Normal scale mixture, POSTERIOR MOMENTS, Ridge regression, Robustness, SELECTION, Shrinkage, Sparsity, Thresholding},
	pages = {465--480},
}

@article{bhadra_lasso_2019,
	title = {Lasso {Meets} {Horseshoe}: {A} {Survey}},
	volume = {34},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Lasso {Meets} {Horseshoe}},
	doi = {10.1214/19-STS700},
	abstract = {The goal of this paper is to contrast and survey the major advances in two of the most commonly used high-dimensional techniques, namely, the Lasso and horseshoe regularization. Lasso is a gold standard for predictor selection while horseshoe is a state-of-the-art Bayesian estimator for sparse signals. Lasso is fast and scalable and uses convex optimization whilst the horseshoe is nonconvex. Our novel perspective focuses on three aspects: (i) theoretical optimality in high-dimensional inference for the Gaussian sparse model and beyond, (ii) efficiency and scalability of computation and (iii) methodological development and performance.},
	language = {English},
	number = {3},
	urldate = {2025-06-07},
	journal = {STATISTICAL SCIENCE},
	author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon},
	month = aug,
	year = {2019},
	keywords = {ASYMPTOTIC PROPERTIES, bayes. LM, BAYESIAN VARIABLE SELECTION, CONFIDENCE-INTERVALS, EMPIRICAL BAYES, ESTIMATOR, Global-local priors, horseshoe, hyper-parameter tuning, INFERENCE, lasso, Lasso, LINEAR-REGRESSION, POSTERIOR CONCENTRATION, regression, regularization, REGULARIZATION, SHRINKAGE PRIORS, sparsity},
	pages = {405--427},
	file = {Full Text:/Users/lona/Zotero/storage/9V5ANV6A/Bhadra et al. - 2019 - Lasso Meets Horseshoe A Survey.pdf:application/pdf},
}

@article{mitchell_bayesian_1988,
	title = {Bayesian {Variable} {Selection} in {Linear}-{Regression}},
	volume = {83},
	issn = {0162-1459},
	doi = {10.2307/2290129},
	language = {English},
	number = {404},
	urldate = {2025-06-07},
	journal = {JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION},
	author = {Mitchell, Tj and Beauchamp, Jj},
	month = dec,
	year = {1988},
	keywords = {bayes. LM, lasso},
	pages = {1023--1032},
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Applications} to {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	doi = {10.2307/1267352},
	abstract = {This paper is an exposition of the use of ridge regression methods. Two examples from the literature are used as a base. Attention is focused on the RIDGE TRACE which is a two-dimensional graphical procedure for portraying the complex relationships in multifactor data. Recommendations are made for obtaining a better regression equation than that given by ordinary least squares estimation.},
	number = {1},
	urldate = {2025-06-08},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {1970},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	keywords = {bayes. LM, ridge},
	pages = {69--82},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/RSEYIKEA/Hoerl and Kennard - 1970 - Ridge Regression Applications to Nonorthogonal Problems.pdf:application/pdf},
}

@article{hoerl_ridge_1970-1,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	doi = {10.2307/1267351},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X?X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X?X to obtain biased estimates with smaller mean square error.},
	number = {1},
	urldate = {2025-06-08},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {1970},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	keywords = {bayes. LM, ridge},
	pages = {55--67},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/CQJ77NQS/Hoerl and Kennard - 1970 - Ridge Regression Biased Estimation for Nonorthogonal Problems.pdf:application/pdf},
}

@incollection{ahmed_geometry_2014,
	address = {Providence, Rhode Island},
	title = {Geometry and properties of generalized ridge regression in high dimensions},
	volume = {622},
	abstract = {Hoerl and Kennard proposed generalized ridge regression (GRR) over forty years ago as a means to overcome deﬁciencies in least squares in multicollinear problems. Because high-dimensional regression naturally involves correlated predictors, in part due to the nature of the data and in part due to artifact of the dimensionality, it is reasonable to consider GRR for such problems. We study GRR when the number of predictors p exceeds the sample size n. A novel geometric interpretation for GRR is described in terms of a uniquely deﬁned least squares estimator and lends insight into its properties. It is shown that GRR possesses a shrinkage property useful in correlated settings and that in sparse high-dimensional settings it can have excellent performance but no such guarantees hold in non-sparse settings. We describe a computationally eﬃcient representation for GRR requiring only a linear number of operations in p, thus making GRR computationally applicable to high dimensions.},
	language = {en},
	urldate = {2025-06-08},
	booktitle = {Contemporary {Mathematics}},
	publisher = {American Mathematical Society},
	author = {Ishwaran, Hemant and Rao, J.},
	editor = {Ahmed, S.},
	year = {2014},
	doi = {10.1090/conm/622/12438},
	keywords = {bayes. LM, ridge},
	pages = {81--93},
	file = {PDF:/Users/lona/Zotero/storage/P8DVNAY6/Ishwaran and Rao - 2014 - Geometry and properties of generalized ridge regression in high dimensions.pdf:application/pdf},
}

@article{ishwaran_spikeslab_2010,
	title = {spikeslab: {Prediction} and {Variable} {Selection} {Using} {Spike} and {Slab} {Regression}},
	volume = {2},
	abstract = {Weighted generalized ridge regression offers unique advantages in correlated highdimensional problems. Such estimators can be efﬁciently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slab prediction and variable selection methodology.},
	language = {en},
	author = {Ishwaran, Hemant and Kogalur, Udaya B and Rao, J Sunil},
	year = {2010},
	file = {PDF:/Users/lona/Zotero/storage/L9EHS3LW/Ishwaran et al. - 2010 - spikeslab Prediction and Variable Selection Using Spike and Slab Regression.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2025-06-08},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	keywords = {bayes. LM, lasso},
	pages = {267--288},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/9HEM5WD6/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:application/pdf},
}

@article{murphy_conjugate_nodate,
	title = {Conjugate {Bayesian} analysis of the {Gaussian} distribution},
	language = {en},
	author = {Murphy, Kevin P},
	keywords = {bayes. LM, inference},
	file = {PDF:/Users/lona/Zotero/storage/R6LZ23YL/Murphy - Conjugate Bayesian analysis of the Gaussian distribution.pdf:application/pdf},
}

@article{mackay_bayesian_1992,
	title = {Bayesian {Interpolation}},
	volume = {4},
	issn = {0899-7667},
	doi = {10.1162/neco.1992.4.3.415},
	abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. “Occam's razor” is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
	number = {3},
	urldate = {2025-06-08},
	journal = {Neural Computation},
	author = {MacKay, David J. C.},
	month = may,
	year = {1992},
	keywords = {bayes. LM, ridge},
	pages = {415--447},
	file = {Accepted Version:/Users/lona/Zotero/storage/F9MCMPAY/MacKay - 1992 - Bayesian Interpolation.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/3FFDLF53/Bayesian-Interpolation.html:text/html},
}

@book{dey_generalized_2000,
	address = {Boca Raton},
	title = {Generalized {Linear} {Models}: {A} {Bayesian} {Perspective}},
	isbn = {978-0-429-18240-2},
	shorttitle = {Generalized {Linear} {Models}},
	abstract = {This volume describes how to conceptualize, perform, and critique traditional generalized linear models (GLMs) from a Bayesian perspective and how to use modern computational methods to summarize inferences using simulation. Introducing dynamic modeling for GLMs and containing over 1000 references and equations, Generalized Linear Models considers},
	publisher = {CRC Press},
	editor = {Dey, Dipak K. and Ghosh, Sujit K. and Mallick, Bani K.},
	month = may,
	year = {2000},
	doi = {10.1201/9781482293456},
	keywords = {bayes. GLM},
}

@book{fahrmeir_multivariate_2001,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Multivariate {Statistical} {Modelling} {Based} on {Generalized} {Linear} {Models}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-2900-6 978-1-4757-3454-6},
	urldate = {2025-06-08},
	publisher = {Springer},
	author = {Fahrmeir, Ludwig and Tutz, Gerhard},
	year = {2001},
	doi = {10.1007/978-1-4757-3454-6},
	keywords = {bayes. GLM, best fit, data analysis, expectation–maximization algorithm, Fitting, Generalized linear model, Regression analysis, Survival analysis, Time series},
	file = {Full Text:/Users/lona/Zotero/storage/LA9C5J2M/Fahrmeir and Tutz - 2001 - Multivariate Statistical Modelling Based on Generalized Linear Models.pdf:application/pdf},
}

@article{dellaportas_bayesian_1993,
	title = {Bayesian {Inference} for {Generalized} {Linear} and {Proportional} {Hazards} {Models} via {Gibbs} {Sampling}},
	volume = {42},
	issn = {0035-9254},
	doi = {10.2307/2986324},
	abstract = {It is shown that Gibbs sampling, making systematic use of an adaptive rejection algorithm proposed by Gilks and Wild, provides a straightforward computational procedure for Bayesian inferences in a wide class of generalized linear and proportional hazards models.},
	number = {3},
	urldate = {2025-06-08},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Dellaportas, P. and Smith, A. F. M.},
	year = {1993},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	keywords = {bayes. GLM, MCMC},
	pages = {443--459},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/MJX7QDP9/Dellaportas and Smith - 1993 - Bayesian Inference for Generalized Linear and Proportional Hazards Models via Gibbs Sampling.pdf:application/pdf},
}

@article{gamerman_markov_1998,
	title = {Markov chain {Monte} {Carlo} for dynamic generalised linear models},
	volume = {85},
	issn = {0006-3444},
	doi = {10.1093/biomet/85.1.215},
	abstract = {This paper presents a new methodological approach for carrying out Bayesian inference about dynamic models for exponential family observations. The approach is simulationbased and involves the use of Markov chain Monte Carlo techniques. A Metropolis- Hastings algorithm is combined with the Gibbs sampler in repeated use of an adjusted version of normal dynamic linear models. Different alternative schemes based on sampling from the system disturbances and state parameters separately and in a block are derived and compared. The approach is fully Bayesian in obtaining posterior samples with state parameters and unknown hyperparameters. Illustrations with real datasets with sparse counts and missing values are presented. Extensions to accommodate more general evolution forms and distributions for observations and disturbances are outlined.},
	number = {1},
	urldate = {2025-06-08},
	journal = {Biometrika},
	author = {Gamerman, Dani},
	month = mar,
	year = {1998},
	keywords = {MCMC},
	pages = {215--227},
	file = {Full Text PDF:/Users/lona/Zotero/storage/G5AKMEIK/GAMERMAN - 1998 - Markov chain Monte Carlo for dynamic generalised linear models.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/HRWS5PC5/238759.html:text/html},
}

@article{lenk_bayesian_2000,
	title = {Bayesian {Inference} for {Finite} {Mixtures} of {Generalized} {Linear} {Models} with {Random} {Effects}},
	volume = {65},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0033-3123, 1860-0980},
	doi = {10.1007/BF02294188},
	abstract = {We present an hierarchical Bayes approach to modeling parameter heterogeneity in generalized linear models. The model assumes that there are relevant subpopulations and that within each subpopulation the individual-level regression coefficients have a multivariate normal distribution. However, class membership is not known a priori, so the heterogeneity in the regression coefficients becomes a finite mixture of normal distributions. This approach combines the flexibility of semiparametric, latent class models that assume common parameters for each sub-population and the parsimony of random effects models that assume normal distributions for the regression parameters. The number of subpopulations is selected to maximize the posterior probability of the model being true. Simulations are presented which document the performance of the methodology for synthetic data with known heterogeneity and number of sub-populations. An application is presented concerning preferences for various aspects of personal computers.},
	language = {en},
	number = {1},
	urldate = {2025-06-08},
	journal = {Psychometrika},
	author = {Lenk, Peter J. and DeSarbo, Wayne S.},
	month = mar,
	year = {2000},
	pages = {93--119},
	file = {PDF:/Users/lona/Zotero/storage/RGT7V84D/Lenk and DeSarbo - 2000 - Bayesian Inference for Finite Mixtures of Generalized Linear Models with Random Effects.pdf:application/pdf},
}

@article{albert_bayesian_1993,
	title = {Bayesian {Analysis} of {Binary} and {Polychotomous} {Response} {Data}},
	volume = {88},
	issn = {0162-1459},
	doi = {10.2307/2290350},
	abstract = {A vast literature in statistics, biometrics, and econometrics is concerned with the analysis of binary and polychotomous response data. The classical approach fits a categorical response regression model using maximum likelihood, and inferences about the model are based on the associated asymptotic theory. The accuracy of classical confidence statements is questionable for small sample sizes. In this article, exact Bayesian methods for modeling categorical response data are developed using the idea of data augmentation. The general approach can be summarized as follows. The probit regression model for binary outcomes is seen to have an underlying normal regression structure on latent continuous data. Values of the latent data can be simulated from suitable truncated normal distributions. If the latent data are known, then the posterior distribution of the parameters can be computed using standard results for normal linear models. Draws from this posterior are used to sample new latent data, and the process is iterated with Gibbs sampling. This data augmentation approach provides a general framework for analyzing binary regression models. It leads to the same simplification achieved earlier for censored regression models. Under the proposed framework, the class of probit regression models can be enlarged by using mixtures of normal distributions to model the latent data. In this normal mixture class, one can investigate the sensitivity of the parameter estimates to the choice of "link function," which relates the linear regression estimate to the fitted probabilities. In addition, this approach allows one to easily fit Bayesian hierarchical models. One specific model considered here reflects the belief that the vector of regression coefficients lies on a smaller dimension linear subspace. The methods can also be generalized to multinomial response models with \$J {\textgreater} 2\$ categories. In the ordered multinomial model, the J categories are ordered and a model is written linking the cumulative response probabilities with the linear regression structure. In the unordered multinomial model, the latent variables have a multivariate normal distribution with unknown variance-covariance matrix. For both multinomial models, the data augmentation method combined with Gibbs sampling is outlined. This approach is especially attractive for the multivariate probit model, where calculating the likelihood can be difficult.},
	number = {422},
	urldate = {2025-06-08},
	journal = {Journal of the American Statistical Association},
	author = {Albert, James H. and Chib, Siddhartha},
	year = {1993},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {bayes. Logit, MCMC},
	pages = {669--679},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/KIC4PV9H/Albert and Chib - 1993 - Bayesian Analysis of Binary and Polychotomous Response Data.pdf:application/pdf},
}

@article{nelder_generalized_1972,
	title = {Generalized {Linear} {Models}},
	volume = {135},
	issn = {0035-9238},
	doi = {10.2307/2344614},
	abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
	number = {3},
	urldate = {2025-06-08},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Nelder, J. A. and Wedderburn, R. W. M.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	keywords = {bayes. GLM},
	pages = {370--384},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/88CA93QR/Nelder and Wedderburn - 1972 - Generalized Linear Models.pdf:application/pdf},
}

@article{held_bayesian_2006,
	title = {Bayesian auxiliary variable models for binary and multinomial regression},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/06-BA105},
	abstract = {In this paper we discuss auxiliary variable approaches to Bayesian binary and multinomial regression. These approaches are ideally suited to automated Markov chain Monte Carlo simulation. In the first part we describe a simple technique using joint updating that improves the performance of the conventional probit regression algorithm. In the second part we discuss auxiliary variable methods for inference in Bayesian logistic regression, including covariate set uncertainty. Finally, we show how the logistic method is easily extended to multinomial regression models. All of the algorithms are fully automatic with no user set parameters and no necessary Metropolis-Hastings accept/reject steps.},
	number = {1},
	urldate = {2025-06-08},
	journal = {Bayesian Analysis},
	author = {Held, Leonhard and Holmes, Chris C.},
	month = mar,
	year = {2006},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {auxiliary variables, bayes. Logit, Bayesian binary and multinomial regression, Markov chain Monte Carlo, MCMC, model averaging, Scale mixture of normals, Variable selection},
	pages = {145--168},
	file = {Full Text PDF:/Users/lona/Zotero/storage/JDCFDLQE/Held and Holmes - 2006 - Bayesian auxiliary variable models for binary and multinomial regression.pdf:application/pdf},
}

@incollection{fruhwirth-schnatter_data_2010,
	address = {Heidelberg},
	title = {Data {Augmentation} and {MCMC} for {Binary} and {Multinomial} {Logit} {Models}},
	isbn = {978-3-7908-2413-1},
	abstract = {The paper introduces two new data augmentation algorithms for sampling the parameters of a binary or multinomial logit model from their posterior distribution within a Bayesian framework. The new samplers are based on rewriting the underlying random utility model in such away that only differences of utilities are involved. As a consequence, the error term in the logit model has a logistic distribution. If the logistic distribution is approximated by a finite scale mixture of normal distributions, auxiliary mixture sampling can be implemented to sample from the posterior of the regression parameters. Alternatively, a data augmented Metropolis–Hastings algorithm can be formulated by approximating the logistic distribution by a single normal distribution. A comparative study on five binomial and multinomial data sets shows that the new samplers are superior to other data augmentation samplers and to Metropolis–Hastings sampling without data augmentation.},
	language = {en},
	urldate = {2025-06-08},
	booktitle = {Statistical {Modelling} and {Regression} {Structures}: {Festschrift} in {Honour} of {Ludwig} {Fahrmeir}},
	publisher = {Physica-Verlag HD},
	author = {Frühwirth-Schnatter, Sylvia and Frühwirth, Rudolf},
	editor = {Kneib, Thomas and Tutz, Gerhard},
	year = {2010},
	doi = {10.1007/978-3-7908-2413-1_7},
	keywords = {bayes. Logit, Binomial dataTrondheim, data augmentation, logit model, Markov chain Monte Carlo, MCMC, multinomial data, random utility model},
	pages = {111--132},
	file = {Full Text PDF:/Users/lona/Zotero/storage/RSB6F39J/Frühwirth-Schnatter and Frühwirth - 2010 - Data Augmentation and MCMC for Binary and Multinomial Logit Models.pdf:application/pdf},
}

@article{rue_approximate_2009,
	title = {Approximate {Bayesian} {Inference} for {Latent} {Gaussian} models by using {Integrated} {Nested} {Laplace} {Approximations}},
	volume = {71},
	issn = {1369-7412},
	doi = {10.1111/j.1467-9868.2008.00700.x},
	abstract = {Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.},
	number = {2},
	urldate = {2025-06-08},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Rue, Håvard and Martino, Sara and Chopin, Nicolas},
	month = apr,
	year = {2009},
	keywords = {LA},
	pages = {319--392},
	file = {Full Text PDF:/Users/lona/Zotero/storage/593SH4A8/Rue et al. - 2009 - Approximate Bayesian Inference for Latent Gaussian models by using Integrated Nested Laplace Approxi.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/ISQJ953U/7092907.html:text/html},
}

@misc{mcelreath_statistical_nodate,
	title = {Statistical {Rethinking}},
	urldate = {2025-06-08},
	author = {McElreath, Richard},
	keywords = {simulation, software},
	file = {RM-StatRethink-Bayes.pdf:/Users/lona/Zotero/storage/68MNLNIT/RM-StatRethink-Bayes.pdf:application/pdf;xcelab.net/rm/:/Users/lona/Zotero/storage/533KW994/rm.html:text/html},
}

@misc{ghosh_use_2017,
	title = {On the {Use} of {Cauchy} {Prior} {Distributions} for {Bayesian} {Logistic} {Regression}},
	doi = {10.48550/arXiv.1507.07170},
	abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on Polya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package in the supplement. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
	urldate = {2025-06-08},
	publisher = {arXiv},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	month = feb,
	year = {2017},
	note = {arXiv:1507.07170 [stat]},
	keywords = {bayes. Logit, Statistics - Methodology},
	file = {Preprint PDF:/Users/lona/Zotero/storage/HRL4R45A/Ghosh et al. - 2017 - On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression.pdf:application/pdf;Snapshot:/Users/lona/Zotero/storage/8PRY9Y9V/1507.html:text/html},
}

@article{gelman_weakly_2008,
	title = {A weakly informative default prior distribution for logistic and other regression models},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	doi = {10.1214/08-AOAS191},
	abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
	number = {4},
	urldate = {2025-06-08},
	journal = {The Annals of Applied Statistics},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
	month = dec,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {bayes. Logit, Bayesian inference, generalized linear model, hierarchical model, least squares, Linear regression, logistic regression, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	pages = {1360--1383},
	file = {Full Text PDF:/Users/lona/Zotero/storage/BFXZPNAP/Gelman et al. - 2008 - A weakly informative default prior distribution for logistic and other regression models.pdf:application/pdf},
}

@article{greenland_putting_2001,
	title = {Putting {Background} {Information} {About} {Relative} {Risks} into {Conjugate} {Prior} {Distributions}},
	volume = {57},
	issn = {1541-0420},
	doi = {10.1111/j.0006-341X.2001.00663.x},
	abstract = {Summary. In Bayesian and empirical Bayes analyses of epidemiologic data, the most easily implemented prior specifications use a multivariate normal distribution for the log relative risks or a conjugate distribution for the discrete response vector. This article describes problems in translating background information about relative risks into conjugate priors and a solution. Traditionally, conjugate priors have been specified through flattening constants, an approach that leads to conflicts with the true prior covariance structure for the log relative risks. One can, however, derive a conjugate prior consistent with that structure by using a data-augmentation approximation to the true log relative-risk prior, although a rescaling step is needed to ensure the accuracy of the approximation. These points are illustrated with a logistic regression analysis of neonatal-death risk.},
	language = {en},
	number = {3},
	urldate = {2025-06-08},
	journal = {Biometrics},
	author = {Greenland, Sander},
	year = {2001},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2001.00663.x},
	keywords = {Bayesian analysis, Data augmentation, Epidemiologic methods, Exponential regression, Interpretation, Log-linear models, Logistic regression, Odds ratio, Poisson regression, Relative risk, Risk assessment, Risk regression},
	pages = {663--670},
	file = {Snapshot:/Users/lona/Zotero/storage/U6K857L3/j.0006-341X.2001.00663.html:text/html},
}

@article{bedrick_new_1996,
	title = {A {New} {Perspective} on {Priors} for {Generalized} {Linear} {Models}},
	volume = {91},
	issn = {0162-1459},
	doi = {10.1080/01621459.1996.10476713},
	abstract = {This article deals with specifications of informative prior distributions for generalized linear models. Our emphasis is on specifying distributions for selected points on the regression surface; the prior distribution on regression coefficients is induced from this specification. We believe that it is inherently easier to think about conditional means of observables given the regression variables than it is to think about model-dependent regression coefficients. Previous use of conditional means priors seems to be restricted to logistic regression with one predictor variable and to normal theory regression. We expand on the idea of conditional means priors and extend these to arbitrary generalized linear models. We also consider data augmentation priors where the prior is of the same form as the likelihood. We show that data augmentation priors are special cases of conditional means priors. With current Monte Carlo methodology, such as importance sampling and Gibbs sampling, our priors result in tractable posteriors.},
	number = {436},
	urldate = {2025-06-08},
	journal = {Journal of the American Statistical Association},
	author = {Bedrick, Edward J. and , Ronald, Christensen and and Johnson, Wesley},
	month = dec,
	year = {1996},
	keywords = {bayes. GLM, bayes. Logit, Conditional means priors, Data augmentation priors, Exponential regression, Gamma regression, Linear models, Log-linear models, Logistic regression, Poisson regression},
	pages = {1450--1460},
}

@book{bishop_pattern_2019,
	address = {New York, NY},
	series = {Information {Science} and {Statistics}},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer Science+Business Media, LLC},
	author = {Bishop, Christopher M.},
	year = {2019},
	file = {PDF:/Users/lona/Zotero/storage/CSB9VESL/Bishop - 2019 - Pattern recognition and machine learning.pdf:application/pdf},
}

@article{spiegelhalter_sequential_1990,
	title = {Sequential updating of conditional probabilities on directed graphical structures},
	volume = {20},
	copyright = {Copyright © 1990 Wiley Periodicals, Inc., A Wiley Company},
	issn = {1097-0037},
	doi = {10.1002/net.3230200507},
	abstract = {A directed acyclic graph or influence diagram is frequently used as a representation for qualitative knowledge in some domains in which expert system techniques have been applied, and conditional probability tables on appropriate sets of variables form the quantitative part of the accumulated experience. It is shown how one can introduce imprecision into such probabilities as a data base of cases accumulates. By exploiting the graphical structure, the updating can be performed locally, either approximately or exactly, and the setup makes it possible to take advantage of a range of well-established statistical techniques. As examples we discuss discrete models, models based on Dirichlet distributions and models of the logistic regression type.},
	language = {en},
	number = {5},
	urldate = {2025-06-09},
	journal = {Networks},
	author = {Spiegelhalter, David J. and Lauritzen, Steffen L.},
	year = {1990},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/net.3230200507},
	keywords = {bayes. Logit, LA},
	pages = {579--605},
	file = {Snapshot:/Users/lona/Zotero/storage/45F2Q3HJ/net.html:text/html},
}

@article{mackay_choice_1998,
	title = {Choice of {Basis} for {Laplace} {Approximation}},
	volume = {33},
	issn = {1573-0565},
	doi = {10.1023/A:1007558615313},
	abstract = {Maximum a posteriori optimization of parameters and the Laplace approximation for the marginal likelihood are both basis-dependent methods. This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the 'softmax' basis.},
	language = {en},
	number = {1},
	urldate = {2025-06-09},
	journal = {Machine Learning},
	author = {MacKay, David J.C.},
	month = oct,
	year = {1998},
	keywords = {Applied Probability, Approximations and Expansions, Bayes factor, bayes. Logit, Bayesian inference, Bayesian Inference, Calculus of Variations and Optimization, graphical models, hidden Markov models, LA, latent variable models, marginal likelihood, Non-parametric Inference, Parametric Inference},
	pages = {77--86},
	file = {Full Text PDF:/Users/lona/Zotero/storage/CJRPZV96/MacKay - 1998 - Choice of Basis for Laplace Approximation.pdf:application/pdf},
}

@article{roberts_probabilistic_1965,
	title = {Probabilistic {Prediction}},
	volume = {60},
	issn = {0162-1459},
	doi = {10.1080/01621459.1965.10480774},
	abstract = {The posterior distribution for parameters of a data distribution is usually the major objective of a Bayesian statistical analysis. Relatively little attention has been given to the fact that either a prior or a posterior distribution implies a marginal distribution, which may be called a “predictive distribution,” for outcomes of any sample not yet observed. Predictive distributions have been mainly applied to design problems, such as determination of optimal sample size. In this paper tentative suggestions are made for applications to statistical inference, especially problems of appropriateness, selection, interpretation, and validation of formal models.},
	number = {309},
	urldate = {2025-06-09},
	journal = {Journal of the American Statistical Association},
	author = {Roberts, Harry V.},
	month = mar,
	year = {1965},
	note = {Publisher: ASA Website
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1965.10480774},
	keywords = {inference},
	pages = {50--62},
}

@article{holmes_efficient_nodate,
	title = {Efficient simulation of {Bayesian} logistic regression models},
	author = {Holmes, Knorr-Held},
	keywords = {bayes. Logit},
	file = {PDF:/Users/lona/Zotero/storage/TVH8V4I7/Holmes - Efficient simulation of Bayesian logistic regression models.pdf:application/pdf},
}

@article{zeger_generalized_1991,
	title = {Generalized {Linear} {Models} {With} {Random} {Effects}; {A} {Gibbs} {Sampling} {Approach}},
	volume = {86},
	issn = {0162-1459},
	doi = {10.2307/2289717},
	abstract = {Generalized linear models have unified the approach to regression for a wide variety of discrete, continuous, and censored response variables that can be assumed to be independent across experimental units. In applications such as longitudinal studies, genetic studies of families, and survey sampling, observations may be obtained in clusters. Responses from the same cluster cannot be assumed to be independent. With linear models, correlation has been effectively modeled by assuming there are cluster-specific random effects that derive from an underlying mixing distribution. Extensions of generalized linear models to include random effects has, thus far, been hampered by the need for numerical integration to evaluate likelihoods. In this article, we cast the generalized linear random effects model in a Bayesian framework and use a Monte Carlo method, the Gibbs sampler, to overcome the current computational limitations. The resulting algorithm is flexible to easily accommodate changes in the number of random effects and in their assumed distribution when warranted. The methodology is illustrated through a simulation study and an analysis of infectious disease data.},
	number = {413},
	urldate = {2025-06-09},
	journal = {Journal of the American Statistical Association},
	author = {Zeger, Scott L. and Karim, M. Rezaul},
	year = {1991},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {bayes. GLM, beyes. Logit, MCMC},
	pages = {79--86},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/9PNITXDM/Zeger and Karim - 1991 - Generalized Linear Models With Random Effects\; A Gibbs Sampling Approach.pdf:application/pdf},
}

@article{fruhwirth-schnatter_auxiliary_2007,
	title = {Auxiliary mixture sampling with applications to logistic models},
	volume = {51},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2006.10.006},
	abstract = {A new method of data augmentation for binary and multinomial logit models is described. First, the latent utilities are introduced as auxiliary latent variables, leading to a latent model which is linear in the unknown parameters, but involves errors from the type I extreme value distribution. Second, for each error term the density of this distribution is approximated by a mixture of normal distributions, and the component indicators in these mixtures are introduced as further latent variables. This leads to Markov chain Monte Carlo estimation based on a convenient auxiliary mixture sampler that draws from standard distributions like normal or exponential distributions and, in contrast to more common Metropolis–Hastings approaches, does not require any tuning. It is shown how the auxiliary mixture sampler is implemented for binary or multinomial logit models, and it is demonstrated how to extend the sampler to mixed effect models and time-varying parameter models for binary and categorical data. Finally, an application to Austrian labor market data is discussed.},
	number = {7},
	urldate = {2025-06-09},
	journal = {Computational Statistics \& Data Analysis},
	author = {Frühwirth-Schnatter, Sylvia and Frühwirth, Rudolf},
	month = apr,
	year = {2007},
	keywords = {bayes. Logit, Binary data, Categorical data, Markov chain Monte Carlo, MCMC, Random-effects models, State space models, Utilities},
	pages = {3509--3528},
	file = {ScienceDirect Snapshot:/Users/lona/Zotero/storage/ZZIPIHBU/S0167947306003720.html:text/html},
}

@article{scott_data_2011,
	title = {Data augmentation, frequentist estimation, and the {Bayesian} analysis of multinomial logit models},
	volume = {52},
	issn = {1613-9798},
	doi = {10.1007/s00362-009-0205-0},
	abstract = {This article describes a convenient method of selecting Metropolis– Hastings proposal distributions for multinomial logit models. There are two key ideas involved. The first is that multinomial logit models have a latent variable representation similar to that exploited by Albert and Chib (J Am Stat Assoc 88:669–679, 1993) for probit regression. Augmenting the latent variables replaces the multinomial logit likelihood function with the complete data likelihood for a linear model with extreme value errors. While no conjugate prior is available for this model, a least squares estimate of the parameters is easily obtained. The asymptotic sampling distribution of the least squares estimate is Gaussian with known variance. The second key idea in this paper is to generate a Metropolis–Hastings proposal distribution by conditioning on the estimator instead of the full data set. The resulting sampler has many of the benefits of so-called tailored or approximation Metropolis–Hastings samplers. However, because the proposal distributions are available in closed form they can be implemented without numerical methods for exploring the posterior distribution. The algorithm converges geometrically ergodically, its computational burden is minor, and it requires minimal user input. Improvements to the sampler’s mixing rate are investigated. The algorithm is also applied to partial credit models describing ordinal item response data from the 1998 National Assessment of Educational Progress. Its application to hierarchical models and Poisson regression are briefly discussed.},
	language = {en},
	number = {1},
	urldate = {2025-06-09},
	journal = {Statistical Papers},
	author = {Scott, Steven L.},
	month = feb,
	year = {2011},
	keywords = {Applied Statistics, bayes. Logit, Bayesian Inference, Discrete choice model, Econometrics, Gibbs sampler, inference, Logistic regression, Markov chain Monte Carlo, MCMC, Metropolis–Hastings, Multinomial Poisson transformation, Parametric Inference, Partial credit model, Polychotomous, Polytomous, Stochastic Modelling, Stochastic Modelling in Statistics},
	pages = {87--109},
	file = {Full Text PDF:/Users/lona/Zotero/storage/VVQE37A9/Scott - 2011 - Data augmentation, frequentist estimation, and the Bayesian analysis of multinomial logit models.pdf:application/pdf},
}

@article{polson_bayesian_2013,
	title = {Bayesian {Inference} for {Logistic} {Models} {Using} {Pólya}–{Gamma} {Latent} {Variables}},
	volume = {108},
	issn = {0162-1459},
	doi = {10.1080/01621459.2013.829001},
	abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya–Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis–Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya–Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online.},
	number = {504},
	urldate = {2025-06-09},
	journal = {Journal of the American Statistical Association},
	author = {Polson, Nicholas G. and , James G., Scott and and Windle, Jesse},
	month = dec,
	year = {2013},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2013.829001},
	keywords = {bayes. Logit, Bayesian methods, Data augmentation, lasso, Logistic regression, Negative binomial regression, Pólya–Gamma distribution, ridge},
	pages = {1339--1349},
	file = {Submitted Version:/Users/lona/Zotero/storage/IV6Z82YG/Polson et al. - 2013 - Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables.pdf:application/pdf},
}

@article{hans_shotgun_2007,
	title = {Shotgun {Stochastic} {Search} for “{Large} p” {Regression}},
	volume = {102},
	issn = {0162-1459},
	doi = {10.1198/016214507000000121},
	abstract = {Model search in regression with very large numbers of candidate predictors raises challenges for both model specification and computation, for which standard approaches such as Markov chain Monte Carlo (MCMC) methods are often infeasible or ineffective. We describe a novel shotgun stochastic search (SSS) approach that explores “interesting” regions of the resulting high-dimensional model spaces and quickly identifies regions of high posterior probability over models. We describe algorithmic and modeling aspects, priors over the model space that induce sparsity and parsimony over and above the traditional dimension penalization implicit in Bayesian and likelihood analyses, and parallel computation using cluster computers. We discuss an example from gene expression cancer genomics, comparisons with MCMC and other methods, and theoretical and simulation-based aspects of performance characteristics in large-scale regression model searches. We also provide software implementing the methods.},
	number = {478},
	urldate = {2025-06-09},
	journal = {Journal of the American Statistical Association},
	author = {Hans, Chris and , Adrian, Dobra and and West, Mike},
	month = jun,
	year = {2007},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/016214507000000121},
	keywords = {bayes. LM, bayes. Logit, lasso, Model averaging, Parallel computing, Regression model uncertainty, Stochastic search, Variable selection},
	pages = {507--516},
	file = {Full Text PDF:/Users/lona/Zotero/storage/KYHJ449Z/Hans et al. - 2007 - Shotgun Stochastic Search for “Large p” Regression.pdf:application/pdf},
}

@article{li_fully_2018,
	title = {Fully {Bayesian} logistic regression with hyper-{LASSO} priors for high-dimensional feature selection},
	volume = {88},
	issn = {0094-9655},
	doi = {10.1080/00949655.2018.1490418},
	abstract = {Feature selection arises in many areas of modern science. For example, in genomic research, we want to find the genes that can be used to separate tissues of different classes (e.g. cancer and normal). One approach is to fit regression/classification models with certain penalization. In the past decade, hyper-LASSO penalization (priors) have received increasing attention in the literature. However, fully Bayesian methods that use Markov chain Monte Carlo (MCMC) for regression/classification with hyper-LASSO priors are still in lack of development. In this paper, we introduce an MCMC method for learning multinomial logistic regression with hyper-LASSO priors. Our MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling framework. We have used simulation studies and real data to demonstrate the superior performance of hyper-LASSO priors compared to LASSO, and to investigate the issues of choosing heaviness and scale of hyper-LASSO priors.},
	number = {14},
	urldate = {2025-06-09},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Li, Longhai and and Yao, Weixin},
	month = sep,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949655.2018.1490418},
	keywords = {bayes. Logit, feature selection, fully Bayesian, Gibbs sampling, Hamiltonian Monte Carlo, heavy-tailed prior, High-dimensional, horseshoe, hyper-LASSO priors, lasso, MCMC, non-convex penalties},
	pages = {2827--2851},
	file = {Submitted Version:/Users/lona/Zotero/storage/KS6P5VG9/Li and and Yao - 2018 - Fully Bayesian logistic regression with hyper-LASSO priors for high-dimensional feature selection.pdf:application/pdf},
}

@misc{makalic_high-dimensional_2016,
	title = {High-{Dimensional} {Bayesian} {Regularised} {Regression} with the {BayesReg} {Package}},
	doi = {10.48550/arXiv.1611.06649},
	abstract = {Bayesian penalized regression techniques, such as the Bayesian lasso and the Bayesian horseshoe estimator, have recently received a signiﬁcant amount of attention in the statistics literature. However, software implementing state-of-the-art Bayesian penalized regression, outside of general purpose Markov chain Monte Carlo platforms such as Stan, is relatively rare. This paper introduces bayesreg, a new toolbox for ﬁtting Bayesian penalized regression models with continuous shrinkage prior densities. The toolbox features Bayesian linear regression with Gaussian or heavy-tailed error models and Bayesian logistic regression with ridge, lasso, horseshoe and horseshoe+ estimators. The toolbox is free, open-source and available for use with the MATLAB and R numerical platforms.},
	language = {en},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Makalic, Enes and Schmidt, Daniel F.},
	month = dec,
	year = {2016},
	note = {arXiv:1611.06649 [stat]},
	keywords = {lasso, ridge, software, Statistics - Computation},
	file = {PDF:/Users/lona/Zotero/storage/EZNNLSGH/Makalic and Schmidt - 2016 - High-Dimensional Bayesian Regularised Regression with the BayesReg Package.pdf:application/pdf},
}

@article{polson_bayesian_2019,
	title = {Bayesian regularization: {From} {Tikhonov} to horseshoe},
	volume = {11},
	copyright = {© 2019 Wiley Periodicals, Inc.},
	issn = {1939-0068},
	shorttitle = {Bayesian regularization},
	doi = {10.1002/wics.1463},
	abstract = {Bayesian regularization is a central tool in modern-day statistical and machine learning methods. Many applications involve high-dimensional sparse signal recovery problems. The goal of our paper is to provide a review of the literature on penalty-based regularization approaches, from Tikhonov (Ridge, Lasso) to horseshoe regularization. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Robust Methods Statistical Models {\textgreater} Linear Models Statistical Models {\textgreater} Bayesian Models},
	language = {en},
	number = {4},
	urldate = {2025-06-09},
	journal = {WIREs Computational Statistics},
	author = {Polson, Nicholas G. and Sokolov, Vadim},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1463},
	keywords = {Bayesian regression, horseshoe, lasso, regularization, ridge, simulation, software},
	pages = {e1463},
	file = {Snapshot:/Users/lona/Zotero/storage/K7PAUEQ2/wics.html:text/html;Submitted Version:/Users/lona/Zotero/storage/P673GJSC/Polson and Sokolov - 2019 - Bayesian regularization From Tikhonov to horseshoe.pdf:application/pdf},
}

@article{celeux_regularization_2012,
	title = {Regularization in {Regression}: {Comparing} {Bayesian} and {Frequentist} {Methods} in a {Poorly} {Informative} {Situation}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	shorttitle = {Regularization in {Regression}},
	doi = {10.1214/12-BA716},
	abstract = {Using a collection of simulated and real benchmarks, we compare Bayesian and frequentist regularization approaches under a low informative constraint when the number of variables is almost equal to the number of observations on simulated and real datasets. This comparison includes new global noninformative approaches for Bayesian variable selection built on Zellner’s g-priors that are similar to Liang et al. (2008). The interest of those calibration-free proposals is discussed. The numerical experiments we present highlight the appeal of Bayesian regularization methods, when compared with non-Bayesian alternatives. They dominate frequentist methods in the sense that they provide smaller prediction errors while selecting the most relevant variables in a parsimonious way.},
	number = {2},
	urldate = {2025-06-09},
	journal = {Bayesian Analysis},
	author = {Celeux, Gilles and Anbari, Mohammed El and Marin, Jean-Michel and Robert, Christian P.},
	month = jun,
	year = {2012},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Calibration, Dantzig selector, Elastic net, lasso, Lasso, model choice, noninformative priors, regularization methods, ridge, simulation, Zellner’s g–prior},
	pages = {477--502},
	file = {Full Text PDF:/Users/lona/Zotero/storage/QJ3ARSSA/Celeux et al. - 2012 - Regularization in Regression Comparing Bayesian and Frequentist Methods in a Poorly Informative Sit.pdf:application/pdf},
}

@article{gramacy_simulation-based_2012,
	title = {Simulation-based {Regularized} {Logistic} {Regression}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/12-BA719},
	abstract = {In this paper, we develop a simulation-based framework for regularized logistic regression, exploiting two novel results for scale mixtures of normals. By carefully choosing a hierarchical model for the likelihood by one type of mixture, and implementing regularization with another, we obtain new MCMC schemes with varying efficiency depending on the data type (binary v. binomial, say) and the desired estimator (maximum likelihood, maximum a posteriori, posterior mean). Advantages of our omnibus approach include flexibility, computational efficiency, applicability in p≫n settings, uncertainty estimates, variable selection, and assessing the optimal degree of regularization. We compare our methodology to modern alternatives on both synthetic and real data. An R package called reglogit is available on CRAN.},
	number = {3},
	urldate = {2025-06-09},
	journal = {Bayesian Analysis},
	author = {Gramacy, Robert B. and Polson, Nicholas G.},
	month = sep,
	year = {2012},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {bayes. Logit, Bayesian shrinkage, ‎classification‎, Data augmentation, Gibbs sampling, lasso, Lasso, logistic regression, regularization, ridge, variance-mean mixtures, z–distributions},
	pages = {567--590},
	file = {Full Text PDF:/Users/lona/Zotero/storage/96WR8F5U/Gramacy and Polson - 2012 - Simulation-based Regularized Logistic Regression.pdf:application/pdf},
}

@article{west_dynamic_1985,
	title = {Dynamic {Generalized} {Linear} {Models} and {Bayesian} {Forecasting}},
	volume = {80},
	issn = {0162-1459},
	doi = {10.1080/01621459.1985.10477131},
	abstract = {Dynamic Bayesian models are developed for application in nonlinear, non-normal time series and regression problems, providing dynamic extensions of standard generalized linear models. A key feature of the analysis is the use of conjugate prior and posterior distributions for the exponential family parameters. This leads to the calculation of closed, standard-form predictive distributions for forecasting and model criticism. The structure of the models depends on the time evolution of underlying state variables, and the feedback of observational information to these variables is achieved using linear Bayesian prediction methods. Data analytic aspects of the models concerning scale parameters and outliers are discussed, and some applications are provided. Dynamic Bayesian models are developed for application in nonlinear, non-normal time series and regression problems, providing dynamic extensions of standard generalized linear models. A key feature of the analysis is the use of conjugate prior and posterior distributions for the exponential family parameters. This leads to the calculation of closed, standard-form predictive distributions for forecasting and model criticism. The structure of the models depends on the time evolution of underlying state variables, and the feedback of observational information to these variables is achieved using linear Bayesian prediction methods. Data analytic aspects of the models concerning scale parameters and outliers are discussed, and some applications are provided.},
	number = {389},
	urldate = {2025-06-10},
	journal = {Journal of the American Statistical Association},
	author = {West, Mike and , P. Jeff, Harrison and and Migon, Helio S.},
	month = mar,
	year = {1985},
	keywords = {bayes. GLM, Dynamic regression, Exponential family, Linear Bayes methods},
	pages = {73--83},
	file = {PDF:/Users/lona/Zotero/storage/BQCVPGJ4/West et al. - 1985 - Dynamic Generalized Linear Models and Bayesian Forecasting.pdf:application/pdf},
}


@article{hsiang_bayesian_1975,
	title = {A {Bayesian} {View} on {Ridge} {Regression}},
	volume = {24},
	issn = {0039-0526},
	doi = {10.2307/2987923},
	number = {4},
	urldate = {2025-06-10},
	journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Hsiang, T. C.},
	year = {1975},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	keywords = {bayes. LM, ridge},
	pages = {267--268},
	file = {JSTOR Full Text PDF:/Users/lona/Zotero/storage/AGTN98ZJ/Hsiang - 1975 - A Bayesian View on Ridge Regression.pdf:application/pdf},
}
