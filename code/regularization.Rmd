---
title: "Simulation Study: Regularization with Bayesian regression"
author: "Lona Koers"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("functions.R")
```

```{r}
set.seed(2025)
```

# 1. Data generation

```{r}
thetaA <- c(2, 1.5, 0, 0, 0)
SigmaA <- diag(length(thetaA) - 1)
sigma2A <- 1
nA <- 150

data_regr_A <- data_regularization(
    n = nA,
    Sigma = SigmaA,
    theta_true = thetaA,
    family = "gaussian",
    sigma2 = 1
  )
  
data_class_A <- data_regularization(
    n = nA,
    Sigma = SigmaA,
    theta_true = thetaA,
    family = "binomial",
    sigma2 = 1
  )
```


```{r}
thetaB <- c(2, 1.5, rep(0, 48))
SigmaB <- diag(length(thetaB) - 1)
nB <- 150

# correlation between theta2, theta3, theta4 is 0.8
idx <- 2:4
block <- SigmaB[idx, idx]
block[!diag(length(idx))] <- 0.8
SigmaB[idx, idx] <- block

sigma2B <- 1


data_regr_B <- data_regularization(
    n = nB,
    Sigma = SigmaB,
    theta_true = thetaB,
    family = "gaussian",
    sigma2 = 1
  )

data_class_B <- data_regularization(
    n = nB,
    Sigma = SigmaB,
    theta_true = thetaB,
    family = "binomial",
    sigma2 = 1
  )
```

Train test split:

```{r}
nA_train = 100
nB_train = 50  # for p = n setting

idx_RA <- sample(seq_len(nA), size = nA_train)
idx_RB <- sample(seq_len(nB), size = nA_train)
idx_CA <- sample(seq_len(nA), size = nA_train)
idx_CB <- sample(seq_len(nB), size = nB_train)

train_regr_A <- data_regr_A[idx_RA, ]
train_regr_B <- data_regr_B[idx_RB, ]
train_class_A <- data_class_A[idx_CA, ]
train_class_B <- data_class_B[idx_CB, ]

test_regr_A <- data_regr_A[-idx_RA, ]
test_regr_B <- data_regr_B[-idx_RB, ]
test_class_A <- data_class_A[-idx_CA, ]
test_class_B <- data_class_B[-idx_CB, ]
```


# 2. Model fitting

## Models

```{r}
# regression
fits_regr_A <- fit_models(train_regr_A, family = "gaussian",
                          sample = 5000, burnin = 1000, thin = 10)
fits_regr_B <- fit_models(train_regr_B, family = "gaussian",
                          sample = 5000, burnin = 1000, thin = 10)

# classification
fits_class_A <- fit_models(train_class_A, family = "binomial",
                           sample = 5000, burnin = 1000, thin = 10)
fits_class_B <- fit_models(train_class_B, family = "binomial",
                           sample = 5000, burnin = 1000, thin = 10)
```


## get coefficients

```{r}
fits <- list(regr_A = fits_regr_A,
             regr_B = fits_regr_B,
             class_A = fits_class_A,
             class_B = fits_class_B)

models_df <- imap_dfr(fits, function(model_list, scenario) {
  imap_dfr(model_list, function(fit, model) {
    if (model == "flat") {
      fx <- summary(fit)$fixed
      as_tibble(fx, rownames = "parameter") %>%
        transmute(
          model     = model,
          parameter = parameter,
          theta     = Estimate,
          sd        = Est.Error,
          ci_low    = `l-95% CI`,
          ci_upp    = `u-95% CI`
        )
    } else {
      fx <- summary(fit)
      tibble(
        model = model,
        parameter = rownames(fx$mu.coef),
        theta = fx$mu.coef[, 1],
        sd = fx$se.coef[, 1],
        ci_low = fx$CI.coef[, 1],
        ci_upp = fx$CI.coef[, 1]
      )
    }
  }, .id = "model")
}, .id = "scenario")


models_df <- models_df %>%
  separate(scenario, into = c("type","group"), sep = "_")

models_df <- models_df %>%
  mutate(parameter = case_when(
    parameter %in% c("Intercept", "(Intercept)") ~ "0",
    TRUE ~ str_remove(parameter, "x_")
  ))
```


# 3. Prediction + Evaluation

Prediction

```{r}
# > head(predict(mod, newdata = test_class_A))
#      Estimate  Est.Error Q2.5 Q97.5
# [1,]   0.9325 0.25120013    0     1
# [2,]   0.3525 0.47834693    0     1
# [3,]   0.9700 0.17080086    0     1
# [4,]   0.9900 0.09962335    1     1
# [5,]   0.3875 0.48778950    0     1
# [6,]   0.8275 0.37828759    0     1
# > head(predict(fits_class_A$ridge, newdata = test_class_A))
#          [,1]
# 2  2.66391969
# 4  0.09084899
# 6  3.09099527
# 8  4.87468182
# 11 0.12960716
# 12 1.79132630
# > head(predict(fits_class_A$lasso, newdata = test_class_A))
#         [,1]
# 2  2.4583415
# 4  0.4548547
# 6  2.9238899
# 8  4.9175404
# 11 0.1584352
# 12 1.5937710
```

Evaluation: (for each scenario, for regr and class)

```{r}
# Hits (i.e 0 coefficients): number (percentage) of correctly identified influential variables

# number of non-influential variables declared as influential


# MLPD (mean log posterior predicitve density)
```



# 4. Results

Plot 1: Coefficients

- 2 plots, for regression and classification
- for each 2 data scenarios (facet)
- plot coefficients (flat, ridge, lasso, true) + CI in different colors (on x-axis, value on y-axis)
- draw dotted line on 0 (if 0 in CI, we choose the coefficient as 0)

Plot / Table

- 1, 2: percentage in brackets
- 4 rows, divider between regression and classification
- columns are evaluation statistics


