---
title: "Simulation Study: Regularization with Bayesian regression"
author: "Lona Koers"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("functions.R")
```

```{r}
set.seed(2025)
```

# 1. Data generation

```{r}
thetaA <- c(2, 1.5, 0, 0, 0)
SigmaA <- diag(length(thetaA) - 1)
sigma2A <- 1
nA <- 150

data_regr_A <- data_regularization(
    n = nA,
    Sigma = SigmaA,
    theta_true = thetaA,
    family = "gaussian",
    sigma2 = 1
  )
  
data_class_A <- data_regularization(
    n = nA,
    Sigma = SigmaA,
    theta_true = thetaA,
    family = "binomial",
    sigma2 = 1
  )
```


```{r}
thetaB <- c(2, 1.5, rep(0, 48))
SigmaB <- diag(length(thetaB) - 1)
nB <- 150

# correlation between theta2, theta3, theta4 is 0.8
idx <- 2:4
block <- SigmaB[idx, idx]
block[!diag(length(idx))] <- 0.8
SigmaB[idx, idx] <- block

sigma2B <- 1


data_regr_B <- data_regularization(
    n = nB,
    Sigma = SigmaB,
    theta_true = thetaB,
    family = "gaussian",
    sigma2 = 1
  )

data_class_B <- data_regularization(
    n = nB,
    Sigma = SigmaB,
    theta_true = thetaB,
    family = "binomial",
    sigma2 = 1
  )
```

Train test split:

```{r}
nA_train = 100
nB_train = 50  # for p = n setting

idx_RA <- sample(seq_len(nA), size = nA_train)
idx_RB <- sample(seq_len(nB), size = nA_train)
idx_CA <- sample(seq_len(nA), size = nA_train)
idx_CB <- sample(seq_len(nB), size = nB_train)

train_regr_A <- data_regr_A[idx_RA, ]
train_regr_B <- data_regr_B[idx_RB, ]
train_class_A <- data_class_A[idx_CA, ]
train_class_B <- data_class_B[idx_CB, ]

test_regr_A <- data_regr_A[-idx_RA, ]
test_regr_B <- data_regr_B[-idx_RB, ]
test_class_A <- data_class_A[-idx_CA, ]
test_class_B <- data_class_B[-idx_CB, ]
```


# 2. Model fitting

## Models

```{r}
# regression
fits_regr_A <- fit_models(train_regr_A, family = "gaussian",
                          sample = 5000, burnin = 1000, thin = 10)
fits_regr_B <- fit_models(train_regr_B, family = "gaussian",
                          sample = 5000, burnin = 1000, thin = 10)

# classification
fits_class_A <- fit_models(train_class_A, family = "binomial",
                           sample = 5000, burnin = 1000, thin = 10)
fits_class_B <- fit_models(train_class_B, family = "binomial",
                           sample = 5000, burnin = 1000, thin = 10)
```


## get coefficients

```{r}
fits <- list(regr_A = fits_regr_A,
             regr_B = fits_regr_B,
             class_A = fits_class_A,
             class_B = fits_class_B)

models_df <- imap_dfr(fits, # over the 100 replicates
  ~ imap_dfr(.x, function(fit, model) { # over the three models

  }),
  .id = "replicate"
)


models_df <- imap_dfr(fits, function(model_list, scenario) {
  # model_list is list(flat=brm, ridge=brm, lasso=brm)
  map_dfr(model_list, function(fit, model) {
      if (model == "flat") {
        fx <- summary(fit)$fixed # extract the fixed-effects matrix
        as_tibble(fx, rownames = "parameter") %>%  # turn it into a tibble
          transmute(
            model     = model,  # "flat","ridge","lasso"
            parameter = rownames(fx),  # "(Intercept)","x_1",...
            theta     = Estimate,  # posterior mean
            sd        = Est.Error,  # posterior sd
            ci_low    = `l-95% CI`,  # lower 95% CI
            ci_upp    = `u-95% CI` # upper 95% CI
          )
        
      } else {
        fx <- summary(fit)
        tibble(
          model = model,  # "flat","ridge","lasso"
          parameter = rownames(fx$mu.coef),
          theta = fx$mu.coef[, 1],
          sd = fx$se.coef[, 1],
          ci_low = fx$CI.coef[, 1],
          ci_upp = fx$CI.coef[, 1]
        )
      }
  }, .id = "model")
}, .id = "scenario")

models_df <- models_df %>%
  separate(scenario, into = c("type","group"), sep = "_")
```


# 3. Prediction + Evaluation

Prediction

```{r}

```

Evaluation: (for each scenario, for regr and class)

```{r}
# Hits (i.e 0 coefficients): number (percentage) of correctly identified influential variables

# number of non-influential variables declared as influential


# MLPD (mean log posterior predicitve density)
```



# 4. Results

Plot 1: Coefficients

- 2 plots, for regression and classification
- for each 2 data scenarios (facet)
- plot coefficients (flat, ridge, lasso, true) + CI in different colors (on x-axis, value on y-axis)
- draw dotted line on 0 (if 0 in CI, we choose the coefficient as 0)

Plot / Table

- 1, 2: percentage in brackets
- 4 rows, divider between regression and classification
- columns are evaluation statistics


