---
title: "Simulation Study: Regularization with Bayesian regression"
author: "Lona Koers"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("functions.R")
```

```{r}
set.seed(2025)
```

# 1. Data generation

```{r}
thetaA <- c(2, 1.5, 0, 0, 0)
SigmaA <- diag(length(thetaA) - 1)
sigma2A <- 1
nA <- 150

data_regr_A <- data_regularization(
    n = nA,
    Sigma = SigmaA,
    theta_true = thetaA,
    family = "gaussian",
    sigma2 = 1
  )
  
data_class_A <- data_regularization(
    n = nA,
    Sigma = SigmaA,
    theta_true = thetaA,
    family = "binomial",
    sigma2 = 1
  )
```


```{r}
thetaB <- c(2, 1.5, rep(0, 28))
SigmaB <- diag(length(thetaB) - 1)
nB <- 150

# correlation between theta2, theta3, theta4 is 0.8
idx <- 2:4
block <- SigmaB[idx, idx]
block[!diag(length(idx))] <- 0.8
SigmaB[idx, idx] <- block

sigma2B <- 1


data_regr_B <- data_regularization(
    n = nB,
    Sigma = SigmaB,
    theta_true = thetaB,
    family = "gaussian",
    sigma2 = 1
  )

data_class_B <- data_regularization(
    n = nB,
    Sigma = SigmaB,
    theta_true = thetaB,
    family = "binomial",
    sigma2 = 1
  )
```

Train test split:

```{r}
nA_train = 100
nB_train = 30  # for p = n setting

idx_RA <- sample(seq_len(nA), size = nA_train)
idx_RB <- sample(seq_len(nB), size = nA_train)
idx_CA <- sample(seq_len(nA), size = nA_train)
idx_CB <- sample(seq_len(nB), size = nB_train)

train_regr_A <- data_regr_A[idx_RA, ]
train_regr_B <- data_regr_B[idx_RB, ]
train_class_A <- data_class_A[idx_CA, ]
train_class_B <- data_class_B[idx_CB, ]

test_regr_A <- data_regr_A[-idx_RA, ]
test_regr_B <- data_regr_B[-idx_RB, ]
test_class_A <- data_class_A[-idx_CA, ]
test_class_B <- data_class_B[-idx_CB, ]
```


# 2. Model fitting

## Models

```{r}
# regression
fits_regr_A <- fit_models(train_regr_A, family = "gaussian",
                          sample = 20000, burnin = 1000, thin = 10)
fits_regr_B <- fit_models(train_regr_B, family = "gaussian",
                          sample = 20000, burnin = 1000, thin = 10)

# classification
fits_class_A <- fit_models(train_class_A, family = "binomial",
                           sample = 20000, burnin = 1000, thin = 10)
fits_class_B <- fit_models(train_class_B, family = "binomial",
                           sample = 20000, burnin = 1000, thin = 10)
```


## Get coefficients

```{r}
fits <- list(regr_A = fits_regr_A,
             regr_B = fits_regr_B,
             class_A = fits_class_A,
             class_B = fits_class_B)

models_df <- imap_dfr(fits, function(model_list, scenario) {
  imap_dfr(model_list, function(fit, model) {
    if (model == "flat") {
      fx <- summary(fit)$fixed
      as_tibble(fx, rownames = "parameter") %>%
        transmute(
          model     = model,
          parameter = parameter,
          theta     = Estimate,
          sd        = Est.Error,
          ci_low    = `l-95% CI`,
          ci_upp    = `u-95% CI`
        )
    } else {
      fx <- summary(fit)
      tibble(
        model = model,
        parameter = rownames(fx$mu.coef),
        theta = fx$mu.coef[, 1],
        sd = fx$se.coef[, 1],
        ci_low = fx$CI.coef[, 1],
        ci_upp = fx$CI.coef[, 2]
      )
    }
  }, .id = "model")
}, .id = "scenario")


models_df <- models_df %>%
  separate(scenario, into = c("type","scenario"), sep = "_")

models_df <- models_df %>%
  mutate(parameter = case_when(
    parameter %in% c("Intercept", "(Intercept)") ~ "0",
    TRUE ~ str_remove(parameter, "x_")
  ))
```


# 3. Prediction + Evaluation

Evaluation: (for each scenario, for regr and class)

```{r}
# add influence column
models_df <- models_df %>%
  mutate(
    influential = !((ci_low <= 0) & (ci_upp >= 0)),  # is parameter declared influential
    actual_influential = parameter %in% c(0, 1)  # is parameter actually influential (only the case for parameter 0 and 1)
  )

# hits and false positives
summary_df <- models_df %>%
  group_by(type, scenario, model) %>%
  summarise(
    # how many true non-zero parameters there are (always 2 here)
    n_true = 2,
    hits = sum(actual_influential & influential), # = true positives
    n_zero = sum(!actual_influential), # how many true zeros
    false_positives = sum(!actual_influential & influential), # = zero declared influential
    .groups = "drop"
  )

tests <- list(
  regr_A   = test_regr_A,
  regr_B   = test_regr_B,
  class_A  = test_class_A,
  class_B  = test_class_B
)

mlppd_df <- imap_dfr(fits, function(model_list, scen) {
  parts  <- str_split(scen, "_", simplify = TRUE)
  type   <- parts[1]
  scenario  <- parts[2]
  family <- if (type=="regr") "gaussian" else "binomial"
  dtest  <- tests[[scen]]
  
  imap_dfr(model_list, function(fit, mod) {
    m = mlppd(fit, dtest, family, mod)
    list(
      type = type,
      scenario = scenario,
      model = mod,
      mlppd = m
    )
  }, .id = "model")
}, .id = NULL)

summary_df <- summary_df %>%
  left_join(
    mlppd_df,
    by = c("scenario", "type", "model")
  )
```


# 4. Results

Plot 1: Coefficients from models_df

- 2 plots, for regression and classification
- for each 2 data scenarios (facet)
- plot coefficients (flat, ridge, lasso, true) + CI in different colors (on x-axis, value on y-axis)
- draw dotted line on 0 (if 0 in CI, we choose the coefficient as 0)

```{r}
true_df <- models_df %>%
  distinct(scenario, parameter) %>%
  mutate(
    true = case_when(
      scenario == "A" & parameter == 0 ~ 2,
      scenario == "A" & parameter == 1 ~ 1.5,
      scenario == "A" ~ 0,
      scenario == "B" & parameter == 0 ~ 2,
      scenario == "B" & parameter == 1 ~ 1.5,
      TRUE ~ 0
    )
  )
```


```{r}
type_labs <- c(
  class = "Logistic Model (Classification)",
  regr  = "Linear Model (Regression)"
)

make_scenario_plot <- function(df, true_df, scen) {
  df2 <- df %>%
    filter(scenario == scen) %>%
    # pick the right parameter range
    filter((scenario=="A" & parameter %in% 0:4) |
           (scenario=="B" & parameter %in% 0:29)) %>%
    mutate(param = as.integer(parameter),
           type = factor(type, levels = c("regr", "class")),
           ci_low = pmax(ci_low, -2),
           ci_upp = pmin(ci_upp,  5))
  tr2 <- true_df %>%
    filter(scenario == scen) %>%
    mutate(param = as.integer(parameter))
    
  ggplot(df2, aes(x = param, y = theta, color = model)) +
    geom_pointrange(aes(ymin = ci_low, ymax = ci_upp),
                    position = position_dodge(0.6),
                    size = 0.3, fatten = 0.8) +
    geom_point(data = tr2, aes(x = param, y = true),
               inherit.aes = FALSE,
               shape = 4, size = 2, stroke = 0.5) +
    geom_hline(yintercept = 0, linetype = "dotted") +
    scale_x_continuous(breaks = unique(df2$param), minor_breaks = NULL) +
    coord_cartesian(ylim = c(-1, 4)) +
    scale_color_brewer("prior", palette = "Set2") +
    facet_wrap(~ type, ncol = 1,
               labeller = as_labeller(type_labs)) +
    labs(
      title    = NULL,
      subtitle = paste0("Scenario ", scen),
      x        = "parameter index",
      y        = "parameter estimate"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
}

# make the two plots
p_A <- make_scenario_plot(models_df, true_df, "A")
p_B <- make_scenario_plot(models_df, true_df, "B")
```


```{r}
p_all <- (p_A + p_B) +
  plot_layout(ncol = 2, widths = c(1, 3), guides="collect", axis_titles = "collect") &
  theme(legend.position="bottom")

ggsave("../figures/reg_all.png", p_all, width = 11, height = 5)
```



```{r}
# 1) Prepare Table for Scenario A
summaryA <- summary_df %>%
  filter(scenario == "A") %>%
  mutate(
    Model = factor(type, levels = c("regr", "class"), labels = c("linear", "logit")),
    Prior = model,
    `Hits (of 2)`  = hits,
    `FP (of 3)`    = false_positives,
    MLPPD = sprintf("%.3f", mlppd)
  )

summaryB <- summary_df %>%
  filter(scenario == "B") %>%
  mutate(
    Model = factor(type, levels = c("regr", "class"), labels = c("linear", "logit")),
    Prior = model,
    `Hits (of 2)`  = hits,
    `FP (of 28)`    = false_positives,
    MLPPD = sprintf("%.3f", mlppd)
  )
  

tabA <- summaryA[, 9:13]
tabB <- summaryB[, 9:13]

tabA
tabB
```



