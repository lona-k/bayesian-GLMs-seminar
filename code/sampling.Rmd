---
title: "Simulation Study: Comparison of approximate Bayesian inference methods"
author: "Lona Koers"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("functions.R")
```

# Experiment setup

Set seed and generate 1000 data sets for regression and classification

```{r}
set.seed(2025)

theta <- c(-0.5, 2, 1)

data_regr <- lapply(seq_len(1000), function(i) {
  data_sim(n = 100, theta_true = theta, family = "gaussian")
})
data_class <- lapply(seq_len(1000), function(i) {
  data_sim(n = 100, theta_true = theta, family = "binomial")
})

prior <- c(mean = 0, var = 10)  # prior for theta
sigma2 <- 10
```

# Metropolis-Hastings

Simulate MCMC algorithm 1000 times (once for each data set) with

- `n = 10000` samples drawn
- `burn_in = 1000` (rule of thumb: burn-in should be 10\% of samples)
- `thin = 10`

```{r}
# regression
mcmc_regr <- map_dfr(data_regr, function(dat) {
  res <- mcmc_sim(dat,
           mcmc_iters = 5000,
           burnin = 500,
           sigma2 = sigma2,
           prior = prior,
           family = "gaussian")
  
  list(time = res$time,
       theta0 = res$mu_post[[1]],
       theta1 = res$mu_post[[2]],
       theta2 = res$mu_post[[3]],
       sd_theta0 = res$sigma_post[[1]],
       sd_theta1 = res$sigma_post[[2]],
       sd_theta2 = res$sigma_post[[3]])
})

# classification
mcmc_class <- map_dfr(data_class, function(dat) {
  res <- mcmc_sim(dat,
           mcmc_iters = 5000,
           burnin = 500,
           sigma2 = sigma2,  # not used for Binomial family
           prior = prior,
           family = "binomial")
  
  list(time = res$time,
       theta0 = res$mu_post[[1]],
       theta1 = res$mu_post[[2]],
       theta2 = res$mu_post[[3]],
       sd_theta0 = res$sigma_post[[1]],
       sd_theta1 = res$sigma_post[[2]],
       sd_theta2 = res$sigma_post[[3]])
})
```




# Laplace Approximation

```{r}
# regression
la_regr <- map_dfr(data_regr, function(dat) {
  res <- la_sim(dat,
           prior = prior,
           sigma2 = sigma2,
           family = "gaussian")
  
  list(time = res$time,
       theta0 = res$mu_post[[1]],
       theta1 = res$mu_post[[2]],
       theta2 = res$mu_post[[3]],
       sd_theta0 = res$sigma_post[[1]],
       sd_theta1 = res$sigma_post[[2]],
       sd_theta2 = res$sigma_post[[3]])
})

# classification
la_class <- map_dfr(data_class, function(dat) {
  res <- la_sim(dat,
           prior = prior,
           sigma2 = sigma2,  # not used for Binomial family
           family = "binomial")
  
  list(time = res$time,
       theta0 = res$mu_post[[1]],
       theta1 = res$mu_post[[2]],
       theta2 = res$mu_post[[3]],
       sd_theta0 = res$sigma_post[[1]],
       sd_theta1 = res$sigma_post[[2]],
       sd_theta2 = res$sigma_post[[3]])
})
```


# Evaluation

## result table

```{r}
# table with all results
res_table <- bind_rows(
  mcmc_regr %>% mutate(method = "MCMC", family = "Gaussian"),
  mcmc_class %>% mutate(method = "MCMC", family = "Binomial"),
  la_regr   %>% mutate(method = "LA", family = "Gaussian"),
  la_class  %>% mutate(method = "LA", family = "Binomial")
)

res_table <- res_table %>%
  mutate(
    # L2‐norm of the error in the 3‐vector of estimates
    loss = sqrt(
      (theta0 - theta[[1]])^2 +
      (theta1 - theta[[2]])^2 +
      (theta2 - theta[[2]])^2
    )
  )

# summary with means
summary_table <- res_table %>%
  group_by(method, family) %>%
  summarise(
    across(
      .cols = where(is.numeric),
      .fns = mean
    ),
    .groups = "drop"
  )
```


## result plots

Data prep: 

```{r}
true_df <- data.frame(
  theta = factor(c("0","1","2"),
                     levels = c("0","1","2")),
  true = c(-0.5, 2, 1)
)

coef_df <- res_table %>%
  pivot_longer(
    cols      = starts_with("theta"),
    names_to  = "theta"
  ) %>%
  mutate(theta = recode(theta,
                      theta0 = "0",
                      theta1 = "1",
                      theta2 = "2"),
       theta = factor(theta, levels = c("0", "1", "2"))) %>%
  left_join(true_df, by = "theta")


sd_df <- res_table %>%
  pivot_longer(
    cols      = starts_with("sd_"),
    names_to  = "parameter",
    values_to = "sd"
  ) %>%
  mutate(parameter = recode(parameter,
    sd_theta0 = "0",
    sd_theta1 = "1",
    sd_theta2 = "2"
  )) %>%
  mutate(parameter = factor(parameter, levels = c("0", "1", "2")))

# split data
combined_gauss <- res_table %>%   filter(family=="Gaussian")
coef_gauss     <- coef_df  %>%   filter(family=="Gaussian")
sd_gauss     <- sd_df  %>%   filter(family=="Gaussian")

combined_binom <- res_table %>%   filter(family=="Binomial")
coef_binom     <- coef_df  %>%   filter(family=="Binomial")
sd_binom     <- sd_df  %>%   filter(family=="Binomial")

```

Plot(s):

```{r}
# regression plots
pp_time_g <- ggplot(combined_gauss, aes(x = method, y = time, fill = method)) +
  geom_boxplot(outlier.size = 0.2) +
  scale_fill_brewer(type="qual", palette="Set1") +
  labs(subtitle="Elapsed time", y="time (s)", x=NULL, fill="Method") +
  theme_minimal()

pp_theta_g <- ggplot(coef_gauss, aes(x = method, y = value, fill = method)) +
  geom_boxplot(outlier.size = 0.2) +
  geom_hline(data = true_df,
             aes(yintercept = true, linetype="True parameter"),
             colour = "black") +
  scale_linetype_manual(name=NULL, values="dashed") +
  scale_fill_brewer(type="qual", palette="Set1") +
  labs(subtitle="Parameter estimates", y="mean estimate", x=NULL, fill="Method") +
  facet_wrap(~ theta, scales="free_y") +
  theme_minimal()

pp_sd_g <- ggplot(sd_gauss, aes(x = method, y = sd, fill = method)) +
  geom_boxplot(outlier.size = 0.2) +
  scale_fill_brewer(type="qual", palette="Set1") +
  labs(subtitle="Posterior parameter standard deviation", y="mean standard deviation", x=NULL, fill="Method") +
  facet_wrap(~ parameter) +
  theme_minimal()


# classif plots
pp_time_b <- ggplot(combined_binom, aes(x = method, y = time, fill = method)) +
  geom_boxplot(outlier.size = 0.2) +
  scale_fill_brewer(type="qual", palette="Set1") +
  labs(subtitle="Elapsed time", y="time (s)", x=NULL, fill="Method") +
  theme_minimal()

pp_theta_b <- ggplot(coef_binom, aes(x = method, y = value, fill = method)) +
  geom_boxplot(outlier.size = 0.2) +
  geom_hline(data=true_df,
             aes(yintercept=true, linetype="True parameter"),
             colour="black") +
  scale_linetype_manual(name=NULL, values="dashed") +
  scale_fill_brewer(type="qual", palette="Set1") +
  labs(subtitle="Parameter estimates", y="mean estimate", x=NULL, fill="Method") +
  facet_wrap(~ theta, scales = "free_y") +
  theme_minimal()

pp_sd_b <- ggplot(sd_binom, aes(x = method, y = sd, fill = method)) +
  geom_boxplot(outlier.size = 0.2) +
  scale_fill_brewer(type="qual", palette="Set1") +
  labs(subtitle="Posterior parameter standard deviation", y="mean standard deviation", x=NULL, fill="Method") +
  facet_wrap(~ parameter) +
  theme_minimal()

```

Plot for paper

```{r}
pp_gauss <- (pp_time_g + pp_theta_g + pp_sd_g) +
  plot_layout(ncol=3, widths=c(1,2,2), guides="collect") &
  theme(legend.position="bottom")

pp_binom <- (pp_time_b + pp_theta_b + pp_sd_b) +
  plot_layout(ncol=3, widths=c(1,2,2), guides="collect") &
  theme(legend.position = "bottom")

ggsave("../figures/approx_regr.png", pp_gauss, width = 10, height = 4)
ggsave("../figures/approx_class.png", pp_binom, width = 10, height = 4)
```


Plot for presentation

```{r}
p_gauss <- (pp_time_g + pp_theta_g + pp_sd_g) +
  plot_layout(ncol=3, widths=c(1,2,2), guides="collect") +
  plot_annotation(title = "Regression") &
  theme(legend.position="bottom")

p_binom <- (pp_time_b + pp_theta_b + pp_sd_b) +
  plot_layout(ncol=3, widths=c(1,2,2), guides="collect") +
  plot_annotation(title = "Classification") &
  theme(legend.position = "bottom")

# combined plot
final_plot <- (p_gauss / p_binom) +
  plot_layout(guides="collect") &
  theme(legend.position="bottom")

ggsave("../figures/approx_all.png", final_plot, width = 15, height = 8)
```

